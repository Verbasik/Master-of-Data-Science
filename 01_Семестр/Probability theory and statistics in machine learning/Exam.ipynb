{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Билеты к экзамену 13.01.25"
      ],
      "metadata": {
        "id": "DFSCmqFGd0EL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Вопрос 1: Визуализация данных с помощью диаграммы рассеяния. Привести пример.**\n",
        "\n",
        "## **Визуализация взаимосвязей между переменными**\n",
        "\n",
        "Диаграмма рассеяния, также известная как точечная диаграмма, является мощным инструментом визуализации в статистическом анализе данных. Ее основное назначение заключается в исследовании взаимосвязей между двумя количественными переменными. Каждая точка на диаграмме представляет собой одно наблюдение, где положение точки по горизонтальной оси (ось X) соответствует значению одной переменной, а положение по вертикальной оси (ось Y) соответствует значению другой переменной.\n",
        "\n",
        "Используя диаграмму рассеяния, мы можем визуально оценить:\n",
        "\n",
        "* **Наличие и направление связи:**  Видно, существует ли какая-либо тенденция в расположении точек. Например, если при увеличении значений одной переменной значения другой переменной также в основном увеличиваются, это указывает на положительную связь. Если же при увеличении значений одной переменной значения другой в основном уменьшаются, это говорит об отрицательной связи.\n",
        "* **Силу связи:**  Насколько тесно точки сгруппированы вокруг определенной тенденции. Если точки расположены близко к прямой линии, связь считается сильной. Если точки разбросаны хаотично, связь слабая или отсутствует.\n",
        "* **Тип связи:**  Линейная или нелинейная. Если точки образуют прямую линию или близкую к ней полосу, связь считается линейной. Если точки образуют кривую, связь нелинейная.\n",
        "* **Наличие выбросов:**  Точки, значительно отклоняющиеся от общей тенденции, могут быть идентифицированы как выбросы, которые могут указывать на ошибки в данных или на особые случаи, требующие дополнительного изучения.\n",
        "\n",
        "Математически, взаимосвязь между переменными может быть оценена с помощью коэффициента корреляции, например, коэффициента корреляции Пирсона для линейных связей. Однако диаграмма рассеяния позволяет получить визуальное представление о характере этой связи, которое не всегда отражается одним числом.\n",
        "\n",
        "## **Корреляция**\n",
        "\n",
        "Корреляция — это статистический метод, который используется для оценки взаимосвязи между двумя переменными.\n",
        "\n",
        "### Математическая формализация\n",
        "\n",
        "Корреляция между двумя переменными $ X $ и $ Y $ может быть измерена с использованием коэффициента корреляции Пирсона, который определяется следующей формулой:\n",
        "\n",
        "$$\n",
        "r = \\frac{cov(X, Y)}{\\sigma_X \\sigma_Y} = \\frac{n(\\sum x_i y_i) - (\\sum x_i)(\\sum y_i)}{\\sqrt{[n\\sum x_i^2 - (\\sum x)^2][n\\sum y_i^2 - (\\sum y_i)^2]}}\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $( r )$ — коэффициент корреляции,\n",
        "- $( cov(X, Y) )$ — ковариация между переменными $( X )$ и $( Y )$,\n",
        "- $( \\sigma_X )$ и $( \\sigma_Y )$ — стандартные отклонения переменных $( X )$ и $( Y )$ соответственно,\n",
        "- $n$ — количество наблюдений.\n",
        "\n",
        "**Числитель**:\n",
        "\n",
        "- $ n(\\sum x_i y_i) - (\\sum x_i)(\\sum y_i) = E[(X - E(X))(Y - E(Y))] = E(XY) - E(X)E(Y) $\n",
        "\n",
        "где:\n",
        "- $E(XY)$ — математическое ожидание произведения $X$ и $Y$,\n",
        "- $E(X)$ и $E(Y)$ — математические ожидания величин $X$ и $Y$.\n",
        "\n",
        "Эта часть формулы представляет \"ковариацию\" между $x_i$ и $y_i$. Ковариация показывает, насколько переменные изменяются вместе. Если $x_i$ возрастает, когда $y_i$ возрастает, то ковариация будет положительной. Если $x_i$ уменьшается, когда $y_i$ возрастает, ковариация будет отрицательной. Если переменные не имеют явной связи, ковариация будет близка к нулю.\n",
        "\n",
        "**Знаменатель**:\n",
        "\n",
        "- $\\sqrt{[n\\sum x_i^2 - (\\sum x)^2][n\\sum y_i^2 - (\\sum y_i)^2]}$\n",
        "\n",
        "Эта часть формулы представляет собой произведение стандартных отклонений $x_i$ и $y_i$. Стандартное отклонение - это мера разброса значений переменной вокруг ее среднего значения. Знаменатель нормализует ковариацию, делая коэффициент корреляции масштабно-инвариантным, т.е. он не зависит от масштаба измерения переменных.\n",
        "\n",
        "**Итоговый коэффициент корреляции**:\n",
        "\n",
        "- **Коэффициент стремится к нулю**: Это означает, что между переменными нет линейной зависимости. В контексте модели линейной регрессии это может быть хорошо, если предикторы действительно не коррелируют с зависимой переменной. Это позволяет модели получить независимую информацию от каждого предиктора, что может улучшить качество прогнозов и интерпретируемость модели.\n",
        "\n",
        "- **Коэффициент стремится к 1**: Это означает сильную положительную линейную зависимость между переменными. В контексте модели линейной регрессии это может быть плохо, если предикторы сильно коррелируют между собой (феномен мультиколлинеарности). Мультиколлинеарность может привести к нестабильным оценкам коэффициентов и усложнить интерпретацию результатов. В таких случаях модель может стать менее надежной и менее эффективной в прогнозировании новых данных.\n",
        "\n",
        "Таким образом, коэффициент корреляции представляет собой нормализованную ковариацию между двумя переменными. Это дает нам меру силы и направления линейной связи между переменными.\n",
        "\n",
        "В контексте линейной регрессии коэффициент может быть использован для оценки степени линейной зависимости между независимыми и зависимой переменными. Это может помочь в определении того, насколько переменные подходят для использования в модели линейной регрессии."
      ],
      "metadata": {
        "id": "bqCAbEMscdYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Данные (пример)\n",
        "hours_studied = np.array([2, 3, 5, 7, 8, 9, 10, 12])\n",
        "exam_scores = np.array([60, 65, 75, 80, 85, 90, 92, 95])\n",
        "\n",
        "# Создание диаграммы рассеяния\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(hours_studied, exam_scores, color='blue', marker='o', label='Студенты')\n",
        "\n",
        "# Добавление заголовка и меток осей\n",
        "plt.title('Зависимость оценки от количества часов подготовки')\n",
        "plt.xlabel('Количество часов подготовки')\n",
        "plt.ylabel('Оценка за экзамен')\n",
        "\n",
        "# Добавление сетки для лучшей читаемости\n",
        "plt.grid(True)\n",
        "\n",
        "# Добавление легенды\n",
        "plt.legend()\n",
        "\n",
        "# Отображение диаграммы\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "o1BfzJm3c2an",
        "outputId": "a5e57321-29ee-444a-f901-6908d82da4d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **Вопрос 2: Описательная статистика разброса данных (размах, квартили, межквартильный размах). Привести пример расчета на небольшом датасете.**\n",
        "\n",
        "### **Вопрос 4: Диаграммы размаха (выбросы). Привести пример расчета на небольшом датасете.**\n",
        "\n",
        "## **Разведывательный анализ данных и его применение**\n",
        "\n",
        "Разведывательный анализ данных (EDA) — это ключевой процесс в обработке данных, который помогает исследовать и понять данные, выявить их структуру и основные характеристики перед проведением более сложных статистических анализов и машинного обучения.\n",
        "\n",
        "\"Ящики с усами\" (или \"boxplot\") — это графический способ представления распределения данных, который позволяет увидеть основные характеристики выборки, такие как медиана, межквартильный размах и выбросы. Ниже подробно описан процесс расчета всех метрик, связанных с ящиками с усами, с использованием математических формул.\n",
        "\n",
        "### 1. Основные метрики для построения ящика с усами\n",
        "\n",
        "#### 1.1 Медиана (Median)\n",
        "Медиана — это центральное значение выборки, которое делит ее на две равные части. Пусть $X = \\{x_1, x_2, \\dots, x_n\\}$ — упорядоченная выборка (по возрастанию). Медиана $M$ вычисляется как:\n",
        "\n",
        "$$\n",
        "M =\n",
        "\\begin{cases}\n",
        "x_{(n+1)/2}, & \\text{если } n \\text{ нечётное}, \\\\\n",
        "\\frac{x_{n/2} + x_{n/2 + 1}}{2}, & \\text{если } n \\text{ чётное}.\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Здесь $n$ — размер выборки, а $x_i$ — элемент выборки.\n",
        "\n",
        "#### 1.2 Первый и третий квартили (Q1 и Q3)\n",
        "\n",
        "**Первый квартиль $Q1$** — значение, ниже которого находится 25% данных выборки.  \n",
        "**Третий квартиль $Q3$** — значение, ниже которого находится 75% данных выборки.\n",
        "\n",
        "Если выборка упорядочена, то $Q1$ и $Q3$ определяются как медианы первой и второй половин выборки соответственно:\n",
        "\n",
        "- Для $Q1$ используем данные $\\{x_1, x_2, \\dots, x_{\\lfloor n/2 \\rfloor}\\}$.\n",
        "- Для $Q3$ используем данные $\\{x_{\\lceil n/2 + 1 \\rceil}, \\dots, x_n\\}$.\n",
        "\n",
        "Формула аналогична формуле медианы, но применяется к соответствующей половине данных.\n",
        "\n",
        "#### 1.3 Межквартильный размах (IQR)\n",
        "\n",
        "Межквартильный размах (Interquartile Range, IQR) определяет ширину \"ящика\" и рассчитывается как:\n",
        "\n",
        "$$\n",
        "IQR = Q3 - Q1\n",
        "$$\n",
        "\n",
        "Зная IQR, мы можем сделать следующие выводы о выборке:\n",
        "\n",
        "1. **Разброс данных в середине выборки**: IQR показывает, насколько широко распределены 50% центральных значений выборки. Чем больше IQR, тем больше разброс данных в середине выборки.\n",
        "\n",
        "2. **Устойчивость к выбросам**: IQR является более устойчивым к выбросам, чем, например, размах (разница между максимальным и минимальным значениями), так как он основан только на центральных 50% данных.\n",
        "\n",
        "3. **Положение медианы**: IQR позволяет оценить, насколько сильно медиана смещена относительно центральных данных. Если IQR мал, это может указывать на то, что данные сконцентрированы вокруг медианы.\n",
        "\n",
        "4. **Дисперсия данных**: Хотя IQR не дает прямой информации о дисперсии всей выборки, он может быть использован для оценки степени разброса данных в центральной части.\n",
        "\n",
        "5. **Определение выбросов**: IQR часто используется для определения выбросов. Обычно выбросами считаются значения, которые лежат ниже $( Q1 - 1.5 \\times IQR )$ или выше $( Q3 + 1.5 \\times IQR )$.\n",
        "\n",
        "Таким образом, IQR предоставляет полезную информацию о распределении данных в середине выборки, позволяя оценить разброс и устойчивость данных к выбросам.\n",
        "\n",
        "### 2. \"Усы\" ящика\n",
        "\n",
        "Усы определяют диапазон данных, которые считаются \"нормальными\". Обычно они ограничиваются значениями, выходящими за пределы 1.5$IQR$ от первого и третьего квартилей:\n",
        "\n",
        "- Нижний предел:\n",
        "$$\n",
        "\\text{Lower Bound} = Q1 - 1.5 \\cdot IQR\n",
        "$$\n",
        "\n",
        "- Верхний предел:\n",
        "$$\n",
        "\\text{Upper Bound} = Q3 + 1.5 \\cdot IQR\n",
        "$$\n",
        "\n",
        "Все значения за пределами этих границ считаются выбросами.\n",
        "\n",
        "### 3. Выбросы (Outliers)\n",
        "\n",
        "Выбросы определяются как точки, которые находятся за пределами интервала $\\text{[Lower Bound, Upper Bound]}$:\n",
        "\n",
        "- Выбросы ниже нижнего предела:\n",
        "$$\n",
        "x < Q1 - 1.5 \\cdot IQR\n",
        "$$\n",
        "\n",
        "- Выбросы выше верхнего предела:\n",
        "$$\n",
        "x > Q3 + 1.5 \\cdot IQR\n",
        "$$\n",
        "\n",
        "Эти точки отображаются отдельно на графике.\n",
        "\n",
        "### 4. Другие статистики, используемые в boxplot\n",
        "\n",
        "#### 4.1 Минимум и максимум (с учетом выбросов)\n",
        "\n",
        "Минимум и максимум внутри \"усов\" определяются как:\n",
        "\n",
        "- Минимум внутри границ:\n",
        "$$\n",
        "\\text{Min} = \\min \\{x \\in X \\mid x \\geq Q1 - 1.5 \\cdot IQR\\}\n",
        "$$\n",
        "\n",
        "- Максимум внутри границ:\n",
        "$$\n",
        "\\text{Max} = \\max \\{x \\in X \\mid x \\leq Q3 + 1.5 \\cdot IQR\\}\n",
        "$$\n",
        "\n",
        "### 5. Пример расчета на данных\n",
        "\n",
        "Допустим, имеется выборка: $X = \\{1, 2, 3, 4, 5, 6, 7, 8, 9\\}$.\n",
        "\n",
        "1. Упорядочим данные (они уже упорядочены).  \n",
        "2. Медиана: $M = 5$.  \n",
        "3. $Q1$: Медиана первой половины $\\{1, 2, 3, 4\\} \\rightarrow Q1 = 2.5$.  \n",
        "4. $Q3$: Медиана второй половины $\\{6, 7, 8, 9\\} \\rightarrow Q3 = 7.5$.  \n",
        "5. $IQR = Q3 - Q1 = 7.5 - 2.5 = 5$.  \n",
        "6. Усы:  \n",
        "   - Нижний предел: $Q1 - 1.5 \\cdot IQR = 2.5 - 1.5 \\cdot 5 = -5$.  \n",
        "   - Верхний предел: $Q3 + 1.5 \\cdot IQR = 7.5 + 1.5 \\cdot 5 = 15$.  \n",
        "\n",
        "Так как все значения выборки $X$ находятся в пределах $[-5, 15]$, выбросов нет.\n",
        "\n",
        "### Итоговое представление метрик:\n",
        "\n",
        "- **Медиана ($M$)**: центральная линия ящика.  \n",
        "- **$Q1$ и $Q3$**: нижняя и верхняя границы ящика.  \n",
        "- **Усы**: нижний и верхний пределы данных (без выбросов).  \n",
        "- **Выбросы**: отдельные точки за пределами усов.  \n",
        "- **IQR**: ширина ящика.\n",
        "\n",
        "Этот метод расчета помогает представить ключевые характеристики распределения и выявить выбросы."
      ],
      "metadata": {
        "id": "ys8syuMpgfkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Датасет\n",
        "data = [-8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 16]\n",
        "\n",
        "# Расчет основных метрик\n",
        "n = len(data)\n",
        "data_sorted = sorted(data)\n",
        "\n",
        "# Медиана\n",
        "if n % 2 == 0:\n",
        "    median = (data_sorted[n // 2 - 1] + data_sorted[n // 2]) / 2\n",
        "else:\n",
        "    median = data_sorted[n // 2]\n",
        "\n",
        "# Квартили\n",
        "q1 = np.median(data_sorted[:n // 2])\n",
        "q3 = np.median(data_sorted[(n + 1) // 2:])\n",
        "\n",
        "# Межквартильный размах (IQR)\n",
        "iqr = q3 - q1\n",
        "\n",
        "# \"Усы\"\n",
        "lower_bound = q1 - 1.5 * iqr\n",
        "upper_bound = q3 + 1.5 * iqr\n",
        "\n",
        "# Выбросы\n",
        "outliers = [x for x in data if x < lower_bound or x > upper_bound]\n",
        "\n",
        "# Результаты расчета\n",
        "print(f\"Медиана: {median}\")\n",
        "print(f\"Первый квартиль (Q1): {q1}\")\n",
        "print(f\"Третий квартиль (Q3): {q3}\")\n",
        "print(f\"Межквартильный размах (IQR): {iqr}\")\n",
        "print(f\"Нижний предел: {lower_bound}\")\n",
        "print(f\"Верхний предел: {upper_bound}\")\n",
        "print(f\"Выбросы: {outliers}\")\n",
        "\n",
        "# Построение boxplot\n",
        "plt.boxplot(data, vert=False, patch_artist=True, showmeans=True)\n",
        "plt.title(\"Диаграмма размаха\")\n",
        "plt.xlabel(\"Значения\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 594
        },
        "id": "z7nVwkUoh_gG",
        "outputId": "c75d700c-c841-4da8-9084-8c5ba29032e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Медиана: 5\n",
            "Первый квартиль (Q1): 2.0\n",
            "Третий квартиль (Q3): 8.0\n",
            "Межквартильный размах (IQR): 6.0\n",
            "Нижний предел: -7.0\n",
            "Верхний предел: 17.0\n",
            "Выбросы: [-8]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **Вопрос 3: Описательная статистика разброса данных (дисперсия, стандартное отклонение). Привести пример расчета на небольшом датасете.**\n",
        "\n",
        "### **Вопрос 14: Математическое ожидание дискретной случайной величины, свойства.**\n",
        "\n",
        "### **Вопрос 15: Дисперсия и среднеквадратическое отклонение дискретной случайной величины, свойства.**\n",
        "\n",
        "### **Вопрос 16: Определение непрерывной случайной величины**\n",
        "\n",
        "## **Нормальное распределение и аномалии**\n",
        "\n",
        "Нормальное распределение — это важная концепция в статистике, которая описывает, как значения переменной распределяются в данных. Например, в случае большого объема выборки, такие как результаты экзаменов студентов, данные часто имеют форму колокола, где большинство наблюдений сосредоточено вокруг среднего значения, с уменьшающейся частотой при удалении от него. Это так называемая \"колоколоподобная\" форма.\n",
        "\n",
        "Однако в реальной жизни данные могут не всегда соответствовать нормальному распределению. Если мы получаем результаты, которые не показывают нормальности, это может указывать на наличие аномалий. Аномалии — это наблюдения, которые значительно отличаются от остальных данных. Их необходимо выявлять, так как они могут исказить результаты анализа.\n",
        "\n",
        "### Нормальное распределение\n",
        "\n",
        "Нормальное распределение (или распределение Гаусса) — это одно из наиболее важных распределений в статистике. Оно характеризуется двумя параметрами:\n",
        "\n",
        "- **Математическое ожидание** $\\mu$ (среднее значение).\n",
        "- **Дисперсия** $\\sigma^2$ (квадрат стандартного отклонения).\n",
        "\n",
        "### Математическое ожидание\n",
        "\n",
        "**Математическое ожидание** (или среднее значение) — это одно из ключевых понятий в теории вероятностей и статистике. Оно представляет собой средневзвешенное значение случайной величины, где весами являются вероятности (для дискретных случайных величин) или плотности вероятности (для непрерывных случайных величин).\n",
        "\n",
        "### Формула для дискретной случайной величины\n",
        "\n",
        "Если $X$ — дискретная случайная величина, которая может принимать значения $x_1, x_2, \\dots, x_n$ с вероятностями $p_1, p_2, \\dots, p_n$ соответственно, то математическое ожидание $E(X)$ определяется как:\n",
        "\n",
        "$$ E(X) = \\sum_{i=1}^{n} x_i \\cdot p_i $$\n",
        "\n",
        "**Пояснение:**\n",
        "\n",
        "- $x_i$ — возможные значения случайной величины $X$.\n",
        "- $p_i$ — вероятности, с которыми $X$ принимает значения $x_i$.\n",
        "- Сумма $\\sum_{i=1}^{n} x_i \\cdot p_i$ — это сумма всех возможных значений, взвешенных по их вероятностям.\n",
        "\n",
        "**Пример:**\n",
        "\n",
        "Предположим, у нас есть игральная кость, и мы хотим найти математическое ожидание числа выпавших очков. Возможные значения $X$ — это 1, 2, 3, 4, 5, 6, и каждое из них имеет вероятность $\\frac{1}{6}$.\n",
        "\n",
        "$$ E(X) = 1 \\cdot \\frac{1}{6} + 2 \\cdot \\frac{1}{6} + 3 \\cdot \\frac{1}{6} + 4 \\cdot \\frac{1}{6} + 5 \\cdot \\frac{1}{6} + 6 \\cdot \\frac{1}{6} $$\n",
        "\n",
        "$$ E(X) = \\frac{1 + 2 + 3 + 4 + 5 + 6}{6} = \\frac{21}{6} = 3.5 $$\n",
        "\n",
        "Таким образом, математическое ожидание числа выпавших очков при броске игральной кости равно 3.5.\n",
        "\n",
        "### Формула для непрерывной случайной величины\n",
        "\n",
        "*   **Непрерывная случайная величина** – величина, которая может принимать любое значение в определенном диапазоне. В отличие от дискретных случайных величин, которые принимают только отдельные значения, непрерывные могут принимать любые значения на числовой оси. Примеры включают рост человека или время, необходимое для выполнения задачи.\n",
        "\n",
        "Если $X$ — непрерывная случайная величина с функцией плотности вероятности $f(x)$, то математическое ожидание $E(X)$ определяется как:\n",
        "\n",
        "$$ E(X) = \\int_{-\\infty}^{\\infty} x \\cdot f(x) \\, dx $$\n",
        "\n",
        "**Пояснение:**\n",
        "\n",
        "- $x$ — переменная интегрирования, представляющая все возможные значения случайной величины $X$.\n",
        "- $f(x)$ — функция плотности вероятности, которая определяет вероятность того, что $X$ примет значение вблизи $x$.\n",
        "- Интеграл $\\int_{-\\infty}^{\\infty} x \\cdot f(x) \\, dx$ — это интеграл по всей области значений $x$, взвешенный по плотности вероятности.\n",
        "\n",
        "**Пример:**\n",
        "\n",
        "Предположим, у нас есть нормально распределенная случайная величина $X$ с параметрами $\\mu = 0$ и $\\sigma^2 = 1$. Функция плотности вероятности для стандартного нормального распределения $N(0, 1)$ задается формулой:\n",
        "\n",
        "$$ f(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right) $$\n",
        "\n",
        "Математическое ожидание $E(X)$ для этого распределения:\n",
        "\n",
        "$$ E(X) = \\int_{-\\infty}^{\\infty} x \\cdot \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right) \\, dx $$\n",
        "\n",
        "Этот интеграл можно вычислить, используя свойства нормального распределения. Для стандартного нормального распределения $N(0, 1)$ математическое ожидание равно 0.\n",
        "\n",
        "### Свойства математического ожидания\n",
        "\n",
        "1. **Линейность:**\n",
        "   $$ E(aX + bY) = aE(X) + bE(Y) $$\n",
        "   где $a$ и $b$ — константы, а $X$ и $Y$ — случайные величины.\n",
        "\n",
        "2. **Математическое ожидание константы:**\n",
        "   $$ E(c) = c $$\n",
        "   где $c$ — константа.\n",
        "\n",
        "3. **Математическое ожидание произведения независимых случайных величин:**\n",
        "   $$ E(XY) = E(X)E(Y) $$\n",
        "   если $X$ и $Y$ независимы.\n",
        "\n",
        "### Заключение\n",
        "\n",
        "Математическое ожидание — это средневзвешенное значение случайной величины, которое может быть вычислено как сумма (для дискретных случайных величин) или интеграл (для непрерывных случайных величин). Оно играет ключевую роль в описании центральной тенденции распределения вероятностей и широко используется в статистике и теории вероятностей.\n",
        "\n",
        "---\n",
        "\n",
        "### Дисперсия\n",
        "\n",
        "Дисперсия — это мера разброса значений случайной величины относительно её математического ожидания. Она показывает, насколько в среднем квадрат отклонения значений случайной величины отличается от квадрата её математического ожидания.\n",
        "\n",
        "### Формулы расчета дисперсии\n",
        "\n",
        "Существует два эквивалентных способа вычисления дисперсии:\n",
        "\n",
        "1. Через отклонение от математического ожидания:\n",
        "$$ D(X) = E((X - E(X))^2) $$\n",
        "\n",
        "2. Через математическое ожидание квадрата:\n",
        "$$ D(X) = E(X^2) - (E(X))^2 $$\n",
        "\n",
        "p.s.\n",
        "- В данных формулах достаточно знать математическое ожидание.\n",
        "\n",
        "### Формула для дискретной случайной величины\n",
        "\n",
        "Если $X$ — дискретная случайная величина, то её дисперсия определяется как:\n",
        "\n",
        "$$ D(X) = \\sum_{i=1}^{n} (x_i - E(X))^2 \\cdot p_i $$\n",
        "\n",
        "или\n",
        "\n",
        "$$ D(X) = \\sum_{i=1}^{n} x_i^2 \\cdot p_i - \\left(\\sum_{i=1}^{n} x_i \\cdot p_i\\right)^2 $$\n",
        "\n",
        "**Пример:**\n",
        "Рассмотрим игральную кость. Найдем дисперсию числа очков:\n",
        "\n",
        "1. Уже известно $E(X) = 3.5$\n",
        "2. Вычислим $E(X^2)$:\n",
        "$$ E(X^2) = 1^2 \\cdot \\frac{1}{6} + 2^2 \\cdot \\frac{1}{6} + 3^2 \\cdot \\frac{1}{6} + 4^2 \\cdot \\frac{1}{6} + 5^2 \\cdot \\frac{1}{6} + 6^2 \\cdot \\frac{1}{6} $$\n",
        "$$ E(X^2) = \\frac{91}{6} \\approx 15.17 $$\n",
        "\n",
        "3. Тогда дисперсия:\n",
        "$$ D(X) = E(X^2) - (E(X))^2 = \\frac{91}{6} - \\left(\\frac{7}{2}\\right)^2 = \\frac{35}{12} \\approx 2.92 $$\n",
        "\n",
        "4. Стандартное отклонение $(\\sigma)$ — это квадратный корень из дисперсии $(D(X))$. В данном случае:\n",
        "\n",
        "$$\n",
        "\\sigma = \\sqrt{D(X)} = \\sqrt{2.92} \\approx 1.71\n",
        "$$\n",
        "\n",
        "Таким образом, стандартное отклонение числа очков при броске игральной кости составляет примерно **1.71**.\n",
        "\n",
        "### Формула для непрерывной случайной величины\n",
        "\n",
        "Для непрерывной случайной величины $X$ с функцией плотности вероятности $f(x)$ дисперсия определяется как:\n",
        "\n",
        "$$ D(X) = \\int_{-\\infty}^{\\infty} (x - E(X))^2 \\cdot f(x) \\, dx $$\n",
        "\n",
        "или\n",
        "\n",
        "$$ D(X) = \\int_{-\\infty}^{\\infty} x^2 f(x) \\, dx - \\left(\\int_{-\\infty}^{\\infty} x f(x) \\, dx\\right)^2 $$\n",
        "\n",
        "**Пример**\n",
        "\n",
        "**Условие:**\n",
        "Пусть $X$ — непрерывная случайная величина, заданная на интервале $[0, 2]$ с функцией плотности вероятности:\n",
        "\n",
        "$$\n",
        "f(x) =\n",
        "\\begin{cases}\n",
        "\\frac{3}{8}x^2, & x \\in [0, 2], \\\\\n",
        "0, & x \\notin [0, 2].\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Эта функция плотности вероятности описывает распределение значений случайной величины $X$, где $f(x)$ указывает вероятность попадания $X$ в малый интервал около точки $x$.\n",
        "\n",
        "**Задача:** Найти дисперсию $D(X)$.\n",
        "\n",
        "### Шаг 1: Математическое ожидание $E(X)$\n",
        "\n",
        "Математическое ожидание вычисляется как средневзвешенное значение $X$, где веса заданы функцией плотности вероятности $f(x)$:\n",
        "\n",
        "$$\n",
        "E(X) = \\int_{-\\infty}^{\\infty} x f(x) \\, dx.\n",
        "$$\n",
        "\n",
        "Так как $f(x) = 0$ вне интервала $[0, 2]$, пределы интегрирования сужаются к $[0, 2]$:\n",
        "\n",
        "$$\n",
        "E(X) = \\int_{0}^{2} x \\cdot \\frac{3}{8}x^2 \\, dx.\n",
        "$$\n",
        "\n",
        "**Объяснение формулы:**\n",
        "- $x$ — это значение случайной величины.\n",
        "- $f(x)$ — это вероятность плотности, взвешивающая $x$.\n",
        "- Умножение $x \\cdot f(x)$ даёт вклад каждого значения $x$ в общее среднее.\n",
        "\n",
        "Упростим выражение:\n",
        "\n",
        "$$\n",
        "E(X) = \\frac{3}{8} \\int_{0}^{2} x^3 \\, dx.\n",
        "$$\n",
        "\n",
        "Теперь вычислим интеграл $\\int_{0}^{2} x^3 \\, dx$. Для этого применим стандартное правило интегрирования:\n",
        "$$\n",
        "\\int x^n \\, dx = \\frac{x^{n+1}}{n+1} + C.\n",
        "$$\n",
        "\n",
        "### Правило интегрирования для степенной функции\n",
        "\n",
        "**Определение**\n",
        "\n",
        "Правило интегрирования функции вида $x^n$ формулируется следующим образом:\n",
        "\n",
        "$$\n",
        "\\int x^n \\, dx = \\frac{x^{n+1}}{n+1} + C,\n",
        "$$\n",
        "\n",
        "где:\n",
        "\n",
        "- $n$ — степень функции (должно быть $n \\neq -1$).\n",
        "- $C$ — произвольная константа интегрирования, которая добавляется из-за неопределенности постоянного слагаемого.\n",
        "\n",
        "Это правило является основным методом нахождения первообразной степенной функции.\n",
        "\n",
        "### Разбор формулы\n",
        "\n",
        "1. **Обозначение интеграла**: $\\int x^n \\, dx$ означает поиск функции, производная которой равна $x^n$.\n",
        "\n",
        "2. **Новый показатель степени**: В правой части формулы показатель степени увеличивается на $1$, что выражено как $n+1$. Таким образом, функция $x^n$ преобразуется в $x^{n+1}$.\n",
        "\n",
        "3. **Коэффициент перед $x^{n+1}$**: Чтобы учесть новое значение степени, результат делится на $n+1$. Это выражается как $\\frac{x^{n+1}}{n+1}$.\n",
        "\n",
        "4. **Константа интегрирования**: $C$ добавляется, чтобы отразить неопределенность, связанную с константами при нахождении первообразной. Производная любой константы равна $0$, поэтому интеграл включает эту неопределенность.\n",
        "\n",
        "### Особый случай $n = -1$\n",
        "\n",
        "Когда $n = -1$, функция $x^n$ становится $\\frac{1}{x}$, а стандартное правило неприменимо, так как $n+1 = 0$. В этом случае применяется другое правило:\n",
        "\n",
        "$$\n",
        "\\int \\frac{1}{x} \\, dx = \\ln|x| + C.\n",
        "$$\n",
        "\n",
        "**Разъяснение:**\n",
        "\n",
        "- Логарифм натуральный $\\ln|x|$ используется, так как производная функции $\\ln|x|$ равна $\\frac{1}{x}$.\n",
        "- Модуль $|x|$ обеспечивает правильность результата для отрицательных значений $x$, так как логарифм не определен для отрицательных чисел.\n",
        "\n",
        "### Примеры применения\n",
        "\n",
        "1. **Интеграл $x^2$:**\n",
        "\n",
        "$$\n",
        "\\int x^2 \\, dx = \\frac{x^{2+1}}{2+1} + C = \\frac{x^3}{3} + C.\n",
        "$$\n",
        "\n",
        "2. **Интеграл $x^{-3}$:**\n",
        "\n",
        "$$\n",
        "\\int x^{-3} \\, dx = \\frac{x^{-3+1}}{-3+1} + C = \\frac{x^{-2}}{-2} + C = -\\frac{1}{2x^2} + C.\n",
        "$$\n",
        "\n",
        "3. **Интеграл дробной степени $x^{1/2}$:**\n",
        "\n",
        "$$\n",
        "\\int x^{1/2} \\, dx = \\frac{x^{1/2+1}}{1/2+1} + C = \\frac{x^{3/2}}{3/2} + C = \\frac{2}{3}x^{3/2} + C.\n",
        "$$\n",
        "\n",
        "4. **Интеграл $\\frac{1}{x}$:**\n",
        "\n",
        "$$\n",
        "\\int \\frac{1}{x} \\, dx = \\ln|x| + C.\n",
        "$$\n",
        "\n",
        "### Применение в реальных задачах\n",
        "\n",
        "1. **Физика**: Нахождение скорости из ускорения, если оно выражено степенной функцией времени:\n",
        "   $$\n",
        "   a(t) = t^2, \\quad v(t) = \\int a(t) \\, dt = \\frac{t^3}{3} + C.\n",
        "   $$\n",
        "\n",
        "2. **Экономика**: Вычисление накопленной прибыли, если функция дохода растет по степенному закону.\n",
        "\n",
        "3. **Биология**: Описание роста клеток или популяций при изменении условий роста.\n",
        "\n",
        "### Свойства\n",
        "\n",
        "1. **Линейность интеграла:**\n",
        "\n",
        "$$\n",
        "\\int (a \\cdot f(x) + b \\cdot g(x)) \\, dx = a \\int f(x) \\, dx + b \\int g(x) \\, dx,\n",
        "$$\n",
        "\n",
        "где $a$ и $b$ — константы.\n",
        "\n",
        "2. **Инвариантность при сдвиге**: Интеграл функции $y = x + c$ имеет ту же форму:\n",
        "   $$\n",
        "   \\int y^n \\, dy = \\frac{y^{n+1}}{n+1} + C.\n",
        "   $$\n",
        "\n",
        "### Заключение\n",
        "\n",
        "Правило интегрирования степенной функции\n",
        "\n",
        "$$\n",
        "\\int x^n \\, dx = \\frac{x^{n+1}}{n+1} + C\n",
        "$$\n",
        "\n",
        "— это основополагающий инструмент для решения задач математического анализа. Его использование охватывает широкий спектр задач в физике, экономике и биологии, где требуется найти первообразные степенных функций.\n",
        "\n",
        "Подставим $n = 3$ и вычислим:\n",
        "\n",
        "$$\n",
        "\\int_{0}^{2} x^3 \\, dx = \\left[\\frac{x^4}{4}\\right]_0^2 = \\frac{2^4}{4} - \\frac{0^4}{4} = \\frac{16}{4} = 4.\n",
        "$$\n",
        "\n",
        "Подставим результат в формулу для $E(X)$:\n",
        "\n",
        "$$\n",
        "E(X) = \\frac{3}{8} \\cdot 4 = \\frac{12}{8} = 1.5.\n",
        "$$\n",
        "\n",
        "**Результат:**\n",
        "Математическое ожидание $E(X) = 1.5$.\n",
        "\n",
        "### Шаг 2: Математическое ожидание квадрата $E(X^2)$\n",
        "\n",
        "Математическое ожидание квадрата случайной величины $E(X^2)$ вычисляется аналогично, только вместо $x$ берётся $x^2$:\n",
        "\n",
        "$$\n",
        "E(X^2) = \\int_{-\\infty}^{\\infty} x^2 f(x) \\, dx.\n",
        "$$\n",
        "\n",
        "Снова учитываем, что $f(x) = 0$ вне интервала $[0, 2]$:\n",
        "\n",
        "$$\n",
        "E(X^2) = \\int_{0}^{2} x^2 \\cdot \\frac{3}{8}x^2 \\, dx = \\frac{3}{8} \\int_{0}^{2} x^4 \\, dx.\n",
        "$$\n",
        "\n",
        "**Объяснение формулы:**\n",
        "- $x^2$ теперь поднимает значение случайной величины в квадрат, что важно для расчёта разброса.\n",
        "- $f(x)$ по-прежнему взвешивает вклад каждого значения.\n",
        "\n",
        "Рассчитаем интеграл $\\int_{0}^{2} x^4 \\, dx$:\n",
        "\n",
        "$$\n",
        "\\int_{0}^{2} x^4 \\, dx = \\left[\\frac{x^5}{5}\\right]_0^2 = \\frac{2^5}{5} - \\frac{0^5}{5} = \\frac{32}{5}.\n",
        "$$\n",
        "\n",
        "Подставим значение интеграла в формулу для $E(X^2)$:\n",
        "\n",
        "$$\n",
        "E(X^2) = \\frac{3}{8} \\cdot \\frac{32}{5} = \\frac{96}{40} = 2.4.\n",
        "$$\n",
        "\n",
        "**Результат:**\n",
        "Математическое ожидание квадрата $E(X^2) = 2.4$.\n",
        "\n",
        "### Шаг 3: Дисперсия $D(X)$\n",
        "\n",
        "Дисперсия вычисляется по формуле:\n",
        "\n",
        "$$\n",
        "D(X) = E(X^2) - (E(X))^2.\n",
        "$$\n",
        "\n",
        "**Объяснение формулы:**\n",
        "- $E(X^2)$ показывает среднее значение квадратов отклонений.\n",
        "- $(E(X))^2$ корректирует это значение, учитывая квадрат среднего значения.\n",
        "\n",
        "Подставим найденные значения $E(X^2)$ и $E(X)$:\n",
        "\n",
        "$$\n",
        "D(X) = 2.4 - (1.5)^2 = 2.4 - 2.25 = 0.15.\n",
        "$$\n",
        "\n",
        "### Ответ:\n",
        "Дисперсия случайной величины $X$ равна:\n",
        "\n",
        "$$\n",
        "D(X) = 0.15.\n",
        "$$\n",
        "\n",
        "\n",
        "### Итог:\n",
        "Мы последовательно нашли математическое ожидание $E(X)$, квадрат математического ожидания $E(X^2)$ и рассчитали дисперсию $D(X)$.\n",
        "\n",
        "### Свойства дисперсии\n",
        "\n",
        "1. **Неотрицательность:**\n",
        "   $$ D(X) \\geq 0 $$\n",
        "\n",
        "2. **Дисперсия константы:**\n",
        "   $$ D(c) = 0 $$\n",
        "   где $c$ — константа\n",
        "\n",
        "3. **Дисперсия линейной функции:**\n",
        "   $$ D(aX + b) = a^2D(X) $$\n",
        "   где $a, b$ — константы\n",
        "\n",
        "4. **Дисперсия суммы независимых случайных величин:**\n",
        "   $$ D(X + Y) = D(X) + D(Y) $$\n",
        "   если $X$ и $Y$ независимы\n",
        "\n",
        "### Связь с нормальным распределением\n",
        "\n",
        "Для нормально распределенной случайной величины $X \\sim N(\\mu, \\sigma^2)$:\n",
        "- Дисперсия равна параметру $\\sigma^2$\n",
        "- Стандартное отклонение $\\sigma = \\sqrt{D(X)}$ определяет форму кривой плотности вероятности\n",
        "- Около 68% значений лежат в интервале $[\\mu - \\sigma, \\mu + \\sigma]$\n",
        "- Около 95% значений лежат в интервале $[\\mu - 2\\sigma, \\mu + 2\\sigma]$\n",
        "- Около 99.7% значений лежат в интервале $[\\mu - 3\\sigma, \\mu + 3\\sigma]$\n",
        "\n",
        "### Практическое значение\n",
        "\n",
        "Дисперсия играет важную роль в:\n",
        "1. Оценке разброса данных\n",
        "2. Проверке статистических гипотез\n",
        "3. Построении доверительных интервалов\n",
        "4. Анализе точности измерений\n",
        "5. Выявлении аномалий в данных\n"
      ],
      "metadata": {
        "id": "aWMwfcJ2igjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Пример небольшого датасета\n",
        "data = [10, 12, 23, 23, 16, 23, 21, 16]\n",
        "\n",
        "# Расчет дисперсии вручную\n",
        "mean = sum(data) / len(data)\n",
        "variance_manual = sum((x - mean) ** 2 for x in data) / len(data)\n",
        "\n",
        "# Расчет стандартного отклонения вручную\n",
        "std_deviation_manual = variance_manual ** 0.5\n",
        "\n",
        "# Расчет с использованием библиотеки numpy\n",
        "variance_np = np.var(data)\n",
        "std_deviation_np = np.std(data)\n",
        "\n",
        "# Вывод результатов\n",
        "print(\"Результаты расчетов:\")\n",
        "print(f\"Среднее значение: {mean}\")\n",
        "print(f\"Дисперсия (вручную): {variance_manual}\")\n",
        "print(f\"Дисперсия (numpy): {variance_np}\")\n",
        "print(f\"Стандартное отклонение (вручную): {std_deviation_manual}\")\n",
        "print(f\"Стандартное отклонение (numpy): {std_deviation_np}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UipQPFHsjS14",
        "outputId": "910ec96b-7fb7-44f5-9e21-aebdbdeb521f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Результаты расчетов:\n",
            "Среднее значение: 18.0\n",
            "Дисперсия (вручную): 24.0\n",
            "Дисперсия (numpy): 24.0\n",
            "Стандартное отклонение (вручную): 4.898979485566356\n",
            "Стандартное отклонение (numpy): 4.898979485566356\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **Вопрос 5: Комбинаторика. Формула числа перестановок, размещений, сочетаний (с повторениями).**\n",
        "\n",
        "### **Вопрос 6: Комбинаторика. Формула числа перестановок, размещений, сочетаний ( без повторений).**\n",
        "\n",
        "**Перестановки и Размещения в Задачах Комбинаторики**\n",
        "\n",
        "В данном фрагменте рассматривается тема перестановок и размещений, которая является основой комбинаторики. Комбинаторика — это область математики, изучающая различные способы упорядочивания и комбинирования элементов в множествах.\n",
        "\n",
        "**Перестановки без Повторений**\n",
        "\n",
        "Перестановки — это способы расположения всех элементов в множестве, когда порядок имеет значение. Например, для множества {A, B, C} возможные перестановки включают ABC, ACB, BAC, BCA, CAB и CBA. Общее количество перестановок $n$ элементов можно вычислить по формуле:\n",
        "\n",
        "**Формула числа перестановок без повторений:**\n",
        "  \n",
        "  $$\n",
        "  P(n) = n! = n \\times (n-1) \\times (n-2) \\times ... \\times 1\n",
        "  $$\n",
        "\n",
        "где $n!$ — факториал числа $n$.\n",
        "\n",
        "**Размещения без Повторений**\n",
        "\n",
        "Размещения — это способы выбрать и упорядочить $k$ элементов из множества $n$ элементов, когда порядок также имеет значение. Формула для расчета размещений записывается следующим образом:\n",
        "\n",
        "**Формула числа размещений без повторений:**\n",
        "  \n",
        "  $$\n",
        "  A(n, k) = \\frac{n!}{(n-k)!}\n",
        "  $$\n",
        "\n",
        "где:\n",
        "*   $A(n, k)$ — количество размещений;\n",
        "*   $n$ — общее количество элементов;\n",
        "*   $k$ — количество элементов, которые нужно выбрать и расположить.\n",
        "\n",
        "**Перестановки с Повторениями**\n",
        "\n",
        "В этом фрагменте мы разберем понятие перестановок с повторениями, которое возникает, когда один или несколько элементов могут повторяться в последовательности. Это довольно важное понятие в комбинаторике, особенно в ситуациях, когда мы имеем дело с множеством одинаковых или однотипных объектов.\n",
        "\n",
        "Например, если у нас есть набор букв {A, A, B}, то перестановки этих букв будут включать такие последовательности, как AAB, ABA и BAA. Однако, в этом случае, необходимо учитывать, что буквы A повторяются. Это влияет на общее количество уникальных перестановок, которые мы можем сформировать.\n",
        "\n",
        "**Формула числа перестановок с повторениями:**\n",
        "\n",
        "Количество способов перестановки $n$ объектов, из которых $n_1$ объектов первого типа, $n_2$ объектов второго типа и так далее, может быть вычислено с помощью формулы:\n",
        "\n",
        "  $$\n",
        "  P(n; n_1, n_2, ..., n_k) = \\frac{n!}{n_1! \\cdot n_2! \\cdots n_k!}\n",
        "  $$\n",
        "\n",
        "где:\n",
        "*   $P(n; n_1, n_2, ..., n_k)$ — количество уникальных перестановок $n$ элементов с повторениями;\n",
        "*   $n!$ — факториал общего количества всех элементов;\n",
        "*   $n_1!$, $n_2!$, ..., $n_k!$ — факториалы количества каждого типа элементов.\n",
        "\n",
        "Используя эту формулу, мы можем определить общее число уникальных перестановок в наборе с повторяющимися элементами.\n",
        "\n",
        "**Размещения с Повторениями**\n",
        "\n",
        "Размещения с повторениями — это выбор $k$ элементов из $n$ с учетом порядка, при этом элементы могут повторяться. Представьте, что у вас есть $n$ различных вариантов для каждого из $k$ мест.\n",
        "\n",
        "**Формула числа размещений с повторениями:**\n",
        "  \n",
        "  $$\n",
        "  \\bar{A}(n, k) = n^k\n",
        "  $$\n",
        "\n",
        "где:\n",
        "*   $\\bar{A}(n, k)$ — количество размещений с повторениями;\n",
        "*   $n$ — количество вариантов для каждого выбора;\n",
        "*   $k$ — количество выборов.\n",
        "\n",
        "**Сочетания Элементов без Повторений**\n",
        "\n",
        "Предыдущий контекст: В предыдущем обсуждении говорилось о размещении элементов, где порядок имеет значение, и была рассмотрена формула для вычисления количества размещений.\n",
        "\n",
        "**Сочетания без Повторений**\n",
        "\n",
        "Теперь мы переходим к понятию сочетаний, которое является еще одной важной концепцией в комбинаторике. Сочетания используются, когда важен только состав выбранных элементов, а порядок их не имеет значения. Например, если у нас есть набор из различных видов фруктов и мы хотим выбрать два фрукта, нас не интересует, в каком порядке мы их выбираем, только какие фрукты попадают в выборку.\n",
        "\n",
        "**Формула числа сочетаний без повторений:**\n",
        "\n",
        "Количество способов выбрать $k$ элементов из $n$ без учета их порядка вычисляется по формуле сочетаний:\n",
        "\n",
        "  $$\n",
        "  C(n, k) = \\frac{n!}{k! \\cdot (n - k)!}\n",
        "  $$\n",
        "\n",
        "где:\n",
        "*   $C(n, k)$ — количество сочетаний, равное числу способов выбрать $k$ элементов из $n$;\n",
        "*   $n!$ — факториал числа $n$, равный произведению всех целых чисел от 1 до $n$;\n",
        "*   $k!$ — факториал числа $k$, равный произведению всех целых чисел от 1 до $k$;\n",
        "*   $(n - k)!$ — факториал разности между общим числом элементов и количеством выбираемых элементов.\n",
        "\n",
        "**Сочетания с Повторениями**\n",
        "\n",
        "Сочетания с повторениями — это выбор $k$ элементов из $n$ возможных, где порядок не важен, и элементы могут повторяться. Представьте, что у вас есть $n$ видов конфет, и вы хотите выбрать $k$ конфет (не обязательно разных видов).\n",
        "\n",
        "**Формула числа сочетаний с повторениями:**\n",
        "  \n",
        "  $$\n",
        "  \\bar{C}(n, k) = C(n + k - 1, k) = \\frac{(n + k - 1)!}{k! \\cdot (n - 1)!}\n",
        "  $$\n",
        "  \n",
        "где:\n",
        "*   $\\bar{C}(n, k)$ — количество сочетаний с повторениями;\n",
        "*   $n$ — количество типов элементов;\n",
        "*   $k$ — количество выбираемых элементов.\n",
        "\n",
        "**Ответ на вопросы:**\n",
        "\n",
        "**Вопрос 5: Комбинаторика. Формула числа перестановок, размещений, сочетаний (с повторениями).**\n",
        "\n",
        "*   **Перестановки с повторениями:** $P(n; n_1, n_2, ..., n_k) = \\frac{n!}{n_1! \\cdot n_2! \\cdots n_k!}$\n",
        "*   **Размещения с повторениями:** $\\bar{A}(n, k) = n^k$\n",
        "*   **Сочетания с повторениями:** $\\bar{C}(n, k) = C(n + k - 1, k) = \\frac{(n + k - 1)!}{k! \\cdot (n - 1)!}$\n",
        "\n",
        "**Вопрос 6: Комбинаторика. Формула числа перестановок, размещений, сочетаний (без повторений).**\n",
        "\n",
        "*   **Перестановки без повторений:** $P(n) = n!$\n",
        "*   **Размещения без повторений:** $A(n, k) = \\frac{n!}{(n-k)!}$\n",
        "*   **Сочетания без повторений:** $C(n, k) = \\frac{n!}{k! \\cdot (n - k)!}$\n"
      ],
      "metadata": {
        "id": "jcndgNmjjerL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **Вопрос 7: Случайные события (случайное, достоверное, невозможное). Вероятность.**\n",
        "\n",
        "### **Вопрос 9: Классическая  формула  вероятности. Свойства вероятности.**\n",
        "\n",
        "## **Случайные события и их классификация**\n",
        "\n",
        "Вероятность – это мера возможности возникновения определенного события в результате случайного эксперимента. События могут быть классифицированы на достоверные, невозможные и случайные.\n",
        "\n",
        "- **Достоверные события** – это события, которые обязательно произойдут. Например, при броске игрального кубика всегда выпадет число от 1 до 6.\n",
        "\n",
        "- **Невозможные события** – события, которые не могут произойти. Например, выпадение числа 7 на игральном кубике.\n",
        "\n",
        "Таким образом, вероятность достоверного события равна 1, а для невозможного – 0. Разберем это на простых примерах из игры с кубиками. При броске игрального кубика, события «выпадение четного числа» могут быть представлены как {2, 4, 6}, что делает вероятности их выпадения актуальными для анализа. Вероятность наступления события может быть логически определена как:\n",
        "\n",
        "$$\n",
        "P(E) = \\frac{n(E)}{n(S)}\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $P(E)$ – вероятность события $E$,\n",
        "- $n(E)$ – количество благоприятных исходов для события $E$,\n",
        "- $n(S)$ – общее количество исходов.\n",
        "\n",
        "Для события «выпадение четного числа», $n(E) = 3$, а общее количество исходов $n(S) = 6$, следовательно, вероятность:\n",
        "\n",
        "$$\n",
        "P(E) = \\frac{3}{6} = \\frac{1}{2}\n",
        "$$\n",
        "\n",
        "## **Операции над событиями**\n",
        "\n",
        "Существует несколько основных операций, которые можно выполнять над событиями, связанными с одним и тем же экспериментом:\n",
        "\n",
        "1. **Объединение событий (A ∪ B)** – событие, которое происходит, если происходит хотя бы одно из событий A или B. Например, если событие A – это выпадение 2, а событие B – выпадение 4, то объединение событий A и B – это наступление события, в котором подбрасывается 2 или 4.\n",
        "\n",
        "$$\n",
        "P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\n",
        "$$\n",
        "\n",
        "2. **Пересечение событий (A ∩ B)** – событие, которое происходит, только если оба события A и B происходят одновременно. Например, если нужно выяснить вероятность того, что на кубике выпало четное число, и это число больше трех (выпадают 4 или 6).\n",
        "\n",
        "$$\n",
        "P(A \\cap B) = P(A) \\cdot P(B|A)\n",
        "$$\n",
        "\n",
        "3. **Дополнение события (A') или (!A)** – это событие, при котором не происходит событие A. Например, если событие A — это выпадение четного числа (2, 4, 6), то дополнение A – это выпадение нечетного числа (1, 3, 5).\n",
        "\n",
        "$$\n",
        "P(A') = 1 - P(A)\n",
        "$$\n",
        "\n",
        "**Интересный факт**\n",
        "> Случайные события всегда имеют погрешность\n",
        "\n",
        "Рассмотрим бросок монеты. Теоретически, вероятность выпадения орла или решки равна 0.5. Однако на практике результат броска может быть подвержен случайным флуктуациям, таким как небольшие изменения в силе броска, положении монеты в воздухе или ее вращении. Эти флуктуации могут привести к тому, что монета упадет на одну из сторон не в соответствии с теоретической вероятностью. Например, при серии из 10 бросков монета может выпасть 7 раз орлом и 3 раза решкой, что является результатом случайных флуктуаций, а не систематической ошибки.\n",
        "\n",
        "**Случайные флуктуации** — это нерегулярные, непредсказуемые изменения в результатах случайного процесса, вызванные множеством неконтролируемых факторов. Эти изменения могут приводить к отклонениям от ожидаемых значений или вероятностных распределений. Случайные флуктуации характерны для систем, где действует элемент случайности, например, в физических, биологических, экономических или социальных процессах. Они играют ключевую роль в определении точности и надежности результатов экспериментов или наблюдений.\n",
        "\n",
        "\n",
        "## **Классическая формула вероятности и её свойства**\n",
        "\n",
        "### 1. Классическая формула вероятности\n",
        "Классическая формула вероятности используется, когда рассматривается конечное множество элементарных событий, где все события равновероятны. Если $m$ — количество благоприятных событий для некоторого события A, и $n$ — общее количество возможных элементарных событий в пространстве, то вероятность события A вычисляется по формуле:\n",
        "\n",
        "$$\n",
        "P(A) = \\frac{m}{n}\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $P(A)$ — вероятность события A,\n",
        "- $m$ — количество успешных исходов (благоприятные события),\n",
        "- $n$ — общее количество исходов (все события).\n",
        "\n",
        "Это означает, что вероятность события — это отношение числа благоприятных исходов к общему количеству возможных исходов.\n",
        "\n",
        "### 2. Свойства вероятности\n",
        "Для любых событий A и B действуют следующие свойства:\n",
        "\n",
        "- **Неотрицательность вероятности**: Для любого события A выполняется $P(A) \\geq 0$.\n",
        "- **Нормированность вероятности**: Вероятность достоверного события равна 1: $P(\\Omega) = 1$, где $Ω$ — это полное множество элементарных событий.\n",
        "- **Аддитивность вероятности**: Если A и B — несовместные события (нет общих исходов), то:\n",
        "\n",
        "$$\n",
        "P(A \\cup B) = P(A) + P(B)\n",
        "$$\n",
        "\n",
        "Если же события A и B пересекаются, то для их объединения необходимо вычесть вероятность пересечения событий A и B:\n",
        "\n",
        "$$\n",
        "P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\n",
        "$$\n",
        "\n",
        "### Примеры вычисления вероятности\n",
        "\n",
        "1. **Пример на простых событиях**:\n",
        "   На экзамене студенту предлагается 12 билетов (2 сложных и 10 легких). Найти вероятность того, что попадется легкий билет.\n",
        "\n",
        "   Обозначим:\n",
        "   - $m = 10$ (количество легких билетов),\n",
        "   - $n = 12$ (общее количество билетов).\n",
        "\n",
        "   Тогда вероятность того, что попадется легкий билет:\n",
        "\n",
        "$$\n",
        "P(легкий) = \\frac{10}{12} = \\frac{5}{6}\n",
        "$$\n",
        "\n",
        "2. **Задача о днях рождения**:\n",
        "   В группе из 30 человек возникает вопрос, какая вероятность, что хотя бы двое из них имеют одинаковый день рождения. Для решения этой задачи мы используем обратный подход — сначала найдем вероятность, что у всех разные дни рождения.\n",
        "\n",
        "   Для первого человека вероятность рождения в выбранный день составляет 1. Для второго человека вероятность, что он родился не в этот день, будет $\\frac{364}{365}$. Для третьего — $\\frac{363}{365}$ и так далее. Общая вероятность, что все 30 человек имеют разные дни рождения, вычисляется как:\n",
        "\n",
        "$$\n",
        "P(разные) = 1 \\cdot \\frac{364}{365} \\cdot \\frac{363}{365} \\cdots \\frac{336}{365}\n",
        "$$\n",
        "\n",
        "Вероятность того, что хотя бы двое имеют одинаковый день рождения равна:\n",
        "\n",
        "$$\n",
        "P(одинаковые) = 1 - P(разные)\n",
        "$$\n",
        "\n"
      ],
      "metadata": {
        "id": "VkEliL8Hqqkl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### **Вопрос 8: Операции над случайными событиями. Диаграммы Венна — Эйлера.**\n",
        "\n",
        "## **Операции над событиями**\n",
        "\n",
        "Существует несколько основных операций, которые можно выполнять над событиями, связанными с одним и тем же экспериментом:\n",
        "\n",
        "1. **Объединение событий (A ∪ B)** – событие, которое происходит, если происходит хотя бы одно из событий A или B. Например, если событие A – это выпадение 2, а событие B – выпадение 4, то объединение событий A и B – это наступление события, в котором подбрасывается 2 или 4.\n",
        "\n",
        "$$\n",
        "P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\n",
        "$$\n",
        "\n",
        "2. **Пересечение событий (A ∩ B)** – событие, которое происходит, только если оба события A и B происходят одновременно. Например, если нужно выяснить вероятность того, что на кубике выпало четное число, и это число больше трех (выпадают 4 или 6).\n",
        "\n",
        "$$\n",
        "P(A \\cap B) = P(A) \\cdot P(B|A)\n",
        "$$\n",
        "\n",
        "3. **Дополнение события (A') или (!A)** – это событие, при котором не происходит событие A. Например, если событие A — это выпадение четного числа (2, 4, 6), то дополнение A – это выпадение нечетного числа (1, 3, 5).\n",
        "\n",
        "$$\n",
        "P(A') = 1 - P(A)\n",
        "$$\n",
        "\n",
        "## **Дополнение события и полные группы событий**\n",
        "\n",
        "В этой секции мы сосредоточимся на понятии дополнения событий, несовместных событиях и понятии полной группы событий в теории вероятностей.\n",
        "\n",
        "1. **Дополнение события** – это участок вероятностного пространства, который не включает в себя данное событие. Например, если событие A – это выпадение четного числа при броске игрового кубика (2, 4, 6), то дополнением A будет выпадение нечетного числа (1, 3, 5). Обозначается это как $A'$ и характеризуется полной вероятностью: если $P(A)$ — вероятность события A, то\n",
        "\n",
        "$$\n",
        "P(A') = 1 - P(A)\n",
        "$$\n",
        "\n",
        "где $P(A')$ — вероятность наступления события, не сопоставимого с A.\n",
        "\n",
        "2. **Несовместные события** – это события, которые не могут произойти одновременно. Например, если A — событие «выпало четное число», а B — событие «выпало нечетное число», то пересечение A и B (обозначаемое как $A \\cap B$) равно пустому множеству. В таком случае\n",
        "\n",
        "$$\n",
        "P(A \\cap B) = 0\n",
        "$$\n",
        "\n",
        "3. **Полные группы событий** образуются, когда событие H1, H2, ..., Hn являются несовместными и в совокупности охватывают все возможные исходы эксперимента. Другими словами, их объединение дает достоверное событие:\n",
        "\n",
        "$$\n",
        "P(H1 \\cup H2 \\cup ... \\cup Hn) = 1\n",
        "$$\n",
        "\n",
        "Примером полной группы событий может служить бросок игрального кубика, где H1 – выпало четное (2, 4, 6), H2 – выпало нечетное (1, 3, 5).\n",
        "\n",
        "## **Соотношение событий и диаграмма Венна-Эйлера**\n",
        "\n",
        "Диаграмма Венна-Эйлера используется для визуального представления вероятностных событий и их группировок. Она позволяет наглядно увидеть, как различные события могут пересекаться или объединяться.\n",
        "\n",
        "- Если событие A влечет за собой событие B, это значит, что все элементарные события, входящие в A, также входят в B. Например, если на кубике выпало число больше трех, это также подразумевает, что выпало число больше двух.\n"
      ],
      "metadata": {
        "id": "QKWhmxX4rUWW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### **Вопрос 10: Условная вероятность.**\n",
        "\n",
        "## **Условные вероятности**\n",
        "\n",
        "Условная вероятность — это вероятность наступления события A при условии, что произошло событие B. Обозначается это как $P(A|B)$ и может быть формально определена следующей формулой:\n",
        "\n",
        "$$\n",
        "P(A|B) = \\frac{P(A \\cap B)}{P(B)}\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $P(A|B)$ — условная вероятность события A при условии, что событие B произошло,\n",
        "- $P(A \\cap B)$ — вероятность того, что произойдут оба события A и B,\n",
        "- $P(B)$ — вероятность события B.\n",
        "\n",
        "### Пример\n",
        "Представим, что у нас есть кубик, и мы хотим узнать вероятность того, что выпало число 2, при условии, что результат броска не больше 5. В этом случае:\n",
        "\n",
        "1. Событие A — это выпадение 2.\n",
        "2. Событие B — это результат броска не больше 5.\n",
        "\n",
        "Сначала найдем $P(A \\cap B)$ — вероятность того, что выпало число 2 и результат был не больше 5 (это возможно, так как 2 ≤ 5, поэтому это событие не пустое). Вероятность выпадения 2 на стандартном кубике равна $P(A) = \\frac{1}{6}$. Далее, событие B включает в себя 1, 2, 3, 4 и 5, следовательно, $P(B) = \\frac{5}{6}$. Теперь можем найти условную вероятность:\n",
        "\n",
        "$$\n",
        "P(A|B) = \\frac{P(A \\cap B)}{P(B)} = \\frac{\\frac{1}{6}}{\\frac{5}{6}} = \\frac{1}{5}\n",
        "$$\n",
        "\n",
        "### Свойства условной вероятности\n",
        "\n",
        "1. **Неотрицательность**: Если вероятность события B больше нуля, то условная вероятность события A при условии B также больше или равна нулю:\n",
        "   $$\n",
        "   P(A|B) \\geq 0 \\quad если \\quad P(B) > 0\n",
        "   $$\n",
        "\n",
        "2. **Нормированность**: Вероятность достоверного события при условии B равна 1, так как если B уже произошло, то достоверное событие также произойдет:\n",
        "   $$\n",
        "   P(B|B) = 1\n",
        "   $$\n",
        "\n",
        "3. **Аддитивность**: Если события A и C несовместны (то есть они не могут произойти одновременно), то вероятность их объединения условно при событии B равна:\n",
        "   $$\n",
        "   P(A \\cup C | B) = P(A|B) + P(C|B) \\quad при \\quad P(B) > 0\n",
        "   $$\n",
        "\n",
        "### Формула умножения вероятностей\n",
        "Условная вероятность часто используется в качестве составной части формулы умножения вероятностей:\n",
        "\n",
        "$$\n",
        "P(A \\cap B) = P(A|B) \\cdot P(B)\n",
        "$$\n",
        "\n",
        "Эта формула гласит, что вероятность того, что одновременно произойдут события A и B, может быть найдена путем умножения условной вероятности A при условии B на вероятность B."
      ],
      "metadata": {
        "id": "WnvqF82X1gKZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **Вопрос 11: Формула Байеса.**\n",
        "\n",
        "## **Формула полной вероятности и формула Байеса**\n",
        "\n",
        "### 1. Формула полной вероятности (при условии: когда один исход зависит от другого)\n",
        "\n",
        "**Формула полной вероятности используется для расчета вероятности события A, основываясь на разбиении пространства вероятностей на несколько несовместных событий.**\n",
        "\n",
        "Пусть события $H_1, H_2, \\ldots, H_n$ образуют полную группу и имеют положительные вероятности. Тогда вероятность события A может быть выражена как сумма вероятностей условной вероятности A при каждом событии $H_i$, умноженной на вероятность самого события $H_i$:\n",
        "\n",
        "$$\n",
        "P(A) = \\sum_{i=1}^{n} P(A|H_i) \\cdot P(H_i)\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $P(A|H_i)$ — условная вероятность A при условии $H_i$,\n",
        "- $P(H_i)$ — вероятность события $H_i$.\n",
        "\n",
        "### Пример объяснения\n",
        "Представьте, что у нас есть шестигранный кубик. Вероятность выпадения определенного числа (например, 1, 2, 3, 4, 5 или 6) в каждом броске равна $\\frac{1}{6}$. Если мы хотим рассмотреть вероятность того, что на кубике выпало четное число (2, 4, или 6), то будем суммировать условные вероятности:\n",
        "\n",
        "$$\n",
        "P(четное) = P(четное|выпало 2) \\cdot P(выпало 2) + P(четное|выпало 4) \\cdot P(выпало 4) + P(четное|выпало 6) \\cdot P(выпало 6)\n",
        "$$\n",
        "\n",
        "\n",
        "Для расчета вероятности выпадения четного числа на игральной кости используем формулу полной вероятности:\n",
        "\n",
        "$$\n",
        "P(A) = \\sum_{i=1}^{n} P(A|H_i) \\cdot P(H_i)\n",
        "$$\n",
        "\n",
        "**Шаг 1: Определим вероятности выпадения каждого числа**\n",
        "\n",
        "На стандартной шестигранной кости вероятность выпадения любого числа равна:\n",
        "\n",
        "$$\n",
        "P(2) = P(4) = P(6) = \\frac{1}{6}\n",
        "$$\n",
        "\n",
        "**Шаг 2: Определим условные вероятности**\n",
        "\n",
        "Поскольку 2, 4 и 6 — это четные числа, то:\n",
        "\n",
        "$$\n",
        "P(\\text{четное}|2) = 1, \\quad P(\\text{четное}|4) = 1, \\quad P(\\text{четное}|6) = 1\n",
        "$$\n",
        "\n",
        "**Шаг 3: Подставим значения в формулу**\n",
        "\n",
        "Используем формулу (1):\n",
        "\n",
        "$$\n",
        "P(\\text{четное}) = 1 \\cdot \\frac{1}{6} + 1 \\cdot \\frac{1}{6} + 1 \\cdot \\frac{1}{6} = \\frac{1}{6} + \\frac{1}{6} + \\frac{1}{6} = \\frac{3}{6} = \\frac{1}{2}\n",
        "$$\n",
        "\n",
        "**Итоговая вероятность:**\n",
        "\n",
        "$$\n",
        "\\boxed{\\dfrac{1}{2}}\n",
        "$$\n",
        "\n",
        "\n",
        "### 2. Формула Байеса\n",
        "Формула Байеса позволяет вычислить вероятность события $( A )$ при условии, что произошло событие $( B )$, с учетом априорных вероятностей и условных вероятностей:\n",
        "\n",
        "$$\n",
        "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n",
        "$$\n",
        "\n",
        "#### Применение формулы Байеса к задаче с игральной костью\n",
        "\n",
        "Предположим, мы хотим найти вероятность того, что выпало четное число (событие \\( A \\)), если известно, что выпало число, кратное 3 (событие $( B )$).\n",
        "\n",
        "**Шаг 1: Определим вероятности**\n",
        "\n",
        "- $( P(A) )$ — вероятность выпадения четного числа. На шестигранной кости четные числа — это 2, 4, 6. Таким образом:\n",
        "\n",
        "$$\n",
        "P(A) = \\frac{3}{6} = \\frac{1}{2}\n",
        "$$\n",
        "\n",
        "- $( P(B) )$ — вероятность выпадения числа, кратного 3. На шестигранной кости числа, кратные 3, — это 3 и 6. Таким образом:\n",
        "\n",
        "$$\n",
        "P(B) = \\frac{2}{6} = \\frac{1}{3}\n",
        "$$\n",
        "\n",
        "**Шаг 2: Определим условные вероятности**\n",
        "\n",
        "- $( P(B|A) )$ — вероятность того, что выпало число, кратное 3, при условии, что выпало четное число. Среди четных чисел (2, 4, 6) только 6 кратно 3. Таким образом:\n",
        "\n",
        "$$\n",
        "P(B|A) = \\frac{1}{3}\n",
        "$$\n",
        "\n",
        "**Шаг 3: Подставим значения в формулу Байеса**\n",
        "\n",
        "Используем формулу (1):\n",
        "\n",
        "$$\n",
        "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} = \\frac{\\frac{1}{3} \\cdot \\frac{1}{2}}{\\frac{1}{3}} = \\frac{1}{2}\n",
        "$$\n",
        "\n",
        "**Итоговая вероятность:**\n",
        "\n",
        "$$\n",
        "\\boxed{\\dfrac{1}{2}}\n",
        "$$\n",
        "\n",
        "### Итог\n",
        "\n",
        "- **Формула полной вероятности** используется для вычисления вероятности события $( A )$ с учетом всех возможных гипотез $( H_i )$.\n",
        "- **Формула Байеса** позволяет \"обновлять\" вероятности гипотез $( H_i )$ после наступления события $( A )$, основываясь на условных вероятностях и априорных данных.\n",
        "- Термин \"обновлять вероятности гипотез\" означает, что вероятности гипотез $( H_i )$ могут изменяться динамически в зависимости от наступления события $( A )$. Это ключевая идея формулы Байеса."
      ],
      "metadata": {
        "id": "GeaQUFmY3zMO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **Вопрос 12: Случайные величины (определение, примеры).**\n",
        "\n",
        "### **Вопрос 13: Функция распределения случайной величины. Привести примеры наиболее известных распределений.**\n",
        "\n",
        "## **Непрерывные случайные величины**\n",
        "\n",
        "Непрерывная случайная величина $X$ — это случайная величина, множество возможных значений которой несчетно. Это означает, что для любых двух различных значений $a$ и $b$, где $a < b$, случайная величина $X$ может принимать любое значение между $a$ и $b$. Формально, ее пространство элементарных событий $\\Omega$ связано с множеством действительных чисел $\\mathbb{R}$ (или его интервалом).\n",
        "\n",
        "**Функция распределения случайной велечины (Cumulative Distribution Function, CDF):**\n",
        "\n",
        "Функция распределения непрерывной случайной величины $X$, обозначаемая как $F_X(x)$, определяется как вероятность того, что случайная величина $X$ примет значение, меньшее или равное $x$:\n",
        "\n",
        "$$\n",
        "F_X(x) = P(X \\leq x)\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $F(x)$ — функция распределения,\n",
        "- $P$ — вероятность,\n",
        "- $X$ — непрерывная случайная величина,\n",
        "- $x$ — определенное значение.\n",
        "\n",
        "Благодаря этой функции мы можем отслеживать, как вероятность аккумулируется, когда мы движемся вдоль числовой оси от меньших значений к большим. Так, функция $F(x)$ всегда возрастает и принимается на границах 0 и 1, что соответствует нулевой вероятности на самом низком значении и полной вероятности на самом высоком. Проще говоря, по мере увеличения 𝑥 вероятность того, что случайная величина X примет значение меньше или равно x, будет увеличиваться.\n",
        "\n",
        "**Свойства функции распределения:**\n",
        "\n",
        "1. $0 \\leq F_X(x) \\leq 1$ для всех $x \\in \\mathbb{R}$.\n",
        "2. $\\lim_{x \\to -\\infty} F_X(x) = 0$.\n",
        "3. $\\lim_{x \\to +\\infty} F_X(x) = 1$.\n",
        "4. $F_X(x)$ является неубывающей функцией, то есть если $a < b$, то $F_X(a) \\leq F_X(b)$.\n",
        "5. $F_X(x)$ является непрерывной функцией.\n",
        "\n",
        "Вероятность того, что случайная величина $X$ примет значение в интервале $(a, b]$, выражается через функцию распределения:\n",
        "\n",
        "$$\n",
        "P(a < X \\leq b) = F_X(b) - F_X(a)\n",
        "$$\n",
        "\n",
        "**Функция плотности вероятности (Probability Density Function, PDF):**\n",
        "\n",
        "Для непрерывной случайной величины существует функция плотности вероятности (если она существует), обозначаемая как $f_X(x)$, которая является производной функции распределения:\n",
        "\n",
        "$$\n",
        "f_X(x) = \\frac{d}{dx} F_X(x)\n",
        "$$\n",
        "\n",
        "где производная берется в тех точках, где она существует.\n",
        "\n",
        "**Свойства функции плотности вероятности:**\n",
        "\n",
        "1. $f_X(x) \\geq 0$ для всех $x \\in \\mathbb{R}$.\n",
        "2. Интеграл функции плотности вероятности по всему пространству элементарных событий равен 1:\n",
        "    $$\n",
        "    \\int_{-\\infty}^{\\infty} f_X(x) \\, dx = 1\n",
        "    $$\n",
        "3. Вероятность того, что случайная величина $X$ примет значение в интервале $(a, b)$, вычисляется как интеграл функции плотности вероятности на этом интервале:\n",
        "    $$\n",
        "    P(a < X < b) = \\int_a^b f_X(x) \\, dx\n",
        "    $$\n",
        "\n",
        "**Важные замечания:**\n",
        "\n",
        "*   Для непрерывной случайной величины вероятность того, что она примет конкретное значение, равна нулю:\n",
        "    $$\n",
        "    P(X = c) = \\int_c^c f_X(x) \\, dx = 0\n",
        "    $$\n",
        "    Это связано с тем, что интеграл по точке равен нулю. Следовательно, для непрерывных случайных величин не имеет значения, являются ли границы интервала включенными или исключенными:\n",
        "    $$\n",
        "    P(a < X < b) = P(a \\leq X < b) = P(a < X \\leq b) = P(a \\leq X \\leq b) = \\int_a^b f_X(x) \\, dx\n",
        "    $$\n",
        "*   Функция плотности вероятности $f_X(x)$ не является вероятностью. Она представляет собой плотность вероятности в точке $x$. Вероятность определяется интегралом плотности вероятности по интервалу.\n",
        "\n",
        "**Связь между CDF и PDF:**\n",
        "\n",
        "Функция распределения может быть выражена через функцию плотности вероятности с помощью интеграла:\n",
        "\n",
        "$$\n",
        "F_X(x) = \\int_{-\\infty}^{x} f_X(t) \\, dt\n",
        "$$\n",
        "\n",
        "где $t$ — переменная интегрирования.\n",
        "\n",
        "**Пример:**\n",
        "\n",
        "Рассмотрим равномерное распределение на интервале $[0, 1]$. Функция плотности вероятности задается как:\n",
        "\n",
        "$$\n",
        "f_X(x) = \\begin{cases}\n",
        "1, & 0 \\leq x \\leq 1 \\\\\n",
        "0, & \\text{иначе}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Функция распределения для этого случая будет:\n",
        "\n",
        "$$\n",
        "F_X(x) = \\begin{cases}\n",
        "0, & x < 0 \\\\\n",
        "\\int_0^x 1 \\, dt = x, & 0 \\leq x \\leq 1 \\\\\n",
        "1, & x > 1\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "**Физический и геометрический смысл**\n",
        "\n",
        "В физике непрерывные случайные величины часто применяются для моделирования явлений, таких как распределение скоростей молекул в газах или время, необходимое для реакции в химических процессах. Рассмотрим пример: если мы хотим рассчитать вероятность того, что молекула газа имеет скорость между 200 м/с и 300 м/с, мы можем использовать функцию плотности вероятности для нахождения интеграла за данным диапазоном. Это позволяет нам визуализировать распределение скоростей и понять, как они связаны с макроскопическими свойствами газа, такими как температура и давление.\n",
        "\n",
        "**Заключение:**\n",
        "\n",
        "Математическая формализация непрерывных случайных величин опирается на понятия функции распределения и функции плотности вероятности. Функция распределения описывает вероятность того, что случайная величина не превысит заданное значение, а функция плотности вероятности описывает плотность вероятности в каждой точке. Интегральное исчисление играет ключевую роль в работе с непрерывными случайными величинами, позволяя вычислять вероятности попадания в заданные интервалы."
      ],
      "metadata": {
        "id": "N-Qz61Ra3_gI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **Вопрос 17: Плотность распределения. Свойства.**\n",
        "\n",
        "**Плотность распределения** — это функция, которая описывает распределение вероятностей непрерывной случайной величины. Она показывает, как вероятность сосредоточена в различных областях значений случайной величины. Плотность распределения часто обозначается как $f(x)$.\n",
        "\n",
        "### Определение\n",
        "Функция $f(x)$ называется **плотностью распределения** случайной величины $X$, если для любого интервала $[a, b]$ вероятность того, что $X$ примет значение из этого интервала, равна интегралу от $f(x)$ по этому интервалу:\n",
        "\n",
        "$$\n",
        "P(a \\leq X \\leq b) = \\int_a^b f(x) \\, dx.\n",
        "$$\n",
        "\n",
        "### Свойства плотности распределения\n",
        "1. **Неотрицательность**:\n",
        "   $$\n",
        "   f(x) \\geq 0 \\quad \\text{для всех } x.\n",
        "   $$\n",
        "   Плотность распределения не может принимать отрицательные значения.\n",
        "\n",
        "2. **Нормированность**:\n",
        "   $$\n",
        "   \\int_{-\\infty}^{+\\infty} f(x) \\, dx = 1.\n",
        "   $$\n",
        "   Полная площадь под графиком плотности распределения равна 1, что соответствует тому, что вероятность всех возможных значений случайной величины равна 1.\n",
        "\n",
        "3. **Вероятность в точке**:\n",
        "   Для непрерывной случайной величины вероятность того, что она примет конкретное значение $x$, равна нулю:\n",
        "   $$\n",
        "   P(X = x) = 0.\n",
        "   $$\n",
        "   Вероятность определяется только для интервалов.\n",
        "\n",
        "4. **Связь с функцией распределения**:\n",
        "   Плотность распределения $f(x)$ связана с функцией распределения $F(x)$ следующим образом:\n",
        "   $$\n",
        "   F(x) = \\int_{-\\infty}^x f(t) \\, dt.\n",
        "   $$\n",
        "   Функция распределения $F(x)$ — это интеграл от плотности распределения.\n",
        "\n",
        "5. **Непрерывность**:\n",
        "   Плотность распределения $f(x)$ может быть непрерывной или кусочно-непрерывной функцией.\n",
        "\n",
        "### Пример\n",
        "Пусть $X$ — случайная величина с плотностью распределения:\n",
        "$$\n",
        "f(x) = \\begin{cases}\n",
        "2x, & \\text{если } 0 \\leq x \\leq 1, \\\\\n",
        "0, & \\text{иначе}.\n",
        "\\end{cases}\n",
        "$$\n",
        "Тогда:\n",
        "- $f(x) \\geq 0$ для всех $x$.\n",
        "- $\\int_{-\\infty}^{+\\infty} f(x) \\, dx = \\int_0^1 2x \\, dx = 1$.\n",
        "- Вероятность $P(0.5 \\leq X \\leq 0.8) = \\int_{0.5}^{0.8} 2x \\, dx = 0.8^2 - 0.5^2 = 0.39$.\n",
        "\n",
        "Плотность распределения является важным инструментом для анализа непрерывных случайных величин и нахождения вероятностей.\n",
        "\n"
      ],
      "metadata": {
        "id": "7-CN_x0Sju0y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **Вопрос 18: Математическое ожидание абсолютно непрерывной случайной величины и его свойства.**\n",
        "\n",
        "### Математическое ожидание и его свойства\n",
        "\n",
        "Математическое ожидание непрерывной случайной величины $X$ определяется как интеграл от произведения переменной $x$ и её плотности распределения $f(x)$:\n",
        "\n",
        "\n",
        "$$\n",
        "E(X) = \\int_{-\\infty}^{+\\infty} x f(x) \\, dx\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $E(X)$ — математическое ожидание случайной величины $X$,\n",
        "- $f(x)$ — плотность вероятности.\n",
        "\n",
        "Свойства математического ожидания, такие как линейность и аддитивность, остаются аналогичными тем, что применяются к дискретным случайным величинам. Также важно отметить, что для любой ненадёжной величины $c$:\n",
        "\n",
        "$$\n",
        "E(c) = c\n",
        "$$\n",
        "\n",
        "Кроме того, для независимых случайных величин $X$ и $Y$ выполняется правило:\n",
        "\n",
        "$$\n",
        "E(X + Y) = E(X) + E(Y)\n",
        "$$\n",
        "\n",
        "Подходой аналогичен тому, что мы рассматривали ранее для дискретных случайных величин.\n",
        "\n",
        "## **Пример: Равномерное распределение на интервале [0, 2]**\n",
        "\n",
        "Предположим, у нас есть непрерывная случайная величина $X$, равномерно распределенная на интервале $[0, 2]$. Это означает, что любое значение в этом интервале имеет одинаковую вероятность \"появления\".\n",
        "\n",
        "**1. Функция плотности вероятности (PDF):**\n",
        "\n",
        "Для равномерного распределения на интервале $[a, b]$ функция плотности вероятности задается как:\n",
        "\n",
        "$$\n",
        "f(x) = \\begin{cases}\n",
        "\\frac{1}{b-a}, & a \\leq x \\leq b \\\\\n",
        "0, & \\text{иначе}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "В нашем случае $a = 0$ и $b = 2$, поэтому:\n",
        "\n",
        "$$\n",
        "f(x) = \\begin{cases}\n",
        "\\frac{1}{2-0} = \\frac{1}{2}, & 0 \\leq x \\leq 2 \\\\\n",
        "0, & \\text{иначе}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "**2. Функция распределения (CDF):**\n",
        "\n",
        "Функция распределения $F(x) = P(X \\leq x)$ вычисляется как интеграл от PDF:\n",
        "\n",
        "$$\n",
        "F(x) = \\int_{-\\infty}^{x} f(t) \\, dt\n",
        "$$\n",
        "\n",
        "Для нашего примера:\n",
        "\n",
        "*   Если $x < 0$:  $F(x) = \\int_{-\\infty}^{x} 0 \\, dt = 0$\n",
        "*   Если $0 \\leq x \\leq 2$: $F(x) = \\int_{0}^{x} \\frac{1}{2} \\, dt = \\frac{1}{2} [t]_0^x = \\frac{1}{2}(x - 0) = \\frac{x}{2}$\n",
        "*   Если $x > 2$: $F(x) = \\int_{-\\infty}^{0} 0 \\, dt + \\int_{0}^{2} \\frac{1}{2} \\, dt + \\int_{2}^{x} 0 \\, dt = 0 + \\frac{1}{2}[t]_0^2 + 0 = \\frac{1}{2}(2 - 0) = 1$\n",
        "\n",
        "Таким образом, функция распределения для нашего примера:\n",
        "\n",
        "$$\n",
        "F(x) = \\begin{cases}\n",
        "0, & x < 0 \\\\\n",
        "\\frac{x}{2}, & 0 \\leq x \\leq 2 \\\\\n",
        "1, & x > 2\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "**3. Квантиль (Медиана):**\n",
        "\n",
        "Найдем медиану, которая является квантилем уровня 0.5. Это означает, что мы ищем такое значение $q_{0.5}$, что $F(q_{0.5}) = 0.5$.\n",
        "\n",
        "Используя функцию распределения:\n",
        "\n",
        "$$\n",
        "F(q_{0.5}) = \\frac{q_{0.5}}{2} = 0.5\n",
        "$$\n",
        "\n",
        "Решая уравнение относительно $q_{0.5}$:\n",
        "\n",
        "$$\n",
        "q_{0.5} = 0.5 \\times 2 = 1\n",
        "$$\n",
        "\n",
        "**Пояснение:** Медиана равна 1. Это означает, что 50% значений случайной величины $X$ будут меньше или равны 1, и 50% значений будут больше 1. В контексте равномерного распредения на интервале [0, 2], это интуитивно понятно, так как середина интервала и есть 1.\n",
        "\n",
        "**4. Математическое ожидание:**\n",
        "\n",
        "Математическое ожидание $E(X)$ вычисляется по формуле:\n",
        "\n",
        "$$\n",
        "E(X) = \\int_{-\\infty}^{+\\infty} x f(x) \\, dx\n",
        "$$\n",
        "\n",
        "Подставляя нашу функцию плотности вероятности:\n",
        "\n",
        "$$\n",
        "E(X) = \\int_{-\\infty}^{0} x \\cdot 0 \\, dx + \\int_{0}^{2} x \\cdot \\frac{1}{2} \\, dx + \\int_{2}^{+\\infty} x \\cdot 0 \\, dx\n",
        "$$\n",
        "\n",
        "$$\n",
        "E(X) = 0 + \\int_{0}^{2} \\frac{x}{2} \\, dx + 0\n",
        "$$\n",
        "\n",
        "$$\n",
        "E(X) = \\frac{1}{2} \\int_{0}^{2} x \\, dx\n",
        "$$\n",
        "\n",
        "$$\n",
        "E(X) = \\frac{1}{2} \\left[ \\frac{x^2}{2} \\right]_0^2\n",
        "$$\n",
        "\n",
        "$$\n",
        "E(X) = \\frac{1}{2} \\left( \\frac{2^2}{2} - \\frac{0^2}{2} \\right)\n",
        "$$\n",
        "\n",
        "$$\n",
        "E(X) = \\frac{1}{2} \\left( \\frac{4}{2} - 0 \\right)\n",
        "$$\n",
        "\n",
        "$$\n",
        "E(X) = \\frac{1}{2} \\cdot 2\n",
        "$$\n",
        "\n",
        "$$\n",
        "E(X) = 1\n",
        "$$\n",
        "\n",
        "**Пояснение:** Математическое ожидание случайной величины $X$ равно 1. Математическое ожидание можно интерпретировать как среднее значение, которое мы ожидаем получить, если будем многократно наблюдать значения случайной величины. Для равномерного распределения на интервале $[a, b]$, математическое ожидание всегда равно $\\frac{a+b}{2}$. В нашем случае $\\frac{0+2}{2} = 1$, что соответствует полученному результату.\n",
        "\n",
        "**В заключение:**\n",
        "\n",
        "Этот пример демонстрирует, как использовать функцию распределения для нахождения квантиля (в данном случае медианы) и как вычислить математическое ожидание непрерывной случайной величины с использованием функции плотности вероятности. Равномерное распределение является простым, но наглядным примером, позволяющим понять основные концепции."
      ],
      "metadata": {
        "id": "6UjKJWks-rdS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **Вопрос 19: Дисперсия абсолютно непрерывных случайных величин и ее свойства.**\n",
        "\n",
        "\n",
        "*   **Дисперсия** – мера разброса случайной величины относительно её математического ожидания. Дисперсия $D(X)$ для непрерывной случайной величины $X$ определяется как: $D(X) = \\int_{-\\infty}^{\\infty} (x - E(X))^2 \\cdot f(x) \\, dx$.\n",
        "\n",
        "### Дисперсия непрерывной случайной величины\n",
        "\n",
        "Для непрерывной случайной величины $X$ дисперсия определяется как:\n",
        "\n",
        "$$\n",
        "D(X) = E[(X - E(X))^2] = \\int_{-\\infty}^{\\infty} (x - E(X))^2 f(x) \\, dx\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $D(X)$ — дисперсия случайной величины $X$,\n",
        "- $E(X)$ — математическое ожидание $X$,\n",
        "- $f(x)$ — плотность распределения $X$.\n",
        "\n",
        "Здесь мы замечаем, что аналогично дискритным величинам, дисперсия сохраняет свои важные свойства, такие как:\n",
        "1. Для любой ненадёжной величины $c$, $D(c) = 0$.\n",
        "2. Квадратичное вычитание: если $d$ — константа, то $D(dX) = d^2 \\cdot D(X)$.\n",
        "3. Аддитивность для независимых случайных величин: если $X$ и $Y$ независимы, тогда:\n",
        "\n",
        "$$\n",
        "D(X + Y) = D(X) + D(Y)\n",
        "$$"
      ],
      "metadata": {
        "id": "ANE6WQO9CAmH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **Вопрос 20: Равномерный закон распределения (плотность распределения, мат.ожидание, дисперсия)**\n",
        "\n",
        "*   **Равномерное распределение** – распределение, при котором случайная величина равновероятно принимает значения в определённом интервале $[A, B]$.  Плотность вероятности для равномерного распределения $f(x) = \\frac{1}{B-A}$.\n",
        "\n",
        "### **1. Плотность распределения**\n",
        "\n",
        "Плотность вероятности $f(x)$ для равномерного распределения на интервале $[A, B]$ задаётся следующей функцией:\n",
        "\n",
        "$$\n",
        "f(x) =\n",
        "\\begin{cases}\n",
        "\\frac{1}{B - A}, & \\text{если } A \\leq x \\leq B, \\\\\n",
        "0, & \\text{иначе}.\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "**Пояснение:**\n",
        "- Поскольку все значения внутри интервала $[A, B]$ равновероятны, плотность вероятности постоянна на этом интервале.\n",
        "- Значение $\\frac{1}{B - A}$ обеспечивает выполнение условия нормировки, то есть площадь под графиком плотности распределения равна 1:\n",
        "  $$\n",
        "  \\int_{-\\infty}^{\\infty} f(x) \\, dx = \\int_{A}^{B} \\frac{1}{B - A} \\, dx = 1.\n",
        "  $$\n",
        "\n",
        "**Пример:**\n",
        "Пусть случайная величина $X$ равномерно распределена на интервале $[2, 5]$. Тогда плотность распределения:\n",
        "$$\n",
        "f(x) =\n",
        "\\begin{cases}\n",
        "\\frac{1}{5 - 2} = \\frac{1}{3}, & \\text{если } 2 \\leq x \\leq 5, \\\\\n",
        "0, & \\text{иначе}.\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "\n",
        "### **2. Математическое ожидание**\n",
        "\n",
        "Математическое ожидание $E(X)$ для равномерного распределения вычисляется по формуле:\n",
        "$$\n",
        "E(X) = \\frac{A + B}{2}.\n",
        "$$\n",
        "\n",
        "**Пояснение:**\n",
        "- Математическое ожидание равномерного распределения равно середине интервала $[A, B]$, так как все значения внутри интервала равновероятны.\n",
        "- Это значение является \"центром масс\" распределения.\n",
        "\n",
        "**Пример:**\n",
        "Для интервала $[2, 5]$ математическое ожидание:\n",
        "$$\n",
        "E(X) = \\frac{2 + 5}{2} = 3.5.\n",
        "$$\n",
        "\n",
        "\n",
        "### **3. Дисперсия**\n",
        "\n",
        "Дисперсия $D(X)$ для равномерного распределения вычисляется по формуле:\n",
        "$$\n",
        "D(X) = \\frac{(B - A)^2}{12}.\n",
        "$$\n",
        "\n",
        "**Пояснение:**\n",
        "- Дисперсия характеризует разброс значений случайной величины относительно её математического ожидания.\n",
        "- Для равномерного распределения дисперсия зависит от длины интервала $B - A$. Чем больше интервал, тем больше разброс значений.\n",
        "\n",
        "**Пример:**\n",
        "Для интервала $[2, 5]$ дисперсия:\n",
        "$$\n",
        "D(X) = \\frac{(5 - 2)^2}{12} = \\frac{9}{12} = 0.75.\n",
        "$$\n",
        "\n",
        "\n",
        "### **4. График плотности распределения**\n",
        "\n",
        "График плотности равномерного распределения представляет собой прямоугольник с основанием $[A, B]$ и высотой $\\frac{1}{B - A}$. Вне интервала $[A, B]$ график совпадает с осью $x$.\n",
        "\n",
        "**Пример:**\n",
        "Для интервала $[2, 5]$ график плотности будет прямоугольником с основанием от 2 до 5 и высотой $\\frac{1}{3}$.\n",
        "\n",
        "\n",
        "### **5. Применение равномерного распределения**\n",
        "\n",
        "Равномерное распределение используется в различных областях, таких как:\n",
        "- Моделирование случайных процессов, где все значения равновероятны (например, выбор случайной точки на отрезке).\n",
        "- Генерация случайных чисел в компьютерных алгоритмах.\n",
        "- Теория очередей и теория массового обслуживания.\n",
        "\n",
        "**Пример:**\n",
        "Если время ожидания автобуса равномерно распределено на интервале от 0 до 10 минут, то вероятность того, что автобус приедет в любой момент времени в этом интервале, одинакова.\n",
        "\n",
        "\n",
        "### **Итог**\n",
        "\n",
        "Равномерное распределение — это простое и важное распределение, которое описывает ситуации, где все значения случайной величины в определённом интервале равновероятны. Его основные характеристики (плотность распределения, математическое ожидание и дисперсия) легко вычисляются и интерпретируются."
      ],
      "metadata": {
        "id": "DYbPmj8WDJir"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **Вопрос 22: Нормальный закон распределения (плотность распределения, мат.ожидание, дисперсия).**\n",
        "\n",
        "### **Вопрос 23: Стандартный нормальный закон. Правило 3 сигм.**\n",
        "\n",
        "**Нормальный закон распределения (распределение Гаусса)**\n",
        "\n",
        "Нормальный закон распределения, также известный как распределение Гаусса, является одним из самых важных и широко используемых распределений вероятностей в статистике и теории вероятностей. Его распространенность обусловлена центральной предельной теоремой, которая утверждает, что сумма большого количества независимых, одинаково распределенных случайных величин, имеющих конечное математическое ожидание и дисперсию, будет приблизительно распределена по нормальному закону. Нормальное распределение описывает, как значения случайной величины распределяются вокруг среднего, формируя симметричную колоколообразную кривую. Оно полностью определяется двумя параметрами: математическим ожиданием ($\\mu$) и дисперсией ($\\sigma^2$). Математическое ожидание определяет положение центра распределения, а дисперсия, или ее квадратный корень – стандартное отклонение ($\\sigma$), характеризует разброс данных относительно среднего значения.\n",
        "\n",
        "**1. Плотность распределения (Probability Density Function - PDF)**\n",
        "\n",
        "Плотность распределения нормального закона задается следующей формулой:\n",
        "\n",
        "$$f(x | \\mu, \\sigma^2) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{1}{2} \\left(\\frac{x - \\mu}{\\sigma}\\right)^2}$$\n",
        "\n",
        "где:\n",
        "\n",
        "*   $x$ - значение случайной величины.\n",
        "*   $\\mu$ - математическое ожидание (среднее значение) распределения.\n",
        "*   $\\sigma^2$ - дисперсия распределения.\n",
        "*   $\\sigma$ - стандартное отклонение распределения ($\\sigma > 0$).\n",
        "*   $\\pi$ - математическая константа, приблизительно равная 3.14159.\n",
        "*   $e$ - основание натурального логарифма, приблизительно равное 2.71828.\n",
        "\n",
        "**Разбор формулы:**\n",
        "\n",
        "*   **$\\frac{1}{\\sigma \\sqrt{2\\pi}}$**: Этот множитель является **НОРМИРОВОЧНОЙ КОНСТАНТОЙ**, обеспечивающей, что интеграл плотности вероятности по всей области определения равен 1 (то есть, общая вероятность всех возможных исходов равна 1).\n",
        "*   **$e^{-\\frac{1}{2} \\left(\\frac{x - \\mu}{\\sigma}\\right)^2}$**:  Это экспоненциальная часть формулы, определяющая форму колоколообразной кривой.\n",
        "    *   **$\\frac{x - \\mu}{\\sigma}$**: Это стандартизированное значение случайной величины $x$, показывающее, на сколько стандартных отклонений значение $x$ отличается от среднего значения $\\mu$.\n",
        "    *   **$\\left(\\frac{x - \\mu}{\\sigma}\\right)^2$**: Квадрат стандартизированного значения.\n",
        "    *   **$-\\frac{1}{2} \\left(\\frac{x - \\mu}{\\sigma}\\right)^2$**: Отрицательное значение в экспоненте приводит к тому, что функция достигает максимума при $x = \\mu$ и убывает по мере удаления от $\\mu$.\n",
        "\n",
        "**2. Математическое ожидание (Expected Value)**\n",
        "\n",
        "Математическое ожидание нормального распределения равно параметру $\\mu$. Математически это можно показать следующим образом:\n",
        "\n",
        "$E[X] = \\int_{-\\infty}^{\\infty} x f(x | \\mu, \\sigma^2) dx$\n",
        "\n",
        "Подставляем функцию плотности вероятности:\n",
        "\n",
        "$E[X] = \\int_{-\\infty}^{\\infty} x \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{1}{2} \\left(\\frac{x - \\mu}{\\sigma}\\right)^2} dx$\n",
        "\n",
        "Введем замену переменной: $z = \\frac{x - \\mu}{\\sigma}$, тогда $x = \\mu + \\sigma z$, и $dx = \\sigma dz$. Пределы интегрирования останутся от $-\\infty$ до $\\infty$.\n",
        "\n",
        "$E[X] = \\int_{-\\infty}^{\\infty} (\\mu + \\sigma z) \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{1}{2} z^2} \\sigma dz$\n",
        "\n",
        "$E[X] = \\int_{-\\infty}^{\\infty} (\\mu + \\sigma z) \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2} z^2} dz$\n",
        "\n",
        "Разделим интеграл на две части:\n",
        "\n",
        "$E[X] = \\mu \\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2} z^2} dz + \\sigma \\int_{-\\infty}^{\\infty} z \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2} z^2} dz$\n",
        "\n",
        "*   Первый интеграл $\\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2} z^2} dz$ является интегралом плотности вероятности стандартного нормального распределения (с $\\mu=0$ и $\\sigma^2=1$), и его значение равно 1.\n",
        "\n",
        "*   Второй интеграл $\\int_{-\\infty}^{\\infty} z \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2} z^2} dz$ равен 0, поскольку подынтегральная функция является нечетной ($g(-z) = -g(z)$), а интегрирование происходит на симметричном интервале от $-\\infty$ до $\\infty$.\n",
        "\n",
        "Таким образом:\n",
        "\n",
        "$E[X] = \\mu \\cdot 1 + \\sigma \\cdot 0 = \\mu$\n",
        "\n",
        "Следовательно, математическое ожидание нормального распределения действительно равно $\\mu$.\n",
        "\n",
        "**3. Дисперсия (Variance)**\n",
        "\n",
        "Дисперсия нормального распределения равна параметру $\\sigma^2$. Математически это можно показать следующим образом:\n",
        "\n",
        "$Var(X) = E[(X - E[X])^2] = E[(X - \\mu)^2] = \\int_{-\\infty}^{\\infty} (x - \\mu)^2 f(x | \\mu, \\sigma^2) dx$\n",
        "\n",
        "Подставляем функцию плотности вероятности:\n",
        "\n",
        "$Var(X) = \\int_{-\\infty}^{\\infty} (x - \\mu)^2 \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{1}{2} \\left(\\frac{x - \\mu}{\\sigma}\\right)^2} dx$\n",
        "\n",
        "Используем ту же замену переменной $z = \\frac{x - \\mu}{\\sigma}$, тогда $x - \\mu = \\sigma z$, и $dx = \\sigma dz$.\n",
        "\n",
        "$Var(X) = \\int_{-\\infty}^{\\infty} (\\sigma z)^2 \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{1}{2} z^2} \\sigma dz$\n",
        "\n",
        "$Var(X) = \\int_{-\\infty}^{\\infty} \\sigma^2 z^2 \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2} z^2} dz$\n",
        "\n",
        "$Var(X) = \\sigma^2 \\int_{-\\infty}^{\\infty} z^2 \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2} z^2} dz$\n",
        "\n",
        "Интеграл $\\int_{-\\infty}^{\\infty} z^2 \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2} z^2} dz$ является дисперсией стандартного нормального распределения (с $\\mu=0$ и $\\sigma^2=1$), и его значение равно 1. Это можно показать с помощью интегрирования по частям.\n",
        "\n",
        "Пусть $u = z$, $dv = z \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2} z^2} dz$.\n",
        "Тогда $du = dz$, $v = -\\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2} z^2}$.\n",
        "\n",
        "$\\int_{-\\infty}^{\\infty} z^2 \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2} z^2} dz = \\left[ -z \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2} z^2} \\right]_{-\\infty}^{\\infty} - \\int_{-\\infty}^{\\infty} -\\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2} z^2} dz$\n",
        "\n",
        "Первое слагаемое $\\left[ -z \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2} z^2} \\right]_{-\\infty}^{\\infty}$ равно 0 (предел $ze^{-z^2/2}$ при $z \\to \\pm \\infty$ равен 0).\n",
        "\n",
        "Второе слагаемое равно $\\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2} z^2} dz$, что является интегралом плотности вероятности стандартного нормального распределения и равно 1.\n",
        "\n",
        "Таким образом:\n",
        "\n",
        "$Var(X) = \\sigma^2 \\cdot 1 = \\sigma^2$\n",
        "\n",
        "**Свойства нормального распределения**\n",
        "\n",
        "Нормальное распределение обладает рядом важных свойств:\n",
        "\n",
        "1. **Симметрия**: Распределение симметрично относительно математического ожидания $\\mu$. Это означает, что половина значений находится слева от $\\mu$, а другая половина — справа.\n",
        "2. **Эмпирическое правило (правило трех сигм)**:\n",
        "    *   Около 68% значений лежат в пределах одного стандартного отклонения от среднего ($\\mu \\pm \\sigma$).\n",
        "    *   Около 95% значений лежат в пределах двух стандартных отклонений от среднего ($\\mu \\pm 2\\sigma$).\n",
        "    *   Приблизительно 99.7% значений лежат в пределах трех стандартных отклонений от среднего ($\\mu \\pm 3\\sigma$).\n",
        "3. **Асимптотичность**: Плотность вероятности приближается к нулю по мере удаления от среднего, но никогда его не достигает.\n",
        "\n",
        "**Стандартное нормальное распределение**\n",
        "\n",
        "Стандартное нормальное распределение — это частный случай нормального распределения с математическим ожиданием $\\mu = 0$ и стандартным отклонением $\\sigma = 1$. Его плотность вероятности задается формулой:\n",
        "\n",
        "$$f(z) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{z^2}{2}}$$\n",
        "\n",
        "где $z$ обозначает стандартизированную случайную величину. Любое нормальное распределение можно преобразовать в стандартное нормальное распределение путем центрирования и масштабирования данных с использованием формулы $z = \\frac{x - \\mu}{\\sigma}$. Это позволяет использовать таблицы стандартного нормального распределения для расчета вероятностей для любого нормального распределения.\n",
        "\n",
        "В заключение, нормальный закон распределения является фундаментальным понятием в статистике, позволяющим анализировать и интерпретировать данные в различных областях. Его свойства, такие как симметрия и правило трех сигм, делают его мощным инструментом для принятия решений и оценки вероятностей."
      ],
      "metadata": {
        "id": "Id9BmiuMEpkE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "### **Вопрос 24: Моменты и критические границы случайной величины (начальные, центральные).**\n",
        "\n",
        "### **Вопрос 25: Моменты и критические границы случайной величины ( эксцесс, асимметрия). Привести пример расчета на небольшом датасете.**\n",
        "\n",
        "## **Квантиль и двусторонние критические границы**\n",
        "\n",
        "В этом фрагменте мы сосредоточимся на понятии двусторонних критических границ и их связи с квантилями и функциями распределения. Эти концепции являются важными для анализа вероятностных данных и принятия статистических решений.\n",
        "\n",
        "### Двусторонние критические границы\n",
        "\n",
        "**Двусторонние критические границы** определяются как границы уровня $\\alpha$, которые включают как левую, так и правую хвостовые области распределения. Это значит, что мы ищем такие значения $x_\\alpha$, которые обеспечивают равные вероятности для обеих сторон от среднего значения:\n",
        "\n",
        "$$\n",
        "P(X < x_\\alpha) = \\alpha/2 \\quad \\text{и} \\quad P(X > x_\\alpha) = \\alpha/2\n",
        "$$\n",
        "\n",
        "Здесь мы говорим о том, что сумма вероятностей этих хвостов будет равна $\\alpha$. Квантили в этом случае можно записать как:\n",
        "\n",
        "$$\n",
        "x_{\\alpha/2} \\quad \\text{и} \\quad x_{1 - \\alpha/2}\n",
        "$$\n",
        "\n",
        "При этом левосторонний критический уровень $\\alpha/2$ соответствует нижней границе, а правосторонний критический уровень $1 - \\alpha/2$ — верхней границе.\n",
        "\n",
        "### Симметрия нормального распределения\n",
        "\n",
        "Если распределение симметрично относительно оси абсцисс, то:\n",
        "\n",
        "$$\n",
        "x_{\\alpha/2} = -x_{1 - \\alpha/2}\n",
        "$$\n",
        "\n",
        "Это значит, что обе границы находятся на равном расстоянии от математического ожидания, и их вероятности равны.\n",
        "\n",
        "### **Основная идея**\n",
        "\n",
        "**Основная идея заключается в том, что критические границы и квантили позволяют нам принимать решения относительно наших данных и делать выводы о генеральной совокупности, основываясь на выборке.**  Они предоставляют инструменты для оценки того, насколько \"экстремальным\" является наблюдаемое значение или насколько вероятно получить определенный результат, если бы верна некоторая гипотеза.\n",
        "\n",
        "Вот несколько ключевых моментов, объясняющих их значимость:\n",
        "\n",
        "1. **Определение \"редких\" или \"необычных\" значений (Выявление выбросов):**\n",
        "   - Критические границы, особенно двусторонние, очерчивают область, где, при условии верности определенной модели (например, нормального распределения), большинство наблюдений должны находиться.\n",
        "   - Значения, выходящие за эти границы, считаются относительно редкими или маловероятными. Это может указывать на наличие выбросов в данных, которые требуют дополнительного внимания. Например, если мы анализируем рост людей и видим значение, выходящее за пределы наших критических границ, это может быть ошибкой измерения или действительно очень высокий человек.\n",
        "   - Квантили, соответствующие критическим границам (например, $x_{\\alpha/2}$ и $x_{1 - \\alpha/2}$),  точно определяют эти пороговые значения.\n",
        "\n",
        "2. **Проверка статистических гипотез:**\n",
        "   - Это, пожалуй, одно из самых важных применений критических границ. В процессе проверки гипотез мы формулируем нулевую гипотезу (например, \"средний рост мужчин в популяции равен 175 см\").\n",
        "   - Затем мы рассчитываем статистику теста на основе нашей выборки (например, среднее значение роста в нашей выборке).\n",
        "   - Критические границы определяют так называемую \"область отклонения\" нулевой гипотезы. Если значение статистики теста попадает в эту область (то есть выходит за критические границы), мы отвергаем нулевую гипотезу в пользу альтернативной.\n",
        "   - Квантили помогают нам точно определить эти границы, основываясь на выбранном уровне значимости $\\alpha$. Например, если мы выбрали $\\alpha = 0.05$, то квантили $x_{0.025}$ и $x_{0.975}$ определят границы, за которые попадут 5% наиболее экстремальных значений, если нулевая гипотеза верна.\n",
        "\n",
        "3. **Построение доверительных интервалов:**\n",
        "   - Доверительные интервалы предоставляют диапазон значений, в котором, с определенной вероятностью (уровнем доверия), находится истинное значение параметра генеральной совокупности.\n",
        "   - Квантили играют ключевую роль в определении границ этого интервала. Например, для 95% доверительного интервала мы используем квантили $x_{0.025}$ и $x_{0.975}$ стандартного нормального распределения (если мы предполагаем нормальное распределение).\n",
        "\n",
        "4. **Оценка риска и вероятности экстремальных событий:**\n",
        "   - В финансах, например, квантили используются для оценки Value at Risk (VaR), который показывает максимальные потенциальные потери с заданной вероятностью. Критические границы, основанные на квантилях, помогают определить пороги, при превышении которых риск считается неприемлемым.\n",
        "\n",
        "5. **Сравнение выборок:**\n",
        "   - При сравнении двух или более выборок критические границы могут помочь определить, является ли разница между ними статистически значимой или обусловлена случайными колебаниями. Если статистика теста, сравнивающая выборки, выходит за критические границы, мы можем заключить, что разница, вероятно, не случайна.\n",
        "\n",
        "**Пример для иллюстрации:**\n",
        "\n",
        "Представьте, что мы изучаем время реакции водителей на красный свет. Мы собрали выборку и хотим проверить гипотезу о том, что среднее время реакции составляет 0.5 секунды.\n",
        "\n",
        "- Мы выбираем уровень значимости $\\alpha = 0.05$.\n",
        "- Если мы предполагаем, что время реакции распределено нормально, мы можем найти квантили стандартного нормального распределения, соответствующие $\\alpha/2 = 0.025$ и $1 - \\alpha/2 = 0.975$. Это будут примерно -1.96 и 1.96.\n",
        "- Эти значения определяют наши критические границы для стандартизированной статистики теста (например, t-статистики).\n",
        "- Если рассчитанное значение статистики теста на основе нашей выборки выходит за пределы этих границ (например, больше 1.96 или меньше -1.96), мы отвергаем нулевую гипотезу о среднем времени реакции в 0.5 секунды. Это означает, что наша выборка предоставляет достаточно доказательств против этой гипотезы.\n",
        "\n",
        "**В заключение:**\n",
        "\n",
        "Определение критических границ и квантилей критических границ дает нам **инструменты для принятия обоснованных решений на основе вероятностного анализа данных**. Они позволяют:\n",
        "\n",
        "- **Идентифицировать необычные наблюдения.**\n",
        "- **Проверять статистические гипотезы о параметрах генеральной совокупности.**\n",
        "- **Строить доверительные интервалы для оценки диапазонов возможных значений параметров.**\n",
        "- **Оценивать риски и вероятности экстремальных событий.**\n",
        "- **Сравнивать различные выборки и делать выводы об их различиях.**\n",
        "\n",
        "Таким образом, эти концепции являются фундаментальными для статистического анализа и позволяют нам переходить от простого описания выборки к более глубоким выводам о генеральной совокупности, из которой эта выборка была получена.\n",
        "\n",
        "### Пример кода\n",
        "\n",
        "Для дальнейшего понимания концепции двусторонних критических границ рассмотрим код, который вычисляет квантили для двустороннего нормального распределения:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Уровень альфа\n",
        "alpha = 0.05\n",
        "\n",
        "# Находим двусторонние критические границы\n",
        "lower_bound = stats.norm.ppf(alpha / 2)  # Нижняя граница\n",
        "upper_bound = stats.norm.ppf(1 - alpha / 2)  # Верхняя граница\n",
        "\n",
        "# Выводим результаты\n",
        "print(f\"Двусторонние критические границы для уровня {alpha}:\")\n",
        "print(f\"Нижняя граница: {lower_bound}\")\n",
        "print(f\"Верхняя граница: {upper_bound}\")\n",
        "```\n",
        "\n",
        "В данном коде используется библиотека `scipy` для нахождения двусторонних критических границ нормального распределения. Функция `$ppf$` вычисляет соответствующий квантиль для заданного уровня $\\alpha$.\n",
        "\n",
        "### Физический и геометрический смысл\n",
        "\n",
        "Понимание двусторонних критических границ важно в таких областях, как контроль качества, медицина, и социология. Например, в производстве можно использовать эти границы, чтобы определить, какие изделия соответствуют стандартам. Если, скажем, 5% продукции отклоняются от нормы, знание двусторонних критических границ позволяет компаниям оценить уровень дефектов и принимать решения о контроле качества или улучшении производственных процессов.\n",
        "\n",
        "Таким образом, концепция квантилей и двусторонних критических границ предоставляет полезные инструменты для анализа и интерпретации вероятностных данных, что имеет важное значение для принятия обоснованных решений в различных отраслях.\n",
        "\n",
        "\n",
        "## **Моменты случайной величины: асимметрия и эксцесс**\n",
        "\n",
        "Ключевой темой данного фрагмента являются моменты случайной величины, а именно асимметрия и эксцесс, которые помогают охарактеризовать распределение данных и понять их поведение.\n",
        "\n",
        "### Начальные и центральные моменты\n",
        "\n",
        "Рассмотрим, что такое моменты случайной величины. **Начальные моменты** — это математическое ожидание величины, возведенной в степень. Например, начальный момент первого порядка равен математическому ожиданию случайной величины $X$:\n",
        "\n",
        "$$\n",
        "E(X) = \\int_{-\\infty}^{\\infty} x f(x) \\, dx\n",
        "$$\n",
        "\n",
        "где $f(x)$ — плотность вероятности случайной величины.\n",
        "\n",
        "**Центральные моменты** вычисляются относительно среднего значения, представляя собой большее количество информации о распределении. Центральный момент второго порядка — это дисперсия, описывающая разброс значений относительно их среднего:\n",
        "\n",
        "$$\n",
        "D(X) = E[(X - E(X))^2]\n",
        "$$\n",
        "\n",
        "### Асимметрия\n",
        "\n",
        "**Асимметрия** — это центральный момент третьего порядка, который описывает скошенность распределения. Формула для асимметрии $A$ записывается как:\n",
        "\n",
        "$$\n",
        "A = \\frac{E[(X - E(X))^3]}{\\sigma^3}\n",
        "$$\n",
        "\n",
        "где $\\sigma$ — стандартное отклонение.\n",
        "\n",
        "Асимметрия может принимать следующие значения:\n",
        "- $A = 0$: распределение симметрично;\n",
        "- $A > 0$: распределение скошено вправо (длинный хвост справа);\n",
        "- $A < 0$: распределение скошено влево (длинный хвост слева).\n",
        "\n",
        "Пример: Если у нас есть баллы студентов за контрольную работу, и большинство студентов набрали низкие баллы, а несколько получили высокие — это будет представлять собой положительную асимметрию.\n",
        "\n",
        "### Эксцесс\n",
        "\n",
        "**Эксцесс** — это центральный момент четвертого порядка и характеризует \"тяжесть\" хвостов распределения по сравнению с нормальным распределением. Он вычисляется по формуле:\n",
        "\n",
        "$$\n",
        "E = \\frac{E[(X - E(X))^4]}{\\sigma^4} - 3\n",
        "$$\n",
        "\n",
        "Значение эксцесса также может быть разным:\n",
        "- $E = 0$: распределение нормальное;\n",
        "- $E > 0$: распределение острое (длинные и тяжелые хвосты);\n",
        "- $E < 0$: распределение тупое (короткие и легкие хвосты).\n",
        "\n",
        "### Итог\n",
        "\n",
        "> Имем еще несколько описательных статистик данных\n",
        "\n",
        "1. **Моменты**:\n",
        "   - **Начальные**: Среднее значение (первый момент).\n",
        "   - **Центральные**: Дисперсия (второй момент) — разброс данных.\n",
        "\n",
        "2. **Асимметрия**:\n",
        "   - Третий момент, показывает скошенность.\n",
        "   - $( A = 0 )$: симметрия, $( A > 0 )$: вправо, $( A < 0 )$: влево.\n",
        "\n",
        "3. **Эксцесс**:\n",
        "   - Четвертый момент, характеризует хвосты.\n",
        "   - $( E = 0 )$: нормальное, $( E > 0 )$: тяжелые хвосты, $( E < 0 )$: легкие хвосты.\n",
        "\n",
        "4. **Пример**:\n",
        "   - Асимметрия: Большинство низких баллов — скошено вправо.\n",
        "   - Эксцесс: Острое распределение — длинные хвосты.\n",
        "\n",
        "### Пример кода\n",
        "\n",
        "Рассмотрим пример на Python, который вычисляет асимметрию и эксцесс для заданного множества данных:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from scipy.stats import skew, kurtosis\n",
        "\n",
        "# Генерируем случайные данные с нормальным распределением\n",
        "data = np.random.normal(loc=0, scale=1, size=1000)\n",
        "\n",
        "# Вычисляем асимметрию\n",
        "asymmetry = skew(data)\n",
        "\n",
        "# Вычисляем эксцесс\n",
        "excess = kurtosis(data)\n",
        "\n",
        "# Выводим результаты\n",
        "print(f\"Асимметрия: {asymmetry}\")\n",
        "print(f\"Eксцесс: {excess}\")\n",
        "```\n",
        "\n",
        "В этом коде мы используем библиотеку `scipy` для вычисления асимметрии и эксцесса для набора данных с нормальным распределением. Мы генерируем 1000 случайных значений из нормального распределения, а затем применяем функции `skew` и `kurtosis` для вычисления асимметрии и эксцесса соответственно.\n",
        "\n",
        "### Физический и геометрический смысл\n",
        "\n",
        "Анализ асимметрии и эксцесса важен в таком контексте, как контроль качества, где необходимо оценивать, как распределение дефектов продукции или результатов эксперимента отклоняются от ожидаемого нормального поведения. Например, если часть товаров имеет высокую вероятность бракованности, это может указывать на проблемы в процессе производства.\n",
        "\n",
        "Понимание этих моментов позволяет принимать обоснованные решения о ходе исследования, улучшении процессов и ожидаемых результатах, а также помогает в разработке стратегий управления рисками при планировании или запуске новых проектов."
      ],
      "metadata": {
        "id": "T0zUAf26c8tH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **Вопрос 26: Квантили и процентные точки случайной величины. Привести пример расчета 10% проц.точки на небольшом датасете (10-20 значений).**\n",
        "\n",
        "## **Квантили и процентные точки случайной величины**\n",
        "\n",
        "Ключевой концепцией в данном фрагменте являются квантили и процентные точки случайной величины, которые помогают статистикам оценивать распределение данных и принимать решения на основе этой информации.\n",
        "\n",
        "### Квантили\n",
        "\n",
        "**Квантили** — это точки, которые делят распределение данных на равные части. Квантили определяются как значения, которые делят вероятность на заданные доли. Например, $x_\\alpha$ — это квантиль уровня $\\alpha$, такой что:\n",
        "\n",
        "$$\n",
        "F(x_\\alpha) = \\alpha\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $F(x)$ — функция распределения случайной величины $X$,\n",
        "- $\\alpha$ — уровень вероятности, обычно выбираемый между 0 и 1 (например, 0.25 для первого квартиля, 0.5 для медианы и 0.75 для третьего квартиля).\n",
        "\n",
        "### Пример:\n",
        "\n",
        "Чтобы найти вероятность попадания непрерывной случайной величины $X$ в интервал от нуля до первого квартиля $x_{\\alpha}$, воспользуемся определением квантиля, которое было дано выше и функцией распределения.\n",
        "\n",
        "### Вероятность попадания в интервал\n",
        "Вероятность попадания случайной величины $X$ в интервал от $a$ до $b$ вычисляется как:\n",
        "\n",
        "$$\n",
        "P(a \\leq X \\leq b) = F(b) - F(a).\n",
        "$$\n",
        "\n",
        "В нашем случае интервал — от $0$ до $x_{\\alpha}$, где $\\alpha = 0.25$. Тогда:\n",
        "\n",
        "$$\n",
        "P(0 \\leq X \\leq x_{\\alpha}) = F(x_{\\alpha}) - F(0).\n",
        "$$\n",
        "\n",
        "### Упрощение\n",
        "По определению квантиля:\n",
        "\n",
        "$$\n",
        "F(x_{\\alpha}) = \\alpha = 0.25.\n",
        "$$\n",
        "\n",
        "Если $X$ — неотрицательная случайная величина (то есть $X \\geq 0$), то:\n",
        "\n",
        "$$\n",
        "F(0) = P(X \\leq 0) = 0.\n",
        "$$\n",
        "\n",
        "Таким образом, вероятность попадания $X$ в интервал от $0$ до $x_{\\alpha}$ равна:\n",
        "\n",
        "$$\n",
        "P(0 \\leq X \\leq x_{\\alpha}) = 0.25 - 0 = 0.25.\n",
        "$$\n",
        "\n",
        "### Ответ\n",
        "Вероятность попадания непрерывной случайной величины $X$ в интервал от нуля до первого квартиля равна **$0.25$** (или $25\\%$).\n",
        "\n",
        "### Процентные точки\n",
        "\n",
        "**Процентные точки** — это особые случаи квантилей, определяющие значения, при которых функция распределения равна определенному проценту от общей вероятности. По сути, это способ выразить положение определенного значения в распределении данных, ориентируясь на процент значений, которые *превышают* это значение. В отличие от обычных квантилей, где мы смотрим на долю значений *ниже* определенной точки, процентные точки фокусируются на \"хвосте\" распределения, то есть мы смотрим на долю значений выше определенной точки. Для ситуации с процентной точкой $x_{\\alpha}$:\n",
        "\n",
        "$$\n",
        "F(x_{\\alpha}) = 1 - \\alpha\n",
        "$$\n",
        "\n",
        "где, например, $x_{0.025}$ — это 2.5 процентная точка, где вероятность того, что случайная величина меньше или равна этой точке, равна 0.975 (или 97.5%).\n",
        "\n",
        "**Ключевое отличие:**\n",
        "\n",
        "$[\n",
        "\\boxed{\n",
        "\\begin{aligned}\n",
        "&\\quad \\bullet \\ \\textbf{Квантиль уровня } \\alpha \\ (x_\\alpha): \\text{Это значение, для которого вероятность того, что случайная величина } X \\text{ меньше или равна } x_\\alpha, \\text{ равна } \\alpha. \\text{ То есть, } P(X \\leq x_\\alpha) = \\alpha. \\\\\n",
        "&\\quad \\bullet \\ \\textbf{(Перцентиль) Процентная точка уровня } \\alpha \\ (x_\\alpha): \\text{Это значение, для которого вероятность того, что случайная величина } X \\text{ больше или равна } x_\\alpha, \\text{ равна } \\alpha. \\text{ То есть, } P(X \\geq x_\\alpha) = \\alpha.\n",
        "\\end{aligned}\n",
        "}\n",
        "]\n",
        "$\n",
        "\n",
        "**Разберем формулу $F(x_{\\alpha}) = 1 - \\alpha$ для процентных точек:**\n",
        "\n",
        "* $F(x_{\\alpha})$ — это функция распределения, которая по определению дает вероятность того, что случайная величина $X$ меньше или равна $x_{\\alpha}$. То есть, $F(x_{\\alpha}) = P(X \\leq x_{\\alpha})$.\n",
        "* Для процентной точки $x_{\\alpha}$ мы знаем, что вероятность того, что $X$ больше или равна $x_{\\alpha}$, равна $\\alpha$. То есть, $P(X \\geq x_{\\alpha}) = \\alpha$.\n",
        "* Поскольку сумма вероятностей противоположных событий равна 1, мы имеем: $P(X \\leq x_{\\alpha}) + P(X > x_{\\alpha}) = 1$. Для непрерывной случайной величины $P(X > x_{\\alpha}) = P(X \\geq x_{\\alpha})$.\n",
        "* Подставляя известное значение для процентной точки, получаем: $P(X \\leq x_{\\alpha}) + \\alpha = 1$.\n",
        "* Выражая $P(X \\leq x_{\\alpha})$, получаем: $P(X \\leq x_{\\alpha}) = 1 - \\alpha$.\n",
        "* Поскольку $F(x_{\\alpha}) = P(X \\leq x_{\\alpha})$, мы приходим к формуле для процентных точек: $F(x_{\\alpha}) = 1 - \\alpha$.\n",
        "\n",
        "**Примеры для лучшего понимания:**\n",
        "\n",
        "* **2.5-я процентная точка ($x_{0.025}$):**  Это значение, такое что вероятность того, что случайная величина $X$ больше или равна этому значению, составляет $0.025$ или $2.5\\%$. Это означает, что $97.5\\%$ значений случайной величины будут меньше этого значения. В контексте функции распределения: $F(x_{0.025}) = 1 - 0.025 = 0.975$.\n",
        "\n",
        "* **5-я процентная точка ($x_{0.05}$):** Это значение, выше которого лежит $5\\%$ распределения. Соответственно, $95\\%$ значений находятся ниже этой точки. $F(x_{0.05}) = 1 - 0.05 = 0.95$.\n",
        "\n",
        "* **10-я процентная точка ($x_{0.10}$):**  Это значение, выше которого находится $10\\%$ распределения. $90\\%$ значений лежат ниже этой точки. $F(x_{0.10}) = 1 - 0.10 = 0.90$.\n",
        "\n",
        "**Связь с квантилями:**\n",
        "\n",
        "Важно понимать, что процентные точки и квантили тесно связаны. Процентная точка уровня $\\alpha$ соответствует квантилю уровня $1 - \\alpha$.\n",
        "\n",
        "* Например, 2.5-я процентная точка ($x_{0.025}$) — это то же самое, что и квантиль уровня $1 - 0.025 = 0.975$. В обоих случаях мы говорим о значении, ниже которого находится $97.5\\%$ распределения.\n",
        "\n",
        "**Практическое применение:**\n",
        "\n",
        "Процентные точки часто используются в ситуациях, когда нас интересуют значения, которые редко достигаются или превышаются. Например:\n",
        "\n",
        "* **В финансах:** При оценке рисков часто используют 1% или 5% процентные точки для определения максимальных потенциальных убытков с определенной вероятностью.\n",
        "* **В медицине:**  При установлении границ нормальных значений для анализов могут использоваться процентные точки, чтобы определить значения, которые встречаются лишь у небольшого процента здоровых людей.\n",
        "\n",
        "Таким образом, процентные точки предоставляют альтернативный, но эквивалентный способ описания положения значений в распределении, акцентируя внимание на вероятности того, что случайная величина превысит определенный порог.\n",
        "\n",
        "### Визуализация и физический смысл\n",
        "\n",
        "Для наглядного понимания можно представить график функции распределения, где квантиль $x_\\alpha$ соответствует площади под кривой до этой точки, равной $\\alpha$. Например, если на графике плотности распределения выделить область от начала до $x_\\alpha$, то эта площадь будет равна $\\alpha$.\n",
        "\n",
        "### Пример\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Небольшой датасет\n",
        "data = [15, 22, 18, 25, 30, 12, 28, 16, 20, 24, 19, 27]\n",
        "\n",
        "# Для расчета 10% процентной точки нам нужно найти значение,\n",
        "# выше которого находится 10% данных. Это соответствует\n",
        "# 90-му перцентилю (квантилю уровня 0.9).\n",
        "\n",
        "# Используем функцию numpy.percentile для расчета перцентиля\n",
        "percentile_10_point = np.percentile(data, 90)\n",
        "\n",
        "print(f\"Датасет: {data}\")\n",
        "print(f\"10% процентная точка (90-й перцентиль): {percentile_10_point}\")\n",
        "\n",
        "# Пояснение:\n",
        "# Функция np.percentile принимает два аргумента:\n",
        "# 1. Массив данных.\n",
        "# 2. Процент (от 0 до 100), соответствующий желаемому перцентилю.\n",
        "#\n",
        "# В нашем случае, чтобы найти 10% процентную точку, мы ищем значение,\n",
        "# выше которого находится 10% данных. Это эквивалентно поиску значения,\n",
        "# ниже которого находится 90% данных, то есть 90-го перцентиля.\n",
        "```\n",
        "\n",
        "**Разбор кода:**\n",
        "\n",
        "1. **`import numpy as np`**: Импортируем библиотеку `numpy` для работы с массивами и статистическими функциями.\n",
        "2. **`data = [...]`**: Создаем небольшой список `data`, представляющий наш датасет.\n",
        "3. **Комментарий**:  Объясняется связь между процентной точкой и перцентилем. 10% процентная точка соответствует 90-му перцентилю.\n",
        "4. **`percentile_10_point = np.percentile(data, 90)`**:\n",
        "    *   Используется функция `np.percentile()` для расчета перцентиля.\n",
        "    *   Первый аргумент — это наш датасет `data`.\n",
        "    *   Второй аргумент — это значение перцентиля, который мы хотим рассчитать. В данном случае, для 10% процентной точки мы указываем `90` (что соответствует 90-му перцентилю).\n",
        "5. **`print(...)`**: Выводим исходный датасет и рассчитанную 10% процентную точку.\n",
        "\n",
        "**Как это работает:**\n",
        "\n",
        "Функция `np.percentile` интерполирует значения в датасете, чтобы найти значение, которое соответствует указанному перцентилю. В данном случае, она находит такое значение в `data`, что примерно 90% значений в датасете меньше или равны этому значению. Соответственно, примерно 10% значений будут больше этого значения, что и является определением 10% процентной точки.\n",
        "\n",
        "**Альтернативный (ручной) способ (для понимания):**\n",
        "\n",
        "Хотя `numpy.percentile` является наиболее эффективным способом, для понимания можно представить ручной процесс:\n",
        "\n",
        "1. **Сортировка данных:** Отсортировать датасет по возрастанию.\n",
        "2. **Расчет индекса:** Рассчитать индекс, соответствующий 90-му перцентилю: `index = (90 / 100) * (n - 1)`, где `n` — количество элементов в датасете.\n",
        "3. **Интерполяция (если индекс не целое число):** Если индекс не целое число, нужно интерполировать между значениями с ближайшими целыми индексами.\n",
        "\n",
        "Однако, для практических задач рекомендуется использовать `numpy.percentile`, так как он обрабатывает различные случаи и интерполяции автоматически.\n"
      ],
      "metadata": {
        "id": "fhWmDYWZDsOt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **Вопрос 27: Меры связи случайных величин. Коэффициент ковариации.**\n",
        "\n",
        "### **Вопрос 28: Коэффициент корреляции Пирсона. Привести пример расчета на 2 небольших датасетах.**\n",
        "\n",
        "### **Вопрос 29: Коэффициент корреляции Спирмена. Привести пример расчета на 2 небольших датасетах.**\n",
        "\n",
        "## **Корреляция и меры зависимости между случайными величинами**\n",
        "\n",
        "В этом фрагменте рассматриваются концепции корреляции и способов измерения степени зависимости случайных величин. Понимание зависимости между переменными важно для анализа данных и принятия обоснованных решений.\n",
        "\n",
        "### Корреляция\n",
        "\n",
        "**Корреляция** — это статистическая мера, которая описывает степень взаимосвязи между двумя случайными величинами. Она может принимать значения от -1 до +1:\n",
        "- Значение +1 указывает на идеальную положительную корреляцию, когда при увеличении одной величины другая также увеличивается.\n",
        "- Значение -1 указывает на идеальную отрицательную корреляцию, когда при увеличении одной величины другая уменьшается.\n",
        "- Значение 0 указывает на отсутствие линейной зависимости между величинами.\n",
        "\n",
        "Примеры:\n",
        "- Корреляция между оценками по математике и русскому языку может быть положительной, поскольку, как правило, успешные студенты могут хорошо учиться по обоим предметам.\n",
        "- Если студент хорошо справляется с математикой, но плохо пишет на русском, корреляция между этими двумя предметами может быть низкой или даже отрицательной.\n",
        "\n",
        "### Измерение степени зависимости\n",
        "\n",
        "Для измерения степени зависимости между двумя случайными величинами $X$ и $Y$, можно использовать **ковариацию**. Определение ковариации имеет следующий вид:\n",
        "\n",
        "$$\n",
        "\\text{cov}(X, Y) = E[(X - E(X))(Y - E(Y))] = E(XY) - E(X)E(Y)\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $E(XY)$ — математическое ожидание произведения $X$ и $Y$,\n",
        "- $E(X)$ и $E(Y)$ — математические ожидания величин $X$ и $Y$.\n",
        "\n",
        "Если случайные величины $X$ и $Y$ независимы, то:\n",
        "\n",
        "$$\n",
        "\\text{cov}(X, Y) = 0\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### **Коэффициент Пирсона**\n",
        "\n",
        "Коэффициент корреляции Пирсона — это статистическая мера силы и направления линейной связи между двумя переменными. Коэффициент корреляции может принимать значения от -1 до +1. Значение +1 означает совершенную положительную линейную связь, значение -1 означает совершенную отрицательную линейную связь, а 0 означает отсутствие линейной связи.\n",
        "\n",
        "Коэффициент корреляции Пирсона рассчитывается по формуле:\n",
        "\n",
        "- $ r = \\frac{n(\\sum x_i y_i) - (\\sum x_i)(\\sum y_i)}{\\sqrt{[n\\sum x_i^2 - (\\sum x)^2][n\\sum y_i^2 - (\\sum y_i)^2]}} $\n",
        "\n",
        "где:\n",
        "\n",
        "- $x$ и $y$ — значения переменных;\n",
        "\n",
        "- $n$ — количество наблюдений.\n",
        "\n",
        "**Числитель**:\n",
        "\n",
        "- $ n(\\sum x_i y_i) - (\\sum x_i)(\\sum y_i)$\n",
        "\n",
        "Эта часть формулы представляет \"ковариацию\" между $x_i$ и $y_i$. Ковариация показывает, насколько переменные изменяются вместе. Если $x_i$ возрастает, когда $y_i$ возрастает, то ковариация будет положительной. Если $x_i$ уменьшается, когда $y_i$ возрастает, ковариация будет отрицательной. Если переменные не имеют явной связи, ковариация будет близка к нулю.\n",
        "\n",
        "**Знаменатель**:\n",
        "\n",
        "- $\\sqrt{[n\\sum x_i^2 - (\\sum x)^2][n\\sum y_i^2 - (\\sum y_i)^2]}$\n",
        "\n",
        "Эта часть формулы представляет собой произведение стандартных отклонений $x_i$ и $y_i$. Стандартное отклонение - это мера разброса значений переменной вокруг ее среднего значения. Знаменатель нормализует ковариацию, делая коэффициент корреляции Пирсона масштабно-инвариантным, т.е. он не зависит от масштаба измерения переменных.\n",
        "\n",
        "**Итоговый коэффициент корреляции**:\n",
        "\n",
        "- **Коэффициент Пирсона стремится к нулю**: Это означает, что между переменными нет линейной зависимости. В контексте модели линейной регрессии это может быть хорошо, если предикторы действительно не коррелируют с зависимой переменной. Это позволяет модели получить независимую информацию от каждого предиктора, что может улучшить качество прогнозов и интерпретируемость модели.\n",
        "\n",
        "- **Коэффициент Пирсона стремится к 1**: Это означает сильную положительную линейную зависимость между переменными. В контексте модели линейной регрессии это может быть плохо, если предикторы сильно коррелируют между собой (феномен мультиколлинеарности). Мультиколлинеарность может привести к нестабильным оценкам коэффициентов и усложнить интерпретацию результатов. В таких случаях модель может стать менее надежной и менее эффективной в прогнозировании новых данных.\n",
        "\n",
        "Таким образом, коэффициент корреляции Пирсона представляет собой нормализованную ковариацию между двумя переменными. Это дает нам меру силы и направления линейной связи между переменными.\n",
        "\n",
        "В контексте линейной регрессии коэффициент Пирсона может быть использован для оценки степени линейной зависимости между независимыми и зависимой переменными. Это может помочь в определении того, насколько переменные подходят для использования в модели линейной регрессии.\n",
        "\n",
        "### Пример\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Два небольших датасета\n",
        "dataset1 = [10, 12, 15, 18, 20]\n",
        "dataset2 = [25, 30, 35, 40, 45]\n",
        "\n",
        "# Рассчитываем коэффициент корреляции Пирсона с помощью scipy.stats.pearsonr\n",
        "correlation_coefficient, p_value = pearsonr(dataset1, dataset2)\n",
        "\n",
        "print(f\"Датасет 1: {dataset1}\")\n",
        "print(f\"Датасет 2: {dataset2}\")\n",
        "print(f\"Коэффициент корреляции Пирсона: {correlation_coefficient:.3f}\")\n",
        "print(f\"P-значение: {p_value:.3f}\")\n",
        "\n",
        "print(\"\\n---\")\n",
        "\n",
        "# Альтернативный способ с использованием numpy.corrcoef\n",
        "correlation_matrix = np.corrcoef(dataset1, dataset2)\n",
        "\n",
        "# Коэффициент корреляции находится на пересечении недиагональных элементов матрицы\n",
        "correlation_coefficient_np = correlation_matrix[0, 1]\n",
        "\n",
        "print(f\"Коэффициент корреляции Пирсона (numpy.corrcoef): {correlation_coefficient_np:.3f}\")\n",
        "```\n",
        "\n",
        "**Разбор кода:**\n",
        "\n",
        "1. **`import numpy as np`**: Импортируем библиотеку `numpy` для работы с массивами.\n",
        "2. **`from scipy.stats import pearsonr`**: Импортируем функцию `pearsonr` из модуля `scipy.stats` для расчета коэффициента корреляции Пирсона.\n",
        "3. **`dataset1 = [...]` и `dataset2 = [...]`**: Создаем два небольших списка, представляющих наши датасеты. В данном примере датасеты имеют положительную корреляцию (с увеличением значений в `dataset1` значения в `dataset2` также увеличиваются).\n",
        "4. **`correlation_coefficient, p_value = pearsonr(dataset1, dataset2)`**:\n",
        "   - Вызываем функцию `pearsonr`, передавая ей два датасета в качестве аргументов.\n",
        "   - Функция возвращает два значения:\n",
        "     - **`correlation_coefficient`**: Сам коэффициент корреляции Пирсона.\n",
        "     - **`p_value`**: P-значение, которое показывает статистическую значимость корреляции.\n",
        "5. **`print(...)`**: Выводим исходные датасеты, рассчитанный коэффициент корреляции и p-значение. Форматирование `:.3f` используется для отображения числа с тремя знаками после запятой.\n",
        "6. **`print(\"\\n---\")`**: Разделитель для наглядности.\n",
        "7. **`correlation_matrix = np.corrcoef(dataset1, dataset2)`**:\n",
        "   - Используем функцию `np.corrcoef` из библиотеки `numpy`. Эта функция вычисляет матрицу корреляции.\n",
        "   - Для двух входных массивов матрица корреляции будет иметь размер 2x2:\n",
        "     ```\n",
        "     [[cor(dataset1, dataset1), cor(dataset1, dataset2)],\n",
        "      [cor(dataset2, dataset1), cor(dataset2, dataset2)]]\n",
        "     ```\n",
        "     - Диагональные элементы всегда равны 1 (корреляция массива с самим собой).\n",
        "     - Недиагональные элементы (верхний правый и нижний левый) содержат коэффициент корреляции между двумя датасетами.\n",
        "8. **`correlation_coefficient_np = correlation_matrix[0, 1]`**: Извлекаем коэффициент корреляции из матрицы. `correlation_matrix[0, 1]` соответствует элементу в первой строке и втором столбце (корреляция `dataset1` и `dataset2`).\n",
        "9. **`print(...)`**: Выводим коэффициент корреляции, рассчитанный с помощью `numpy.corrcoef`.\n",
        "\n",
        "**Интерпретация коэффициента корреляции Пирсона:**\n",
        "\n",
        "* Коэффициент корреляции Пирсона принимает значения от -1 до +1.\n",
        "* **+1**: Означает идеальную положительную линейную связь между двумя наборами данных (когда одна переменная увеличивается, другая также увеличивается строго пропорционально).\n",
        "* **-1**: Означает идеальную отрицательную линейную связь (когда одна переменная увеличивается, другая уменьшается строго пропорционально).\n",
        "* **0**: Означает отсутствие линейной связи между переменными.\n",
        "* Значения между 0 и 1 (или 0 и -1) указывают на степень положительной (или отрицательной) линейной связи. Чем ближе значение к 1 или -1, тем сильнее связь.\n",
        "\n",
        "В приведенном примере, если коэффициент корреляции будет близок к 1, это будет указывать на сильную положительную линейную связь между `dataset1` и `dataset2`.\n",
        "\n",
        "**Когда использовать какой метод:**\n",
        "\n",
        "* **`scipy.stats.pearsonr`**:  Возвращает непосредственно коэффициент корреляции и p-значение, что удобно, если вам нужна только корреляция между двумя переменными и оценка ее статистической значимости.\n",
        "* **`numpy.corrcoef`**: Возвращает полную матрицу корреляции, что полезно, если у вас есть несколько переменных и вы хотите рассчитать корреляции между всеми парами переменных одновременно.\n",
        "\n",
        "---\n",
        "\n",
        "**Коэффициент корреляции Спирмана:**\n",
        "\n",
        "Коэффициент корреляции Спирмана ($r_s$), также известный как ранговый коэффициент корреляции Спирмана, является непараметрической мерой монотонной связи между двумя переменными. В отличие от коэффициента корреляции Пирсона, который измеряет линейную зависимость между переменными, коэффициент Спирмана оценивает, насколько хорошо связь между двумя переменными может быть описана монотонной функцией (возрастающей или убывающей), даже если эта связь не является строго линейной.\n",
        "\n",
        "**1. Суть ранговой корреляции**\n",
        "\n",
        "Основная идея коэффициента Спирмана заключается в преобразовании исходных значений переменных в их ранги. Ранг – это порядковый номер значения в отсортированном наборе данных. Вместо того чтобы работать с абсолютными значениями, мы анализируем относительное положение каждого наблюдения в своих соответствующих выборках.\n",
        "\n",
        "**1.1. Процесс ранжирования**\n",
        "\n",
        "Для каждой переменной (обозначим их как X и Y) значения упорядочиваются от наименьшего к наибольшему. Затем каждому значению присваивается ранг в соответствии с его положением в этом упорядоченном списке.\n",
        "\n",
        "* **Отсутствие связей:** Если все значения в наборе данных уникальны, ранги присваиваются последовательно: наименьшему значению присваивается ранг 1, следующему – ранг 2 и так далее.\n",
        "\n",
        "* **Наличие связей (одинаковых значений):** Если в наборе данных есть одинаковые значения, им присваивается средний ранг. Например, если три значения занимают 5-е, 6-е и 7-е места, каждому из них присваивается ранг (5 + 6 + 7) / 3 = 6.\n",
        "\n",
        "**Пример ранжирования:**\n",
        "\n",
        "Рассмотрим два набора данных:\n",
        "\n",
        "| Наблюдение | Переменная X | Переменная Y | Ранг X | Ранг Y |\n",
        "|------------|--------------|--------------|--------|--------|\n",
        "| 1          | 15           | 25           | 2      | 1      |\n",
        "| 2          | 10           | 30           | 1      | 2      |\n",
        "| 3          | 25           | 40           | 3      | 3      |\n",
        "| 4          | 30           | 55           | 4      | 4      |\n",
        "| 5          | 40           | 70           | 5      | 5      |\n",
        "\n",
        "Здесь значения каждой переменной были отсортированы, и им были присвоены соответствующие ранги.\n",
        "\n",
        "**2. Математическая формализация коэффициента Спирмана**\n",
        "\n",
        "Как уже было указано, коэффициент корреляции Спирмана $r_s$ рассчитывается по формуле:\n",
        "\n",
        "$$\n",
        "r_s = 1 - \\frac{6 \\sum_{i=1}^{n} d_i^2}{n(n^2 - 1)}\n",
        "$$\n",
        "\n",
        "где:\n",
        "\n",
        "* $r_s$ – коэффициент корреляции Спирмана.\n",
        "* $d_i$ – разность между рангами $i$-го наблюдения для двух переменных: $d_i = \\text{Ранг}(X_i) - \\text{Ранг}(Y_i)$.\n",
        "* $\\sum_{i=1}^{n} d_i^2$ – сумма квадратов разностей рангов по всем $n$ наблюдениям.\n",
        "* $n$ – количество пар наблюдений (размер выборки).\n",
        "\n",
        "**2.1. Детальный разбор компонентов формулы**\n",
        "\n",
        "* **$d_i$ (Разность рангов):**  Этот компонент отражает, насколько сильно различаются ранги конкретного наблюдения в двух переменных. Если наблюдение занимает схожие позиции в ранжированных списках обеих переменных, $d_i$ будет близко к нулю. Большие значения $|d_i|$ указывают на значительное расхождение в рангах.\n",
        "\n",
        "* **$d_i^2$ (Квадрат разности рангов):** Квадрирование разностей рангов необходимо для того, чтобы:\n",
        "    * Устранить влияние знака разности. Нас интересует величина расхождения, а не его направление.\n",
        "    * Увеличить вклад больших разностей в общую сумму. Это делает коэффициент Спирмана более чувствительным к значительным расхождениям в рангах.\n",
        "\n",
        "* **$\\sum_{i=1}^{n} d_i^2$ (Сумма квадратов разностей рангов):** Эта сумма агрегирует информацию о расхождениях в рангах по всем наблюдениям. Чем больше эта сумма, тем меньше согласованность между рангами двух переменных.\n",
        "\n",
        "* **$n(n^2 - 1)$ (Нормирующий знаменатель):** Этот знаменатель используется для нормализации коэффициента $r_s$ в диапазоне от -1 до +1. Он зависит только от размера выборки $n$. Вывод этого знаменателя связан с суммой квадратов первых $n$ натуральных чисел и используется для масштабирования результата.\n",
        "\n",
        "* **Множитель 6:** Этот множитель является константой, возникающей из математического вывода формулы коэффициента Спирмана при отсутствии связей в рангах.\n",
        "\n",
        "**2.2. Альтернативная формула при наличии связей**\n",
        "\n",
        "При наличии связей в рангах (одинаковых значений) приведенная выше формула является приближенной. Более точный расчет можно выполнить, используя формулу коэффициента корреляции Пирсона, примененную к рангам переменных:\n",
        "\n",
        "$$\n",
        "r_s = \\frac{\\text{Cov}(\\text{Ранг}(X), \\text{Ранг}(Y))}{\\sigma_{\\text{Ранг}(X)} \\sigma_{\\text{Ранг}(Y)}} = \\frac{n(\\sum x_i y_i) - (\\sum x_i)(\\sum y_i)}{\\sqrt{[n\\sum x_i^2 - (\\sum x)^2][n\\sum y_i^2 - (\\sum y_i)^2]}}\n",
        "$$\n",
        "\n",
        "где:\n",
        "\n",
        "* $\\text{Cov}(\\text{Ранг}(X), \\text{Ранг}(Y))$ – ковариация рангов переменных X и Y.\n",
        "* $\\sigma_{\\text{Ранг}(X)}$ – стандартное отклонение рангов переменной X.\n",
        "* $\\sigma_{\\text{Ранг}(Y)}$ – стандартное отклонение рангов переменной Y.\n",
        "* $x$ и $y$ — значения переменных.\n",
        "* $n$ — количество наблюдений.\n",
        "\n",
        "**Числитель**:\n",
        "\n",
        "- $ n(\\sum x_i y_i) - (\\sum x_i)(\\sum y_i)$\n",
        "\n",
        "Эта часть формулы представляет \"ковариацию\" между $x_i$ и $y_i$. Ковариация показывает, насколько переменные изменяются вместе. Если $x_i$ возрастает, когда $y_i$ возрастает, то ковариация будет положительной. Если $x_i$ уменьшается, когда $y_i$ возрастает, ковариация будет отрицательной. Если переменные не имеют явной связи, ковариация будет близка к нулю.\n",
        "\n",
        "**Знаменатель**:\n",
        "\n",
        "- $\\sqrt{[n\\sum x_i^2 - (\\sum x)^2][n\\sum y_i^2 - (\\sum y_i)^2]}$\n",
        "\n",
        "Эта часть формулы представляет собой произведение стандартных отклонений $x_i$ и $y_i$. Стандартное отклонение - это мера разброса значений переменной вокруг ее среднего значения. Знаменатель нормализует ковариацию, делая коэффициент корреляции Пирсона масштабно-инвариантным, т.е. он не зависит от масштаба измерения переменных.\n",
        "\n",
        "Эта формула учитывает влияние связей на дисперсию рангов и обеспечивает более точный результат. Однако, если количество связей невелико, первая формула дает достаточно хорошее приближение.\n",
        "\n",
        "**3. Интерпретация коэффициента корреляции Спирмана**\n",
        "\n",
        "Коэффициент $r_s$ принимает значения в диапазоне от -1 до +1, где:\n",
        "\n",
        "* **$r_s = +1$:**  Идеальная прямая монотонная связь. Это означает, что при увеличении ранга одной переменной ранг другой переменной также всегда увеличивается, и наоборот.\n",
        "\n",
        "* **$r_s = -1$:** Идеальная обратная монотонная связь. Это означает, что при увеличении ранга одной переменной ранг другой переменной всегда уменьшается, и наоборот.\n",
        "\n",
        "* **$r_s = 0$:** Отсутствие монотонной связи между переменными. Ранги переменных не связаны между собой каким-либо последовательным образом.\n",
        "\n",
        "* **$0 < |r_s| < 1$:**  Указывает на наличие монотонной связи определенной силы. Чем ближе $|r_s|$ к 1, тем сильнее монотонная связь.\n",
        "\n",
        "**4. Преимущества коэффициента корреляции Спирмана**\n",
        "\n",
        "* **Устойчивость к выбросам:** Поскольку коэффициент Спирмана основан на рангах, а не на абсолютных значениях, он менее чувствителен к экстремальным значениям (выбросам). Выбросы могут существенно исказить коэффициент корреляции Пирсона, но их влияние на ранги обычно ограничено.\n",
        "\n",
        "* **Применимость к нелинейным монотонным связям:** Коэффициент Спирмана может выявлять монотонные связи, которые не являются линейными. Если при увеличении одной переменной другая переменная систематически увеличивается (или уменьшается), даже если темп изменения не постоянен, коэффициент Спирмана это зафиксирует.\n",
        "\n",
        "* **Применимость к порядковым данным:** Коэффициент Спирмана подходит для анализа связи между порядковыми переменными (например, оценки удовлетворенности по шкале от 1 до 5), где абсолютные значения не имеют строгого количественного смысла, но порядок важен.\n",
        "\n",
        "**5. Недостатки и ограничения коэффициента корреляции Спирмана**\n",
        "\n",
        "* **Потеря информации:** Преобразование данных в ранги приводит к потере информации об абсолютных различиях между значениями. Коэффициент Спирмана фокусируется только на порядке.\n",
        "\n",
        "* **Менее мощный, чем коэффициент Пирсона при линейной связи:** Если связь между переменными действительно линейна и данные не содержат выбросов, коэффициент корреляции Пирсона обычно является более мощным инструментом, то есть он с большей вероятностью обнаружит существующую связь.\n",
        "\n",
        "* **Не подходит для выявления немонотонных связей:** Коэффициент Спирмана предназначен для измерения монотонных связей. Если связь между переменными является, например, U-образной или инвертированной U-образной, коэффициент Спирмана может быть близок к нулю, даже если существует сильная, но немонотонная зависимость.\n",
        "\n",
        "**6. Ложная корреляция и причинно-следственная связь**\n",
        "\n",
        "Важно подчеркнуть, что наличие корреляции (как по Пирсону, так и по Спирману) не означает наличия причинно-следственной связи между переменными. **Корреляция не подразумевает причинность (Correlation does not imply causation).**\n",
        "\n",
        "**Ложная корреляция (Spurious correlation)** возникает, когда две переменные кажутся связанными между собой, но на самом деле их связь обусловлена влиянием третьей, не учтенной переменной (скрытой переменной или вмешивающейся переменной).\n",
        "\n",
        "**Примеры ложной корреляции:**\n",
        "\n",
        "* **Продажи мороженого и количество утоплений:** Наблюдается положительная корреляция между продажами мороженого и количеством случаев утопления. Однако это не означает, что поедание мороженого приводит к утоплению. Обе переменные зависят от третьей переменной – температуры воздуха. В жаркую погоду люди чаще покупают мороженое и чаще купаются, что, к сожалению, может привести к увеличению числа утоплений.\n",
        "\n",
        "* **Количество пожарных машин на месте пожара и ущерб от пожара:** Наблюдается положительная корреляция между количеством пожарных машин, прибывших на место пожара, и размером ущерба от пожара. Очевидно, что отправка большего количества пожарных машин не увеличивает ущерб. Скорее, более серьезные пожары требуют большего количества пожарных машин.\n",
        "\n",
        "**Важность различения корреляции и причинно-следственной связи:**\n",
        "\n",
        "Понимание разницы между корреляцией и причинно-следственной связью критически важно для принятия обоснованных решений на основе данных. Ошибочное заключение о причинно-следственной связи на основе корреляции может привести к неэффективным или даже вредным действиям.\n",
        "\n",
        "Для установления причинно-следственной связи необходимы дополнительные методы исследования, такие как контролируемые эксперименты, анализ временных рядов с учетом лагов, и построение каузальных моделей.\n",
        "\n",
        "**В заключение:**\n",
        "\n",
        "Коэффициент корреляции Спирмана является ценным инструментом для анализа монотонных связей между переменными, особенно в ситуациях, когда данные содержат выбросы или не соответствуют предположениям, необходимым для использования коэффициента Пирсона. Однако важно помнить о его ограничениях и всегда интерпретировать корреляцию в контексте возможного влияния третьих переменных и не путать ее с причинно-следственной связью. Понимание этих нюансов позволяет более точно и эффективно использовать статистические методы для анализа данных.\n",
        "\n",
        "### Пример\n",
        "\n",
        "```python\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "# Два небольших датасета\n",
        "dataset1 = [10, 12, 15, 18, 20]\n",
        "dataset2 = [25, 30, 35, 38, 42]\n",
        "\n",
        "# Рассчитываем коэффициент корреляции Спирмана с помощью scipy.stats.spearmanr\n",
        "correlation_coefficient, p_value = spearmanr(dataset1, dataset2)\n",
        "\n",
        "print(f\"Датасет 1: {dataset1}\")\n",
        "print(f\"Датасет 2: {dataset2}\")\n",
        "print(f\"Коэффициент корреляции Спирмана: {correlation_coefficient:.3f}\")\n",
        "print(f\"P-значение: {p_value:.3f}\")\n",
        "\n",
        "# Интерпретация коэффициента корреляции Спирмана:\n",
        "# - Значения от -1 до +1.\n",
        "# - +1: Идеальная монотонная возрастающая связь.\n",
        "# - -1: Идеальная монотонная убывающая связь.\n",
        "# - 0: Отсутствие монотонной связи.\n",
        "```\n",
        "\n",
        "**Разбор кода:**\n",
        "\n",
        "1. **`from scipy.stats import spearmanr`**: Импортируем функцию `spearmanr` из модуля `scipy.stats`. Эта функция предназначена для расчета коэффициента корреляции Спирмана.\n",
        "2. **`dataset1 = [...]` и `dataset2 = [...]`**: Создаем два небольших списка, представляющих наши датасеты.\n",
        "3. **`correlation_coefficient, p_value = spearmanr(dataset1, dataset2)`**:\n",
        "   - Вызываем функцию `spearmanr`, передавая ей два датасета.\n",
        "   - Функция возвращает два значения:\n",
        "     - **`correlation_coefficient`**: Коэффициент корреляции Спирмана.\n",
        "     - **`p_value`**: P-значение, оценивающее статистическую значимость корреляции.\n",
        "4. **`print(...)`**: Выводим исходные датасеты, рассчитанный коэффициент корреляции Спирмана и p-значение. Форматирование `:.3f` используется для отображения числа с тремя знаками после запятой.\n",
        "5. **Комментарий**: Приводится краткое описание интерпретации коэффициента корреляции Спирмана.\n",
        "\n",
        "**Как работает корреляция Спирмана:**\n",
        "\n",
        "В отличие от корреляции Пирсона, которая измеряет *линейную* зависимость, корреляция Спирмана измеряет *монотонную* зависимость между двумя переменными. Это означает, что она оценивает, насколько хорошо связь между двумя наборами данных может быть описана монотонной функцией (возрастающей или убывающей), даже если эта связь не является строго линейной.\n",
        "\n",
        "**Шаги расчета корреляции Спирмана:**\n",
        "\n",
        "1. **Ранжирование данных:**  Значения в каждом датасете заменяются их рангами (порядковыми номерами). При наличии одинаковых значений им присваивается средний ранг.\n",
        "2. **Расчет разностей рангов:** Для каждой пары наблюдений вычисляется разность между рангами соответствующих значений.\n",
        "3. **Применение формулы:** Коэффициент корреляции Спирмана рассчитывается на основе этих разностей рангов по формуле, которая похожа на формулу для коэффициента корреляции Пирсона, но применяется к рангам, а не к исходным значениям.\n",
        "\n",
        "**Интерпретация коэффициента корреляции Спирмана:**\n",
        "\n",
        "* Коэффициент корреляции Спирмана принимает значения от -1 до +1.\n",
        "* **+1**: Означает идеальную монотонную возрастающую связь. Это значит, что по мере увеличения значений в одном датасете, значения в другом датасете также всегда увеличиваются (не обязательно линейно).\n",
        "* **-1**: Означает идеальную монотонную убывающую связь. По мере увеличения значений в одном датасете, значения в другом датасете всегда уменьшаются.\n",
        "* **0**: Означает отсутствие монотонной связи между переменными.\n",
        "\n",
        "В приведенном примере, если коэффициент корреляции Спирмана будет близок к 1, это будет указывать на сильную монотонную возрастающую связь между `dataset1` и `dataset2`.\n",
        "\n",
        "**Когда использовать корреляцию Спирмана:**\n",
        "\n",
        "* Когда связь между переменными не является обязательно линейной, но предполагается монотонная зависимость.\n",
        "* Когда данные содержат выбросы, которые могут сильно влиять на коэффициент корреляции Пирсона (ранги менее чувствительны к выбросам).\n",
        "* Когда данные представлены в порядковой шкале (например, оценки качества).\n"
      ],
      "metadata": {
        "id": "nnqRh4RKKaal"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **Вопрос 30: Центральная предельная теорема.**\n",
        "\n",
        "### ЦПТ\n",
        "\n",
        "Центральная Предельная Теорема (ЦПТ) является одним из фундаментальных результатов в теории вероятностей и математической статистике. В своей сути, ЦПТ утверждает, что при определенных условиях сумма (или среднее арифметическое) большого числа независимых случайных величин, имеющих произвольное исходное распределение, будет приближаться к нормальному распределению. Это удивительное свойство делает нормальное распределение \"вездесущим\" в статистическом анализе.\n",
        "\n",
        "**1. Основная идея ЦПТ: От хаоса к порядку**\n",
        "\n",
        "Представьте себе множество случайных процессов, каждый из которых имеет свое собственное, возможно, весьма причудливое распределение. ЦПТ говорит нам, что если мы возьмем достаточно много таких процессов и сложим их результаты, то распределение этой суммы будет выглядеть как знакомый \"колокол\" нормального распределения. Это означает, что, несмотря на разнообразие исходных распределений, их совокупное влияние приводит к предсказуемому и хорошо изученному нормальному распределению.\n",
        "\n",
        "**2. Ключевые компоненты ЦПТ**\n",
        "\n",
        "Для понимания ЦПТ важно разобрать ее ключевые составляющие:\n",
        "\n",
        "* **Независимые случайные величины:**  ЦПТ предполагает, что случайные величины, которые мы суммируем, являются статистически независимыми. Это означает, что значение одной случайной величины не влияет на значение другой.\n",
        "\n",
        "* **Идентично распределенные (не обязательно, но часто рассматривается):** В классической формулировке ЦПТ часто предполагается, что случайные величины имеют одинаковое распределение. Однако существуют версии ЦПТ, которые ослабляют это требование. Важнее, чтобы ни одна отдельная случайная величина не доминировала в сумме.\n",
        "\n",
        "* **Конечное математическое ожидание и дисперсия:** Каждая случайная величина должна иметь конечное математическое ожидание (среднее значение) и конечную дисперсию (меру разброса). Это гарантирует, что ни одна из величин не вносит бесконечно большой вклад в сумму.\n",
        "\n",
        "* **Сумма (или среднее) случайных величин:** ЦПТ относится к распределению суммы (или, эквивалентно, среднего арифметического) этих случайных величин.\n",
        "\n",
        "* **Сходимость к нормальному распределению:**  Ключевой момент ЦПТ заключается в том, что распределение суммы (или среднего) приближается к нормальному распределению по мере увеличения числа слагаемых.\n",
        "\n",
        "**3. Условия применимости ЦПТ: Когда \"колокол\" появляется**\n",
        "\n",
        "Хотя ЦПТ является мощным инструментом, важно понимать условия, при которых она применима:\n",
        "\n",
        "* **Независимость:**  Это критическое условие. Если случайные величины сильно зависимы, ЦПТ может не выполняться.\n",
        "\n",
        "* **Конечность моментов:**  Требование конечного математического ожидания и дисперсии гарантирует, что ни одна случайная величина не является \"слишком большой\" или \"слишком изменчивой\".\n",
        "\n",
        "* **Размер выборки (n):**  \"Большое число\" в формулировке ЦПТ является ключевым. На практике, насколько большим должно быть это число, зависит от формы исходного распределения.\n",
        "    * **Симметричные и умеренно скошенные распределения:** Для таких распределений ЦПТ начинает \"работать\" довольно быстро, иногда уже при $n \\approx 30$.\n",
        "    * **Сильно скошенные распределения:** Для сильно скошенных распределений может потребоваться большее значение $n$ (например, $n > 50$ или даже $n > 100$) для того, чтобы распределение суммы приблизилось к нормальному.\n",
        "\n",
        "**4. Математическая формализация ЦПТ: Строгость и точность**\n",
        "\n",
        "Центральная предельная теорема может быть сформулирована более строго математически. Рассмотрим последовательность независимых и одинаково распределенных $(i.i.d.)$ случайных величин $X_1, X_2, \\ldots, X_n$ с математическим ожиданием $E[X_i] = \\mu$ и дисперсией $\\text{Var}(X_i) = \\sigma^2$.\n",
        "\n",
        "Обозначим сумму этих случайных величин как $S_n = X_1 + X_2 + \\ldots + X_n$.\n",
        "\n",
        "Математическое ожидание суммы: $E[S_n] = E[X_1] + E[X_2] + \\ldots + E[X_n] = n\\mu$.\n",
        "\n",
        "Дисперсия суммы (в силу независимости): $\\text{Var}(S_n) = \\text{Var}(X_1) + \\text{Var}(X_2) + \\ldots + \\text{Var}(X_n) = n\\sigma^2$.\n",
        "\n",
        "Стандартное отклонение суммы: $\\text{SD}(S_n) = \\sqrt{\\text{Var}(S_n)} = \\sigma \\sqrt{n}$.\n",
        "\n",
        "Чтобы стандартизировать сумму $S_n$ (преобразовать ее так, чтобы она имела нулевое среднее и единичную дисперсию), мы вычитаем ее математическое ожидание и делим на стандартное отклонение:\n",
        "\n",
        "$$\n",
        "Z_n = \\frac{S_n - E[S_n]}{\\text{SD}(S_n)} = \\frac{S_n - n\\mu}{\\sigma \\sqrt{n}}\n",
        "$$\n",
        "\n",
        "Центральная предельная теорема утверждает, что распределение стандартизированной суммы $Z_n$ сходится к стандартному нормальному распределению $N(0, 1)$ при $n \\to \\infty$. Это обозначается как:\n",
        "\n",
        "$$\n",
        "Z_n \\xrightarrow{d} N(0, 1)\n",
        "$$\n",
        "\n",
        "где $\\xrightarrow{d}$ обозначает сходимость по распределению.\n",
        "\n",
        "**4.1. Разъяснение математической формулировки**\n",
        "\n",
        "* **$Z_n$:**  Это стандартизированная версия суммы случайных величин. Стандартизация позволяет сравнивать распределения сумм с разными средними и дисперсиями.\n",
        "\n",
        "* **$N(0, 1)$:** Это стандартное нормальное распределение, которое имеет математическое ожидание 0 и дисперсию 1. Его функция плотности вероятности имеет классическую колоколообразную форму.\n",
        "\n",
        "* **Сходимость по распределению:**  Это тип сходимости случайных величин. Говоря простым языком, это означает, что функция распределения $Z_n$ приближается к функции распределения стандартного нормального распределения по мере увеличения $n$.\n",
        "\n",
        "**5. Визуализация ЦПТ: От кубиков к колоколу**\n",
        "\n",
        "Представьте себе бросание игрального кубика. Распределение результатов (1, 2, 3, 4, 5, 6) является равномерным. Теперь представьте, что вы бросаете два кубика и складываете результаты. Распределение суммы уже не будет равномерным; значения в середине (например, 7) будут встречаться чаще, чем крайние значения (2 или 12).\n",
        "\n",
        "По мере увеличения числа бросаемых кубиков и суммирования результатов, форма распределения суммы будет становиться все более и более похожей на колокол нормального распределения. Это наглядная иллюстрация работы ЦПТ.\n",
        "\n",
        "**6. Вариации ЦПТ: Расширение горизонтов**\n",
        "\n",
        "Существуют различные варианты Центральной Предельной Теоремы, которые ослабляют некоторые из исходных предположений:\n",
        "\n",
        "* **Теорема Ляпунова:**  Эта теорема ослабляет требование идентичной распределенности случайных величин. Она требует, чтобы выполнялось определенное условие на моменты третьего порядка (условие Ляпунова).\n",
        "\n",
        "* **Теорема Линдеберга-Феллера:**  Это еще более общее условие, которое также позволяет рассматривать случайные величины с разными распределениями.\n",
        "\n",
        "Эти обобщения делают ЦПТ применимой к еще более широкому кругу задач.\n",
        "\n",
        "**7. Практическое значение ЦПТ: Почему она так важна?**\n",
        "\n",
        "Центральная Предельная Теорема имеет огромное практическое значение в статистике и анализе данных:\n",
        "\n",
        "* **Обоснование использования нормального распределения:**  Многие статистические методы (например, t-тесты, ANOVA, построение доверительных интервалов) основаны на предположении о нормальности распределения данных или выборочных статистик. ЦПТ предоставляет теоретическое обоснование для этого предположения, особенно при работе с большими выборками.\n",
        "\n",
        "* **Статистический вывод:** ЦПТ позволяет делать выводы о генеральной совокупности на основе выборочных данных. Например, мы можем использовать выборочное среднее для оценки среднего значения в генеральной совокупности и построить доверительный интервал, опираясь на нормальное распределение, гарантированное ЦПТ.\n",
        "\n",
        "* **Моделирование случайных явлений:**  Многие реальные явления можно рассматривать как результат сложения большого числа независимых случайных факторов. ЦПТ объясняет, почему нормальное распределение так часто встречается при моделировании таких явлений (например, ошибки измерений, колебания цен на акции).\n",
        "\n",
        "**8. Связь с предоставленным текстом: Нормальность и выбросы**\n",
        "\n",
        "Предоставленный фрагмент правильно подчеркивает важность ЦПТ для анализа данных, поскольку многие статистические методы действительно предполагают нормальность распределения. ЦПТ объясняет, почему выборочные средние (и суммы) часто имеют приблизительно нормальное распределение, даже если исходные данные не являются нормальными.\n",
        "\n",
        "Однако фрагмент также упоминает, как выбросы могут исказить результаты анализа и как важно учитывать их при вычислении коэффициентов корреляции. Хотя ЦПТ сама по себе не решает проблему выбросов, понимание ее принципов помогает осознать, что при достаточно большом размере выборки влияние отдельных выбросов на распределение выборочного среднего будет уменьшаться (хотя они все равно могут влиять на дисперсию).\n",
        "\n",
        "Важно отметить, что ЦПТ говорит о распределении *суммы* или *среднего*, а не о распределении самих исходных данных. Выбросы в исходных данных могут привести к тому, что для достижения \"достаточно нормального\" распределения суммы потребуется большее значение $n$.\n",
        "\n",
        "**9. Ограничения ЦПТ: Когда \"колокол\" не появляется**\n",
        "\n",
        "Несмотря на свою мощь, ЦПТ имеет ограничения:\n",
        "\n",
        "* **Недостаточно большой размер выборки:** Если размер выборки $n$ слишком мал, распределение суммы может существенно отличаться от нормального.\n",
        "\n",
        "* **Сильная зависимость:** Если случайные величины сильно зависимы, ЦПТ может не выполняться.\n",
        "\n",
        "* **Распределения с \"тяжелыми хвостами\":** Для распределений с очень \"тяжелыми хвостами\" (например, распределение Коши), у которых не существует конечной дисперсии, ЦПТ в классической формулировке не применима.\n",
        "\n",
        "**Программная реализация:**\n",
        "Пример кода для симуляции центральной предельной теоремы и вычисления коэффициента корреляции:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from scipy.stats import norm, pearsonr\n",
        "\n",
        "def central_limit_theorem_simulation(n: int, num_samples: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Description:\n",
        "        Симуляция центральной предельной теоремы.\n",
        "\n",
        "    Args:\n",
        "        n: int - количество случайных величин\n",
        "        num_samples: int - количество выборок\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: массив средних значений выборок\n",
        "    \"\"\"\n",
        "    samples = np.random.uniform(0, 1, (num_samples, n))\n",
        "    sample_means = np.mean(samples, axis=1)\n",
        "    return sample_means\n",
        "\n",
        "def calculate_pearson_correlation(x: np.ndarray, y: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Description:\n",
        "        Вычисляет коэффициент корреляции Пирсона между двумя случайными величинами.\n",
        "\n",
        "    Args:\n",
        "        x: np.ndarray - массив значений первой случайной величины\n",
        "        y: np.ndarray - массив значений второй случайной величины\n",
        "\n",
        "    Returns:\n",
        "        float: коэффициент корреляции Пирсона между x и y\n",
        "    \"\"\"\n",
        "    correlation, _ = pearsonr(x, y)\n",
        "    return correlation\n",
        "\n",
        "# Модульные тесты\n",
        "def test_central_limit_theorem_simulation():\n",
        "    means = central_limit_theorem_simulation(30, 1000)\n",
        "    assert np.allclose(np.mean(means), 0.5, atol=0.05), \"Среднее должно быть близко к 0.5\"\n",
        "\n",
        "def test_calculate_pearson_correlation():\n",
        "    x = np.array([1, 2, 3, 4, 5])\n",
        "    y = np.array([2, 4, 6, 8, 10])\n",
        "    correlation = calculate_pearson_correlation(x, y)\n",
        "    assert correlation == 1.0, \"Коэффициент корреляции Пирсона должен быть 1 для зависимых величин\"\n",
        "```\n",
        "\n",
        "### Заключение\n",
        "\n",
        "Центральная Предельная Теорема является краеугольным камнем современной статистики. Она объясняет, почему нормальное распределение так распространено в природе и предоставляет мощный инструмент для статистического вывода. Понимание условий ее применимости и ограничений позволяет более эффективно использовать статистические методы для анализа данных и принятия обоснованных решений. Несмотря на то, что ЦПТ не решает всех проблем (например, проблему выбросов), она является фундаментальной концепцией, необходимой для глубокого понимания статистического анализа."
      ],
      "metadata": {
        "id": "AJ3VicmSMi-v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **Вопрос 31: Выборочный метод. Генеральная и выборочная совокупность. Репрезентативная выборка.**\n",
        "\n",
        "### **Вопрос 32: Конкретная выборка, случайная выборка, стратифицированная выборка.**\n",
        "\n",
        "## **Выборочный метод и его значение**\n",
        "\n",
        "Выборочный метод — это статистический подход, который позволяет исследовать и делать выводы о генеральной совокупности на основе анализа только части этой совокупности, называемой выборкой. Это особенно важно в ситуациях, когда полное исследование всей совокупности невозможно из-за ограничений по времени, ресурсам или доступности данных.\n",
        "\n",
        "Например, если мы хотим оценить успеваемость всех школьников в Российской Федерации, провести опрос среди всех 40 миллионов учеников было бы крайне сложно и затратно. Вместо этого мы можем выбрать, скажем, 1000 школ и провести опрос только среди них. На основе полученных данных мы можем сделать выводы о всей совокупности.\n",
        "\n",
        "### Математическая формализация\n",
        "\n",
        "Для оценки среднего значения в генеральной совокупности на основе выборки используется формула для выборочного среднего:\n",
        "\n",
        "$$\n",
        "\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $\\bar{x}$ — выборочное среднее;\n",
        "- $n$ — размер выборки (количество наблюдений);\n",
        "- $x_i$ — значения, полученные в выборке.\n",
        "\n",
        "Эта формула позволяет нам вычислить среднее значение на основе данных, собранных из выборки, и использовать его для оценки среднего значения в генеральной совокупности.\n",
        "\n",
        "## **Репрезентативная выборка и методы отбора**\n",
        "\n",
        "Репрезентативная выборка — это выборка, которая адекватно отражает характеристики генеральной совокупности. Это означает, что результаты, полученные на основе выборки, могут быть обобщены на всю совокупность. Если выборка не репрезентативна, то выводы, сделанные на ее основе, могут быть искажены.\n",
        "\n",
        "Существует несколько методов формирования выборки, которые помогают обеспечить ее репрезентативность:\n",
        "\n",
        "1. **Случайная выборка**: Элементы выбираются случайным образом, что позволяет избежать предвзятости. Существует два типа случайной выборки:\n",
        "   - **С повторением**: Элемент может быть выбран несколько раз.\n",
        "   - **Без повторения**: Элемент выбирается только один раз.\n",
        "\n",
        "2. **Стратифицированная выборка**: Генеральная совокупность делится на подгруппы (страты), и из каждой страты выбирается определенное количество элементов. Это позволяет учесть различные характеристики подгрупп, например, сильные и слабые школы.\n",
        "\n",
        "3. **Кластерная выборка**: Генеральная совокупность делится на кластеры, и выбираются целые кластеры для исследования. Это может быть полезно, когда исследование требует значительных ресурсов.\n",
        "\n",
        "### Математическая формализация\n",
        "\n",
        "Для оценки репрезентативности выборки можно использовать коэффициент вариации, который показывает, насколько выборка отклоняется от генеральной совокупности:\n",
        "\n",
        "$$\n",
        "CV = \\frac{\\sigma}{\\bar{x}} \\times 100\\%\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $CV$ — коэффициент вариации;\n",
        "- $\\sigma$ — стандартное отклонение выборки;\n",
        "- $\\bar{x}$ — выборочное среднее.\n",
        "\n",
        "Коэффициент вариации позволяет понять, насколько изменчивы данные в выборке по сравнению со средним значением. Низкий коэффициент вариации указывает на высокую степень однородности выборки."
      ],
      "metadata": {
        "id": "rZ9qeKc6MyIU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **Вопрос 33: Выборочное среднее, выборочная дисперсия, выборочная ковариация.**\n",
        "\n",
        "## **Выборочная средняя и выборочная дисперсия**\n",
        "\n",
        "Выборочная средняя и выборочная дисперсия — это ключевые статистические показатели, которые позволяют оценить характеристики генеральной совокупности на основе данных, собранных из выборки. Эти показатели помогают понять, как результаты выборки соотносятся с реальными значениями в генеральной совокупности.\n",
        "\n",
        "### Выборочная средняя\n",
        "\n",
        "Выборочная средняя ($\\bar{x}$) представляет собой среднее значение всех наблюдений в выборке и рассчитывается по следующей формуле:\n",
        "\n",
        "$$\n",
        "\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $\\bar{x}$ — выборочная средняя;\n",
        "- $n$ — объем выборки (количество наблюдений);\n",
        "- $x_i$ — значения, полученные в выборке.\n",
        "\n",
        "Выборочная средняя является аналогом математического ожидания и позволяет оценить среднее значение в генеральной совокупности.\n",
        "\n",
        "### Выборочная дисперсия\n",
        "\n",
        "Выборочная дисперсия ($s^2$) измеряет, насколько значения в выборке разбросаны относительно выборочной средней. Она рассчитывается по формуле:\n",
        "\n",
        "$$\n",
        "s^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $s^2$ — выборочная дисперсия;\n",
        "- $n$ — объем выборки;\n",
        "- $x_i$ — значения, полученные в выборке;\n",
        "- $\\bar{x}$ — выборочная средняя.\n",
        "\n",
        "Использование $n-1$ вместо $n$ в знаменателе позволяет получить более точную оценку дисперсии, так как это учитывает степень свободы.\n",
        "\n",
        "## **Выборочная ковариация**\n",
        "\n",
        "Выборочная ковариация — это мера линейной зависимости между двумя случайными величинами, основанная на данных выборки. Она показывает, как изменения одной переменной связаны с изменениями другой переменной.\n",
        "\n",
        "### Расчет выборочной ковариации\n",
        "\n",
        "Выборочная ковариация между двумя переменными $x$ и $y$ обозначается как $cov(x, y)$ или $s_{xy}$ и рассчитывается по следующей формуле:\n",
        "\n",
        "$$\n",
        "cov(x, y) = s_{xy} = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $cov(x, y)$ — выборочная ковариация между переменными $x$ и $y$;\n",
        "- $n$ — объем выборки;\n",
        "- $x_i$ — значения переменной $x$ в выборке;\n",
        "- $y_i$ — значения переменной $y$ в выборке, соответствующие $x_i$;\n",
        "- $\\bar{x}$ — выборочная средняя переменной $x$;\n",
        "- $\\bar{y}$ — выборочная средняя переменной $y$.\n",
        "\n",
        "Аналогично выборочной дисперсии, в знаменателе используется $n-1$ для получения несмещенной оценки ковариации генеральной совокупности.\n",
        "\n",
        "### Интерпретация выборочной ковариации\n",
        "\n",
        "Значение выборочной ковариации может быть положительным, отрицательным или близким к нулю:\n",
        "\n",
        "- **Положительная ковариация** ($cov(x, y) > 0$) указывает на то, что когда значения переменной $x$ выше своего среднего значения, значения переменной $y$, как правило, также выше своего среднего значения, и наоборот. Это говорит о прямой линейной зависимости между переменными.\n",
        "\n",
        "- **Отрицательная ковариация** ($cov(x, y) < 0$) указывает на то, что когда значения переменной $x$ выше своего среднего значения, значения переменной $y$, как правило, ниже своего среднего значения, и наоборот. Это говорит об обратной линейной зависимости между переменными.\n",
        "\n",
        "- **Ковариация, близкая к нулю** ($cov(x, y) \\approx 0$), предполагает слабую линейную связь между переменными. Важно отметить, что ковариация, равная нулю, не исключает наличия нелинейной зависимости между переменными.\n",
        "\n",
        "### Связь с выборочной дисперсией\n",
        "\n",
        "Выборочная дисперсия является частным случаем выборочной ковариации, когда рассматриваются две одинаковые переменные. То есть, $cov(x, x) = s_{xx} = s^2$, где $s^2$ — выборочная дисперсия переменной $x$.\n",
        "\n",
        "### Ограничения выборочной ковариации\n",
        "\n",
        "Величина ковариации зависит от масштаба переменных, что затрудняет сравнение силы связи между разными парами переменных, измеренных в разных единицах. Для стандартизации меры линейной зависимости используется коэффициент корреляции."
      ],
      "metadata": {
        "id": "Nmus0DTj6ZNZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **Вопрос 34: Точечные оценки параметров.**\n",
        "\n",
        "## **Оценка параметров и их свойства**\n",
        "\n",
        "Оценка параметров — это процесс, при котором мы используем данные из выборки для оценки характеристик генеральной совокупности. В статистике мы часто используем различные статистики, такие как выборочная средняя или выборочная дисперсия, в качестве приближенных значений для истинных параметров генеральной совокупности.\n",
        "\n",
        "### Оценка параметров\n",
        "\n",
        "Оценка параметра $\\theta$ (например, математического ожидания или дисперсии) может быть выполнена с помощью различных статистик, таких как:\n",
        "- Выборочная средняя ($\\bar{x}$)\n",
        "- Выборочная медиана\n",
        "- Выборочный мод\n",
        "- Минимальное и максимальное значение выборки\n",
        "\n",
        "Эти оценки помогают нам получить представление о характеристиках генеральной совокупности на основе ограниченного количества данных.\n",
        "\n",
        "### Свойства оценок\n",
        "\n",
        "Качество оценок можно определить по следующим свойствам:\n",
        "\n",
        "1. **Состоятельность**: Оценка называется состоятельной, если с увеличением объема выборки вероятность того, что оценка близка к истинному значению параметра, стремится к единице. Формально это можно записать как:\n",
        "\n",
        "$$\n",
        "\\lim_{n \\to \\infty} P(|\\hat{\\theta}_n - \\theta| < \\epsilon) = 1\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $\\hat{\\theta}_n$ — оценка параметра на основе выборки размером $n$;\n",
        "- $\\theta$ — истинное значение параметра;\n",
        "- $\\epsilon$ — любое положительное число.\n",
        "\n",
        "2. **Несмещенность**: Оценка называется несмещенной, если математическое ожидание оценки совпадает с истинным значением параметра:\n",
        "\n",
        "$$\n",
        "E[\\hat{\\theta}] = \\theta\n",
        "$$\n",
        "\n",
        "3. **Эффективность**: Оценка называется эффективной, если она имеет наименьшую дисперсию среди всех несмещенных оценок. Это означает, что оценка должна быть как можно более точной.\n",
        "\n",
        "### Физический и геометрический смысл\n",
        "\n",
        "Представьте, что вы проводите исследование, чтобы оценить средний рост студентов в университете. Вы выбираете случайную выборку из 100 студентов и измеряете их рост. Выборочная средняя даст вам представление о среднем росте в этой группе, а выборочная дисперсия покажет, насколько сильно рост студентов варьируется. Эти показатели помогут вам сделать выводы о росте студентов в целом, даже если вы не измеряли каждого студента в университете.\n",
        "\n",
        "Таким образом, оценка параметров и их свойства, такие как состоятельность, несмещенность и эффективность, являются важными инструментами для статистического анализа, позволяя оценивать характеристики генеральной совокупности на основе ограниченного количества данных."
      ],
      "metadata": {
        "id": "TL7QYQwT_ANH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **Вопрос 35: Метод моментов.**\n",
        "\n",
        "## **Методы построения точных оценок**\n",
        "\n",
        "Для оценки параметров генеральной совокупности используются различные методы, среди которых наиболее распространены метод моментов и метод максимального правдоподобия. Эти методы позволяют получить точные оценки параметров на основе данных выборки.\n",
        "\n",
        "### Метод моментов\n",
        "\n",
        "Метод моментов основан на использовании моментов выборки для оценки параметров. Моменты — это статистические характеристики, которые описывают распределение данных. Существует два типа моментов:\n",
        "- **Начальные моменты**: Это моменты, которые рассчитываются от значений выборки.\n",
        "- **Центральные моменты**: Это моменты, которые рассчитываются от отклонений значений выборки от выборочной средней.\n",
        "- **Начальные моменты:**  Это моменты, рассчитанные относительно нуля.\n",
        "\n",
        "Для оценки параметра $\\theta$ с помощью метода моментов, мы подставляем выборочные аналоги моментов вместо теоретических моментов. Например, если мы хотим оценить математическое ожидание, мы можем использовать выборочную среднюю.\n",
        "\n",
        "> **Ключевой момент:** Метод моментов использует **одну** выборку и ее моменты для оценки параметров **всей** генеральной совокупности, не разделяя ее на подгруппы.\n",
        "\n",
        "**Для теоретических распределений:**\n",
        "\n",
        "* **Начальные моменты (моменты относительно нуля):**\n",
        "    * **Первый начальный момент:** Математическое ожидание, обозначается как $E[X]$ или $\\mu$.\n",
        "    * **Второй начальный момент:** $E[X^2]$.\n",
        "    * **k-й начальный момент:** $E[X^k]$.\n",
        "\n",
        "* **Центральные моменты (моменты относительно среднего):**\n",
        "    * **Первый центральный момент:** Всегда равен нулю, $E[X - E[X]] = 0$.\n",
        "    * **Второй центральный момент:** Дисперсия, обозначается как $Var(X)$, $\\sigma^2$ или $E[(X - E[X])^2]$.\n",
        "    * **Третий центральный момент:** Мера асимметрии (скошенности) распределения, $E[(X - E[X])^3]$.\n",
        "    * **Четвертый центральный момент:** Мера островершинности (эксцесса) распределения, $E[(X - E[X])^4]$.\n",
        "    * **k-й центральный момент:** $E[(X - E[X])^k]$.\n",
        "\n",
        "**Для выборок данных:**\n",
        "\n",
        "* **Выборочные начальные моменты:**\n",
        "    * **Первый выборочный начальный момент:** Выборочное среднее, обозначается как $\\bar{X}$ или $\\frac{1}{n} \\sum_{i=1}^{n} X_i$.\n",
        "    * **Второй выборочный начальный момент:** $\\frac{1}{n} \\sum_{i=1}^{n} X_i^2$.\n",
        "    * **k-й выборочный начальный момент:** $\\frac{1}{n} \\sum_{i=1}^{n} X_i^k$.\n",
        "\n",
        "* **Выборочные центральные моменты:**\n",
        "    * **Первый выборочный центральный момент:** Всегда равен нулю, $\\frac{1}{n} \\sum_{i=1}^{n} (X_i - \\bar{X}) = 0$.\n",
        "    * **Второй выборочный центральный момент:** Выборочная дисперсия (с делителем $n$), $\\frac{1}{n} \\sum_{i=1}^{n} (X_i - \\bar{X})^2$. Иногда используется несмещенная оценка с делителем $n-1$.\n",
        "    * **Третий выборочный центральный момент:** $\\frac{1}{n} \\sum_{i=1}^{n} (X_i - \\bar{X})^3$.\n",
        "    * **Четвертый выборочный центральный момент:** $\\frac{1}{n} \\sum_{i=1}^{n} (X_i - \\bar{X})^4$.\n",
        "    * **k-й выборочный центральный момент:** $\\frac{1}{n} \\sum_{i=1}^{n} (X_i - \\bar{X})^k$.\n",
        "\n",
        "* **Метод моментов:**  Основывается на идее, что выборочные моменты являются оценками соответствующих теоретических моментов генеральной совокупности."
      ],
      "metadata": {
        "id": "s45_e4sB9tTq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **Вопрос 36: Метод максимального правдоподобия.**\n",
        "\n",
        "## **Методы построения точных оценок**\n",
        "\n",
        "Для оценки параметров генеральной совокупности используются различные методы, среди которых наиболее распространены метод моментов и метод максимального правдоподобия. Эти методы позволяют получить точные оценки параметров на основе данных выборки.\n",
        "\n",
        "## Метод максимального правдоподобия:\n",
        "\n",
        "**Метод максимального правдоподобия (ММП)** — это мощный статистический метод оценки параметров распределения, который позволяет найти наиболее вероятные значения параметров, исходя из имеющихся данных.\n",
        "\n",
        "**Основная идея ММП:**\n",
        "\n",
        "Предположим, у нас есть набор наблюдений $X_1, X_2, \\dots, X_n$, которые мы считаем случайными величинами, имеющими некоторое распределение с параметром (или набором параметров) $\\theta$. Мы хотим найти такое значение $\\theta$, которое **максимизирует вероятность** наблюдения именно этих данных.\n",
        "\n",
        "### **Функция правдоподобия для дискретных случайных величин:**\n",
        "\n",
        "Если у нас есть набор независимых и одинаково распределенных наблюдений $X_1, X_2, \\dots, X_n$, которые являются дискретными случайными величинами с функцией вероятности $P(X=x | \\theta)$, то функция правдоподобия определяется как произведение вероятностей каждого конкретного наблюдения:\n",
        "\n",
        "$$ L(\\theta) = P(X_1=x_1, X_2=x_2, \\dots, X_n=x_n | \\theta) = \\prod_{i=1}^{n} P(X_i=x_i | \\theta) $$\n",
        "\n",
        "Здесь $P(X_i=x_i | \\theta)$ - это вероятность того, что случайная величина $X_i$ примет конкретное наблюдаемое значение $x_i$, при заданном значении параметра $\\theta$.\n",
        "\n",
        "Параметр $\\theta$ в контексте метода максимального правдоподобия (ММП) — это **неизвестный параметр, например: математическое ожидание или дисперсия (или вектор параметров, например:  математическое ожидание и дисперсия одновременно) распределения**, из которого, как мы предполагаем, были получены наши наблюдаемые данные.\n",
        "\n",
        "**Пример:**\n",
        "\n",
        "Предположим, мы подбрасываем монету $n$ раз и наблюдаем $k$ выпадений \"орла\". Мы хотим оценить параметр $p$ - вероятность выпадения \"орла\". В этом случае, каждое подбрасывание является испытанием Бернулли, и функция вероятности для одного испытания выглядит так:\n",
        "\n",
        "$$ P(X=x | p) = \\begin{cases} p, & \\text{если } x = 1 \\text{ (\"орел\")} \\\\ 1-p, & \\text{если } x = 0 \\text{ (\"решка\")} \\end{cases} $$\n",
        "\n",
        "Пусть у нас есть результаты $n$ подбрасываний, где $k$ раз выпал \"орел\" (значение 1) и $n-k$ раз выпала \"решка\" (значение 0). Тогда функция правдоподобия будет выглядеть так:\n",
        "\n",
        "$$ L(p) = \\prod_{i=1}^{n} P(X_i=x_i | p) = p^k (1-p)^{n-k} $$\n",
        "\n",
        "Задача метода максимального правдоподобия в данном случае - найти такое значение $p$, которое максимизирует эту функцию $L(p)$. Для этого обычно берут логарифм функции правдоподобия (что упрощает вычисления) и находят его максимум, взяв производную по $p$ и приравняв ее к нулю.\n",
        "\n",
        "### **Пример: Простые случаи**\n",
        "\n",
        "Предположим, мы подбросили монету $n=10$ раз и наблюдали $k=7$ выпадений \"орла\".\n",
        "\n",
        "1. **Функция правдоподобия:**\n",
        "   $$ L(p) = p^7 (1-p)^{10-7} = p^7 (1-p)^3 $$\n",
        "\n",
        "2. **Логарифм функции правдоподобия:**\n",
        "   $$ \\ln L(p) = \\ln(p^7 (1-p)^3) = 7 \\ln(p) + 3 \\ln(1-p) $$\n",
        "\n",
        "3. **Находим производную логарифма функции правдоподобия по $p$:**\n",
        "   $$ \\frac{d}{dp} \\ln L(p) = \\frac{7}{p} + \\frac{3}{1-p} \\cdot (-1) = \\frac{7}{p} - \\frac{3}{1-p} $$\n",
        "\n",
        "4. **Приравниваем производную к нулю и решаем относительно $p$:**\n",
        "   $$ \\frac{7}{p} - \\frac{3}{1-p} = 0 $$\n",
        "   $$ \\frac{7(1-p) - 3p}{p(1-p)} = 0 $$\n",
        "   $$ 7 - 7p - 3p = 0 $$\n",
        "   $$ 7 - 10p = 0 $$\n",
        "   $$ 10p = 7 $$\n",
        "   $$ p = \\frac{7}{10} = 0.7 $$\n",
        "\n",
        "   Таким образом, оценка максимального правдоподобия для вероятности выпадения \"орла\" составляет 0.7. Это интуитивно понятно, так как в 10 подбрасываниях \"орел\" выпал 7 раз, что составляет 70%.\n",
        "\n",
        "\n",
        "### **Пример для нормального распределения:**\n",
        "\n",
        "Представьте, что у вас есть набор чисел (выборка), и вы предполагаете, что эти числа взяты из нормального распределения. Нормальное распределение описывается двумя параметрами: **средним значением (математическим ожиданием, $\\mu$)** и **разбросом (дисперсией, $\\sigma^2$)**. Однако истинные значения этих параметров вам неизвестны.\n",
        "\n",
        "Метод максимального правдоподобия помогает оценить эти параметры на основе вашей выборки. Основная идея заключается в том, чтобы найти такие значения $\\mu$ и $\\sigma^2$, при которых **вероятность** получить именно вашу выборку будет **максимальной**.\n",
        "\n",
        "#### Шаги метода максимального правдоподобия\n",
        "\n",
        "1. **Записываем функцию правдоподобия:**\n",
        "  Функция правдоподобия показывает, насколько вероятно получить вашу выборку при заданных значениях $\\mu$ и $\\sigma^2$. Для нормального распределения функция правдоподобия выглядит так:\n",
        "\n",
        "  $$\n",
        "  L(\\mu, \\sigma^2) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(X_i-\\mu)^2}{2\\sigma^2}\\right),\n",
        "  $$\n",
        "\n",
        "  где $X_1, X_2, \\dots, X_n$ — ваши выборки.\n",
        "\n",
        "2. **Упрощаем с помощью логарифма:**\n",
        "  Работать с логарифмом функции правдоподобия проще, так как произведение превращается в сумму, и это не меняет положение максимума. Логарифмическая функция правдоподобия:\n",
        "\n",
        "  $$\n",
        "  \\ln L(\\mu, \\sigma^2) = -\\frac{n}{2} \\ln(2\\pi) - \\frac{n}{2} \\ln(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(X_i-\\mu)^2.\n",
        "  $$\n",
        "\n",
        "3. **Ищем максимум:**\n",
        "  Чтобы найти значения $\\mu$ и $\\sigma^2$, которые максимизируют логарифм правдоподобия, мы берем **частные производные** по каждому из параметров и приравниваем их к нулю. Это дает систему уравнений:\n",
        "\n",
        "  $$\n",
        "  \\frac{\\partial \\ln L}{\\partial \\mu} = 0, \\quad \\frac{\\partial \\ln L}{\\partial \\sigma^2} = 0.\n",
        "  $$\n",
        "\n",
        "  ### Шаг 1: Частная производная по $\\mu$\n",
        "\n",
        "  Найдем частную производную логарифма правдоподобия по $\\mu$:\n",
        "\n",
        "  $$\n",
        "  \\frac{\\partial \\ln L}{\\partial \\mu} = \\frac{\\partial}{\\partial \\mu} \\left( -\\frac{n}{2} \\ln(2\\pi) - \\frac{n}{2} \\ln(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(X_i-\\mu)^2 \\right).\n",
        "  $$\n",
        "\n",
        "  Первые два слагаемых не зависят от $\\mu$, поэтому их производные равны нулю. Остается только третье слагаемое:\n",
        "\n",
        "  $$\n",
        "  \\frac{\\partial}{\\partial \\mu} \\left( -\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(X_i-\\mu)^2 \\right).\n",
        "  $$\n",
        "\n",
        "  Раскроем сумму и возьмем производную:\n",
        "\n",
        "  $$\n",
        "  \\frac{\\partial}{\\partial \\mu} \\left( -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (X_i^2 - 2X_i\\mu + \\mu^2) \\right).\n",
        "  $$\n",
        "\n",
        "  Производная суммы равна сумме производных:\n",
        "\n",
        "  $$\n",
        "  -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\mu} (X_i^2 - 2X_i\\mu + \\mu^2).\n",
        "  $$\n",
        "\n",
        "  Теперь вычислим производную каждого слагаемого:\n",
        "\n",
        "  - Производная $X_i^2$ по $\\mu$ равна 0 (так как $X_i$ не зависит от $\\mu$).\n",
        "  - Производная $-2X_i\\mu$ по $\\mu$ равна $-2X_i$.\n",
        "  - Производная $\\mu^2$ по $\\mu$ равна $2\\mu$.\n",
        "\n",
        "  Таким образом:\n",
        "\n",
        "  $$\n",
        "  -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (-2X_i + 2\\mu).\n",
        "  $$\n",
        "\n",
        "  Упростим выражение:\n",
        "\n",
        "  $$\n",
        "  -\\frac{1}{2\\sigma^2} \\cdot 2 \\sum_{i=1}^{n} (-X_i + \\mu) = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (X_i - \\mu).\n",
        "  $$\n",
        "\n",
        "  Теперь приравняем производную к нулю:\n",
        "\n",
        "  $$\n",
        "  \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (X_i - \\mu) = 0.\n",
        "  $$\n",
        "\n",
        "  Так как $\\sigma^2 \\neq 0$, можно умножить обе части на $\\sigma^2$:\n",
        "\n",
        "  $$\n",
        "  \\sum_{i=1}^{n} (X_i - \\mu) = 0.\n",
        "  $$\n",
        "\n",
        "  Раскроем сумму:\n",
        "\n",
        "  $$\n",
        "  \\sum_{i=1}^{n} X_i - n\\mu = 0.\n",
        "  $$\n",
        "\n",
        "  Отсюда получаем оценку для $\\mu$:\n",
        "\n",
        "  $$\n",
        "  \\hat{\\mu}_{ММП} = \\frac{1}{n} \\sum_{i=1}^{n} X_i = \\bar{X}.\n",
        "  $$\n",
        "\n",
        "  ### Шаг 2: Частная производная по $\\sigma^2$\n",
        "\n",
        "  Теперь найдем частную производную логарифма правдоподобия по $\\sigma^2$:\n",
        "\n",
        "  $$\n",
        "  \\frac{\\partial \\ln L}{\\partial \\sigma^2} = \\frac{\\partial}{\\partial \\sigma^2} \\left( -\\frac{n}{2} \\ln(2\\pi) - \\frac{n}{2} \\ln(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(X_i-\\mu)^2 \\right).\n",
        "  $$\n",
        "\n",
        "  Первое слагаемое не зависит от $\\sigma^2$, поэтому его производная равна нулю. Остаются второе и третье слагаемые:\n",
        "\n",
        "  $$\n",
        "  \\frac{\\partial}{\\partial \\sigma^2} \\left( -\\frac{n}{2} \\ln(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(X_i-\\mu)^2 \\right).\n",
        "  $$\n",
        "\n",
        "  Вычислим производную каждого слагаемого:\n",
        "\n",
        "  1. Производная $-\\frac{n}{2} \\ln(\\sigma^2)$ по $\\sigma^2$:\n",
        "\n",
        "  $$\n",
        "  \\frac{\\partial}{\\partial \\sigma^2} \\left( -\\frac{n}{2} \\ln(\\sigma^2) \\right) = -\\frac{n}{2} \\cdot \\frac{1}{\\sigma^2}.\n",
        "  $$\n",
        "\n",
        "  2. Производная $-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(X_i-\\mu)^2$ по $\\sigma^2$:\n",
        "\n",
        "  $$\n",
        "  \\frac{\\partial}{\\partial \\sigma^2} \\left( -\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(X_i-\\mu)^2 \\right) = \\frac{1}{2(\\sigma^2)^2} \\sum_{i=1}^{n}(X_i-\\mu)^2.\n",
        "  $$\n",
        "\n",
        "  Теперь объединим результаты:\n",
        "\n",
        "  $$\n",
        "  \\frac{\\partial \\ln L}{\\partial \\sigma^2} = -\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2} \\sum_{i=1}^{n}(X_i-\\mu)^2.\n",
        "  $$\n",
        "\n",
        "  Приравняем производную к нулю:\n",
        "\n",
        "  $$\n",
        "  -\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2} \\sum_{i=1}^{n}(X_i-\\mu)^2 = 0.\n",
        "  $$\n",
        "\n",
        "  Умножим обе части на $2(\\sigma^2)^2$, чтобы избавиться от знаменателя:\n",
        "\n",
        "  $$\n",
        "  -n\\sigma^2 + \\sum_{i=1}^{n}(X_i-\\mu)^2 = 0.\n",
        "  $$\n",
        "\n",
        "  Перенесем второе слагаемое вправо:\n",
        "\n",
        "  $$\n",
        "  \\sum_{i=1}^{n}(X_i-\\mu)^2 = n\\sigma^2.\n",
        "  $$\n",
        "\n",
        "  Теперь выразим $\\sigma^2$:\n",
        "\n",
        "  $$\n",
        "  \\hat{\\sigma}^2_{ММП} = \\frac{1}{n} \\sum_{i=1}^{n}(X_i-\\hat{\\mu}_{ММП})^2.\n",
        "  $$\n",
        "\n",
        "4. **Решаем систему уравнений:**\n",
        "- Для $\\mu$:\n",
        "  $$\n",
        "  \\frac{\\partial \\ln L}{\\partial \\mu} = \\frac{1}{\\sigma^2}\\sum_{i=1}^{n}(X_i-\\mu) = 0 \\implies \\sum_{i=1}^{n}X_i - n\\mu = 0 \\implies \\hat{\\mu}_{ММП} = \\frac{1}{n}\\sum_{i=1}^{n}X_i = \\bar{X}.\n",
        "  $$\n",
        "  Оценка максимального правдоподобия для $\\mu$ — это просто **среднее арифметическое** выборки.\n",
        "\n",
        "- Для $\\sigma^2$:\n",
        "  $$\n",
        "  \\frac{\\partial \\ln L}{\\partial \\sigma^2} = -\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2}\\sum_{i=1}^{n}(X_i-\\mu)^2 = 0 \\implies \\hat{\\sigma}^2_{ММП} = \\frac{1}{n}\\sum_{i=1}^{n}(X_i-\\hat{\\mu}_{ММП})^2.\n",
        "  $$\n",
        "  Оценка максимального правдоподобия для $\\sigma^2$ — это **выборочная дисперсия**, рассчитанная с делителем $n$.\n",
        "\n",
        "### Главный вывод\n",
        "\n",
        "Метод максимального правдоподобия позволяет найти такие значения параметров распределения ($\\mu$ и $\\sigma^2$), при которых наблюдаемая выборка имеет **максимальную вероятность**. В случае нормального распределения оценки ММП оказываются интуитивно понятными:\n",
        "\n",
        "- **Оценка среднего ($\\mu$)** — это среднее арифметическое выборки.\n",
        "- **Оценка дисперсии ($\\sigma^2$)** — это выборочная дисперсия (с делителем $n$).\n",
        "\n",
        "### Уточнения и нюансы\n",
        "\n",
        "1. **Система уравнений:**\n",
        "   Когда у нас несколько параметров, мы работаем не с одним уравнением, а с **системой уравнений**, полученной путем приравнивания частных производных логарифма правдоподобия к нулю.\n",
        "\n",
        "2. **\"Наилучшая аппроксимация\":**\n",
        "   Точнее говорить не о \"подгонке\" распределения под выборку, а о нахождении параметров, при которых вероятность наблюдения данной выборки **максимальна**.\n",
        "\n",
        "3. **Пример с нормальным распределением:**\n",
        "   В примере выше мы рассмотрели случай нормального распределения, где оценки ММП для $\\mu$ и $\\sigma^2$ оказались равными выборочному среднему и выборочной дисперсии (с делителем $n$).\n",
        "\n",
        "### Заключение\n",
        "\n",
        "Метод максимального правдоподобия — это мощный инструмент для оценки параметров распределения. Он основан на поиске значений параметров, которые делают наблюдаемую выборку наиболее вероятной. В случае нормального распределения оценки ММП совпадают с выборочным средним и выборочной дисперсией, что делает их интуитивно понятными и легко интерпретируемыми.\n",
        "\n",
        "\n",
        "**Ключевое отличие:**\n",
        "\n",
        "Основное отличие при переходе от непрерывных к дискретным случайным величинам в контексте ММП заключается в замене функции плотности вероятности на функцию вероятности. В остальном, принцип максимизации вероятности наблюдаемых данных остается неизменным.\n",
        "\n",
        "### **Функция правдоподобия для непрерывных случайных величин:**\n",
        "\n",
        "Вероятность наблюдения конкретного набора данных $X_1, X_2, \\dots, X_n$ при заданном значении параметра $\\theta$ называется **функцией правдоподобия** и обозначается $L(\\theta)$.\n",
        "\n",
        "Для независимых и одинаково распределенных наблюдений функция правдоподобия определяется как произведение плотностей вероятностей отдельных наблюдений, где под отдельным наблюдением подразумевается например стратифицированная выборка:\n",
        "\n",
        "$$ L(\\theta) = P(X_1, X_2, \\dots, X_n | \\theta) = \\prod_{i=1}^{n} P(X_i | \\theta) $$\n",
        "\n",
        "**Логарифмическая функция правдоподобия:**\n",
        "\n",
        "Вычисление произведения вероятностей может быть сложным, особенно для больших выборок. Поэтому часто используют **логарифмическую функцию правдоподобия**:\n",
        "\n",
        "$$ \\log L(\\theta) = \\sum_{i=1}^{n} \\log P(X_i | \\theta) $$\n",
        "\n",
        "Логарифм — монотонно возрастающая функция, поэтому максимизация логарифма функции правдоподобия эквивалентна максимизации самой функции правдоподобия.\n",
        "\n",
        "**Оценка максимального правдоподобия:**\n",
        "\n",
        "**Оценкой максимального правдоподобия** (ОМП) параметра $\\theta$ называется такое значение $\\hat{\\theta}$, которое максимизирует функцию правдоподобия (или её логарифм):\n",
        "\n",
        "$$ \\hat{\\theta} = \\arg \\max_{\\theta} L(\\theta) $$\n",
        "\n",
        "или\n",
        "\n",
        "$$ \\hat{\\theta} = \\arg \\max_{\\theta} \\log L(\\theta) $$\n",
        "\n",
        "**Алгоритм нахождения ОМП:**\n",
        "\n",
        "1. **Записать функцию правдоподобия** $L(\\theta)$ или логарифмическую функцию правдоподобия $\\log L(\\theta)$.\n",
        "2. **Найти первую производную** $\\frac{\\partial \\log L(\\theta)}{\\partial \\theta}$ и приравнять её к нулю:\n",
        "$$ \\frac{\\partial \\log L(\\theta)}{\\partial \\theta} = 0 $$\n",
        "3. **Решить полученное уравнение** относительно $\\theta$. Полученное значение $\\hat{\\theta}$ и будет оценкой максимального правдоподобия.\n",
        "4. **Проверить достаточное условие максимума:** вторая производная $\\frac{\\partial^2 \\log L(\\theta)}{\\partial \\theta^2}$ должна быть отрицательной в точке $\\hat{\\theta}$.\n",
        "\n",
        "**Пример:**\n",
        "\n",
        "Предположим, у нас есть выборка $X_1, X_2, \\dots, X_n$ из нормального распределения $N(\\mu, \\sigma^2)$. Мы хотим оценить параметры $\\mu$ и $\\sigma^2$.\n",
        "\n",
        "**Функция правдоподобия:**\n",
        "\n",
        "$$ L(\\mu, \\sigma^2) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(X_i - \\mu)^2}{2\\sigma^2}\\right) $$\n",
        "\n",
        "**Логарифмическая функция правдоподобия:**\n",
        "\n",
        "$$ \\log L(\\mu, \\sigma^2) = -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (X_i - \\mu)^2 $$\n",
        "\n",
        "**Нахождение ОМП для $\\mu$:**\n",
        "\n",
        "1. Взять производную по $\\mu$:\n",
        "$$ \\frac{\\partial \\log L(\\mu, \\sigma^2)}{\\partial \\mu} = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (X_i - \\mu) $$\n",
        "2. Приравнять к нулю и решить относительно $\\mu$:\n",
        "$$ \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (X_i - \\mu) = 0 $$\n",
        "$$ \\sum_{i=1}^{n} X_i - n\\mu = 0 $$\n",
        "$$ \\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^{n} X_i $$\n",
        "\n",
        "**Нахождение ОМП для $\\sigma^2$:**\n",
        "\n",
        "1. Взять производную по $\\sigma^2$:\n",
        "$$ \\frac{\\partial \\log L(\\mu, \\sigma^2)}{\\partial \\sigma^2} = -\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2} \\sum_{i=1}^{n} (X_i - \\mu)^2 $$\n",
        "2. Приравнять к нулю и решить относительно $\\sigma^2$:\n",
        "$$ -\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2} \\sum_{i=1}^{n} (X_i - \\mu)^2 = 0 $$\n",
        "$$ \\hat{\\sigma^2} = \\frac{1}{n} \\sum_{i=1}^{n} (X_i - \\hat{\\mu})^2 $$\n",
        "\n",
        "**Доказательство:**\n",
        "\n",
        "Доказательство того, что найденные оценки $\\hat{\\mu}$ и $\\hat{\\sigma^2}$ действительно являются оценками максимального правдоподобия, заключается в проверке достаточного условия максимума: вторая производная логарифмической функции правдоподобия должна быть отрицательной в точке $(\\hat{\\mu}, \\hat{\\sigma^2})$.\n",
        "\n",
        "**Преимущества ММП:**\n",
        "\n",
        "* **Состоятельность:** ОМП сходится к истинному значению параметра при увеличении объема выборки.\n",
        "* **Асимптотическая нормальность:** ОМП имеет асимптотически нормальное распределение.\n",
        "* **Асимптотическая эффективность:** ОМП является асимптотически наиболее эффективной оценкой среди всех состоятельных оценок.\n",
        "\n",
        "**Недостатки ММП:**\n",
        "\n",
        "* **Сложность вычислений:** Нахождение ОМП может быть сложным, особенно для многомерных параметров.\n",
        "* **Чувствительность к априорным предположениям:** ММП требует знания формы распределения данных.\n",
        "\n",
        "**Заключение:**\n",
        "\n",
        "Метод максимального правдоподобия — это мощный и универсальный инструмент статистического анализа, который широко используется в различных областях науки и техники.\n",
        "\n",
        "### 1. Функция правдоподобия для нормального распределения\n",
        "\n",
        "Предположим, у нас есть выборка $X_1, X_2, \\dots, X_n$ из нормального распределения $N(\\mu, \\sigma^2)$.\n",
        "\n",
        "**Плотность вероятности** для одного наблюдения $X_i$ из нормального распределения с параметрами $\\mu$ и $\\sigma^2$ задается формулой:\n",
        "\n",
        "$$ P(X_i | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(X_i - \\mu)^2}{2\\sigma^2}\\right) $$\n",
        "\n",
        "**Функция правдоподобия** для выборки $X_1, X_2, \\dots, X_n$ — это произведение плотностей вероятности для каждого наблюдения:\n",
        "\n",
        "$$ L(\\mu, \\sigma^2) = \\prod_{i=1}^{n} P(X_i | \\mu, \\sigma^2) $$\n",
        "\n",
        "Подставляя выражение для плотности вероятности, получаем:\n",
        "\n",
        "$$ L(\\mu, \\sigma^2) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(X_i - \\mu)^2}{2\\sigma^2}\\right) $$\n",
        "\n",
        "### 2. Логарифмическая функция правдоподобия\n",
        "\n",
        "Для упрощения вычислений часто используют **логарифм функции правдоподобия**. Логарифм произведения равен сумме логарифмов:\n",
        "\n",
        "$$ \\log L(\\mu, \\sigma^2) = \\log \\left( \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(X_i - \\mu)^2}{2\\sigma^2}\\right) \\right) $$\n",
        "\n",
        "$$ \\log L(\\mu, \\sigma^2) = \\sum_{i=1}^{n} \\log \\left( \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(X_i - \\mu)^2}{2\\sigma^2}\\right) \\right) $$\n",
        "\n",
        "Теперь раскроем логарифм произведения:\n",
        "\n",
        "$$ \\log L(\\mu, \\sigma^2) = \\sum_{i=1}^{n} \\left( \\log \\left( \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\right) + \\log \\left( \\exp\\left(-\\frac{(X_i - \\mu)^2}{2\\sigma^2}\\right) \\right) \\right) $$\n",
        "\n",
        "Используя свойства логарифма, получаем:\n",
        "\n",
        "$$ \\log L(\\mu, \\sigma^2) = -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (X_i - \\mu)^2 $$\n",
        "\n",
        "\n",
        "### Итог\n",
        "\n",
        "1. **Функция правдоподобия** для нормального распределения получается путем произведения плотностей вероятности для каждого наблюдения.\n",
        "2. **Логарифмическая функция правдоподобия** получается путем логарифмирования функции правдоподобия и использования свойств логарифма.\n"
      ],
      "metadata": {
        "id": "ZsI6EathGoBe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **Вопрос 37: Интервальные оценки параметров. Доверительные интервалы для среднего.**\n",
        "\n",
        "### **Ключевые компоненты интервальной оценки:**\n",
        "\n",
        "*   **Доверительный интервал:**  Это диапазон значений, рассчитанный на основе выборочных данных. Он имеет нижнюю и верхнюю границы.\n",
        "*   **Уровень доверия:**  Это вероятность того, что построенный доверительный интервал будет содержать истинное значение параметра генеральной совокупности. Например, уровень доверия 95% означает, что если мы будем многократно брать выборки и строить для них 95%-ные доверительные интервалы, то примерно в 95% случаев эти интервалы будут содержать истинное значение параметра.\n",
        "\n",
        "### **Чем интервальная оценка отличается от точечной оценки?**\n",
        "\n",
        "*   **Точечная оценка** дает одно конкретное значение в качестве оценки параметра (например, выборочное среднее как оценка среднего генеральной совокупности).\n",
        "*   **Интервальная оценка** дает диапазон значений, в пределах которого, как предполагается, находится истинное значение параметра.\n",
        "\n",
        "Интервальные оценки более информативны, чем точечные, поскольку они предоставляют не только оценку параметра, но и меру неопределенности этой оценки.\n",
        "\n",
        "**Пример:**\n",
        "\n",
        "Предположим, мы хотим оценить средний возраст покупателей нового продукта. Мы берем случайную выборку из 100 покупателей и находим, что их средний возраст составляет 35 лет. Это точечная оценка.\n",
        "\n",
        "Чтобы получить интервальную оценку, мы можем построить, например, 95%-ный доверительный интервал. Допустим, расчеты показывают, что этот интервал составляет от 32 до 38 лет. Это означает, что мы можем с 95% уверенностью утверждать, что средний возраст всех покупателей нового продукта находится в диапазоне от 32 до 38 лет.\n",
        "\n",
        "**Факторы, влияющие на ширину доверительного интервала:**\n",
        "\n",
        "*   **Уровень доверия:** Чем выше уровень доверия, тем шире будет интервал. Чтобы быть более уверенными в том, что интервал содержит истинное значение, нам нужен более широкий диапазон.\n",
        "*   **Размер выборки:** Чем больше размер выборки, тем уже будет интервал. Большая выборка предоставляет больше информации о генеральной совокупности, что позволяет сделать более точную оценку.\n",
        "*   **Дисперсия генеральной совокупности (или ее оценка):** Чем больше дисперсия, тем шире будет интервал. Большая изменчивость в данных затрудняет точную оценку параметра.\n",
        "\n",
        "**Важно понимать интерпретацию доверительного интервала:**\n",
        "\n",
        "Доверительный интервал **не** означает, что с определенной вероятностью истинное значение параметра находится в *конкретном* построенном интервале. Истинное значение параметра является фиксированным, хотя и неизвестным. Интервал либо содержит это значение, либо нет.\n",
        "\n",
        "Правильная интерпретация заключается в том, что если бы мы многократно брали выборки и строили доверительные интервалы с заданным уровнем доверия, то заданный процент этих интервалов содержал бы истинное значение параметра.\n",
        "\n",
        "Интервальные оценки являются важным инструментом в статистическом анализе, позволяющим делать обоснованные выводы о параметрах генеральной совокупности на основе выборочных данных, учитывая при этом присущую неопределенность.\n",
        "\n",
        "## **Доверительные интервалы и их значение**\n",
        "\n",
        "Доверительный интервал — это диапазон значений, в котором с заданной вероятностью (доверительной вероятностью) находится истинное значение параметра генеральной совокупности. Этот интервал позволяет оценить, насколько точно мы можем утверждать о значении параметра на основе данных выборки.\n",
        "\n",
        "### Определение доверительного интервала\n",
        "\n",
        "Доверительный интервал строится на основе выборочной статистики, такой как выборочная средняя, и учитывает дисперсию выборки. Если мы хотим построить доверительный интервал для математического ожидания $\\mu$ с заданной вероятностью (например, 95% или 99%), мы используем следующую формулу:\n",
        "\n",
        "$$\n",
        "\\bar{x} \\pm z_{\\alpha/2} \\cdot \\frac{\\sigma}{\\sqrt{n}}\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $\\bar{x}$ — выборочная средняя;\n",
        "- $z_{\\alpha/2}$ — квантиль стандартного нормального распределения, соответствующий уровню доверия;\n",
        "- $\\sigma$ — стандартное отклонение генеральной совокупности (если известно);\n",
        "- $n$ — объем выборки.\n",
        "\n",
        "**Объяснение $z_{\\alpha/2}$:**\n",
        "\n",
        "1. **Уровень доверия (Confidence Level):**  Начнем с того, что доверительный интервал строится с определенным уровнем доверия. Этот уровень доверия (обычно обозначается как $1 - \\alpha$) показывает, насколько мы уверены, что истинное значение параметра генеральной совокупности попадет в построенный интервал. Часто используемые уровни доверия: 90%, 95%, 99%.\n",
        "\n",
        "2. **Уровень значимости ($\\alpha$):**  Величина $\\alpha$ (альфа) представляет собой уровень значимости. Это вероятность того, что истинное значение параметра *не* попадет в построенный доверительный интервал. Другими словами, это вероятность совершить ошибку. Математически, $\\alpha = 1 - \\text{уровень доверия}$.\n",
        "\n",
        "   * Например, если уровень доверия составляет 95%, то $\\alpha = 1 - 0.95 = 0.05$.\n",
        "   * Если уровень доверия составляет 99%, то $\\alpha = 1 - 0.99 = 0.01$.\n",
        "\n",
        "3. **Двусторонний интервал и $\\alpha/2$:**  Когда мы строим двусторонний доверительный интервал (как в приведенной вами формуле), мы допускаем ошибку как в большую, так и в меньшую сторону от истинного значения. Поэтому уровень значимости $\\alpha$ делится пополам, и $\\alpha/2$ приходится на каждую \"хвостовую\" область стандартного нормального распределения.\n",
        "\n",
        "4. **Стандартное нормальное распределение (Z-распределение):**  Переменная $z_{\\alpha/2}$ является квантилем стандартного нормального распределения. Стандартное нормальное распределение – это симметричное распределение с математическим ожиданием 0 и стандартным отклонением 1. Его часто называют Z-распределением.\n",
        "\n",
        "5. **Квантиль:** Квантиль – это значение, ниже которого лежит определенная доля наблюдений в распределении. В нашем случае, $z_{\\alpha/2}$ – это значение на оси Z-распределения, такое, что площадь под кривой справа от этого значения равна $\\alpha/2$. Из-за симметрии распределения, площадь под кривой слева от $-z_{\\alpha/2}$ также равна $\\alpha/2$.\n",
        "\n",
        "### Применение центральной предельной теоремы\n",
        "\n",
        "При условии, что объем выборки велик (обычно $n > 30$), согласно центральной предельной теореме, распределение выборочной средней приближается к нормальному распределению, даже если исходное распределение не является нормальным. Это позволяет использовать нормальное распределение для построения доверительных интервалов.\n",
        "\n",
        "Согласно ЦПТ, стандартизованная выборочная средняя:\n",
        "\n",
        "$$\n",
        "Z = \\frac{\\bar{x} - \\mu}{\\sigma/\\sqrt{n}}\n",
        "$$\n",
        "\n",
        "имеет стандартное нормальное распределение $N(0, 1)$.\n",
        "\n",
        "Для заданного уровня доверия (например, 95%), мы знаем, что:\n",
        "\n",
        "$$\n",
        "P(-z_{\\alpha/2} \\leq Z \\leq z_{\\alpha/2}) = 1 - \\alpha\n",
        "$$\n",
        "\n",
        "где $\\alpha$ — уровень значимости (например, 0.05 для 95% доверительного интервала).\n",
        "\n",
        "Подставляя выражение для $Z$, получаем:\n",
        "\n",
        "$$\n",
        "P\\left(-z_{\\alpha/2} \\leq \\frac{\\bar{x} - \\mu}{\\sigma/\\sqrt{n}} \\leq z_{\\alpha/2}\\right) = 1 - \\alpha\n",
        "$$\n",
        "\n",
        "Решая это неравенство относительно $\\mu$, получаем:\n",
        "\n",
        "$$\n",
        "P\\left(\\bar{x} - z_{\\alpha/2} \\cdot \\frac{\\sigma}{\\sqrt{n}} \\leq \\mu \\leq \\bar{x} + z_{\\alpha/2} \\cdot \\frac{\\sigma}{\\sqrt{n}}\\right) = 1 - \\alpha\n",
        "$$\n",
        "\n",
        "Таким образом, доверительный интервал для $\\mu$ имеет вид:\n",
        "\n",
        "$$\n",
        "\\bar{x} \\pm z_{\\alpha/2} \\cdot \\frac{\\sigma}{\\sqrt{n}}\n",
        "$$\n",
        "\n",
        "### **Пример использования**\n",
        "\n",
        "**Задача:** Предположим, мы хотим оценить средний вес (в кг) учащихся в школе. Мы взяли случайную выборку из 100 учащихся и получили следующие данные:\n",
        "\n",
        "- Выборочная средняя $\\bar{x} = 55$ кг\n",
        "- Выборочное стандартное отклонение $s = 10$ кг\n",
        "\n",
        "Мы хотим построить 95% доверительный интервал для среднего веса учащихся в школе.\n",
        "\n",
        "**Решение:**\n",
        "\n",
        "1. **Определяем квантиль:** Для 95% доверительного интервала $z_{\\alpha/2} \\approx 1.96$.\n",
        "\n",
        "2. **Подставляем значения в формулу:**\n",
        "\n",
        "$$\n",
        "\\bar{x} \\pm z_{\\alpha/2} \\cdot \\frac{s}{\\sqrt{n}} = 55 \\pm 1.96 \\cdot \\frac{10}{\\sqrt{100}} = 55 \\pm 1.96 \\cdot 1 = 55 \\pm 1.96\n",
        "$$\n",
        "\n",
        "3. **Вычисляем границы интервала:**\n",
        "\n",
        "$$\n",
        "55 - 1.96 = 53.04 \\quad \\text{и} \\quad 55 + 1.96 = 56.96\n",
        "$$\n",
        "\n",
        "Таким образом, 95% доверительный интервал для среднего веса учащихся в школе составляет от 53.04 кг до 56.96 кг.\n",
        "\n",
        "**Вывод:** С 95% уверенностью можно утверждать, что средний вес учащихся в школе находится в интервале от 53.04 кг до 56.96 кг."
      ],
      "metadata": {
        "id": "IQSDzI-7XB7i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **Вопрос 38: Параметрические и непараметрические гипотезы.**\n",
        "\n",
        "### **Вопрос 39: Проверка параметрических гипотез.**\n",
        "\n",
        "## **Статистические гипотезы и их классификация**\n",
        "\n",
        "Статистическая гипотеза — это утверждение о параметрах генеральной совокупности, которое подлежит проверке на основе выборочных данных. Гипотезы могут быть параметрическими или непараметрическими, в зависимости от того, содержат ли они информацию о значениях параметров.\n",
        "\n",
        "### Параметрические и непараметрические гипотезы\n",
        "\n",
        "1. **Параметрические гипотезы**: Эти гипотезы содержат утверждения о значении какого-либо параметра генеральной совокупности. Например, гипотеза о том, что среднее значение равно 5, является параметрической. Параметрические гипотезы могут быть:\n",
        "   - **Простыми**: Утверждают, что параметр равен конкретному числу (например, $H_0: \\mu = 5$).\n",
        "   - **Сложными**: Утверждают, что параметр находится в определенном интервале (например, $H_0: \\mu > 5$).\n",
        "\n",
        "2. **Непараметрические гипотезы**: Эти гипотезы не содержат утверждений о значениях параметров. Например, гипотеза о том, что распределение объема торговых акций имеет нормальный закон распределения, является непараметрической.\n",
        "\n",
        "### Процесс проверки гипотез\n",
        "\n",
        "Процесс проверки гипотез включает несколько шагов:\n",
        "\n",
        "1. **Формулировка гипотез**: Сначала необходимо сформулировать нулевую гипотезу ($H_0$) и альтернативную гипотезу ($H_1$). Нулевая гипотеза обычно представляет собой утверждение о том, что нет эффекта или различия.\n",
        "\n",
        "2. **Выбор статистики критерия**: Необходимо выбрать подходящий статистический критерий для проверки гипотезы. Это может быть t-критерий, z-критерий и другие.\n",
        "\n",
        "3. **Определение уровня значимости**: Уровень значимости ($\\alpha$) — это вероятность отклонения нулевой гипотезы, когда она на самом деле верна. Обычно выбирается значение 0.05 или 0.01.\n",
        "\n",
        "4. **Расчет границ критической области**: На основе выбранного уровня значимости рассчитываются границы критической области, в которую может попадать статистика с малой вероятностью $\\alpha$.\n",
        "\n",
        "5. **Вычисление фактического значения статистики**: На основе выборочных данных вычисляется фактическое значение статистики, которое затем сравнивается с критическими значениями.\n",
        "\n",
        "### Пример t-критерия\n",
        "\n",
        "Используется для сравнения средних значений двух выборок.\n",
        "\n",
        "#### Гипотезы:\n",
        "- $H_0$: Средние двух выборок равны;\n",
        "- $H_1$: Средние двух выборок различны.\n",
        "\n",
        "#### Формула:\n",
        "$$t = \\frac{\\bar{X}_1 - \\bar{X}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}$$\n",
        "\n",
        "где $\\bar{X}_1$ и $\\bar{X}_2$ — средние значения выборок, $s_1^2$ и $s_2^2$ — дисперсии выборок, $n_1$ и $n_2$ — размеры выборок.\n",
        "\n",
        "#### Интерпретация значений теста $t$ и $p$:\n",
        "\n",
        "- **Значение $(t)$**: Показывает, насколько далеко средние значения двух групп находятся друг от друга, выраженное в единицах стандартной ошибки. Большая абсолютная величина $(t)$ указывает на большее различие между средними. Отрицательное значение $(t)$ говорит о том, что среднее первой группы меньше среднего второй группы.\n",
        "\n",
        "- **Значение $(p)$**: Предоставляет вероятность получить наблюдаемые (или более экстремальные) результаты при условии, что нулевая гипотеза верна. Маленькое  $(p)$-значение (обычно меньше 0.05) указывает на то, что различия между группами статистически значимы, и мы можем отвергнуть нулевую гипотезу в пользу альтернативной.\n",
        "\n",
        "#### Применение:\n",
        "t-тест широко используется в научных исследованиях для сравнения средних значений двух групп, например, в клинических испытаниях для сравнения эффективности лечения или в социальных науках для сравнения групп по различным психологическим параметрам.\n",
        "\n",
        "#### Условия:\n",
        "\n",
        "- T-тест предполагает, что данные распределены нормально, и что дисперсии в сравниваемых группах примерно одинаковы. Эти предположения называются предположениями о гомоскедастичности и нормальности, и они являются ключевыми для правильного применения t-теста;\n",
        "\n",
        "- Также, если размеры выборок или дисперсии значительно различаются между группами, это может исказить результаты t-теста, делая его менее надежным.\n",
        "\n",
        "Тест Уилкоксона-Манна-Уитни является непараметрическим альтернативным методом, который не требует этих предположений, и поэтому он часто используется, когда данные не соответствуют предположениям t-теста.\n",
        "\n",
        "Результаты теста Уилкоксона-Манна-Уитни легко интерпретировать. Они включают U-статистику и P-значение, которые помогают понять различия между группами и их статистическую значимость.\n",
        "\n",
        "#### Пример:\n",
        "Предположим, мы проводим t-тест для сравнения среднего роста мужчин и женщин в выборке. Полученное значение $(t)$ равно -2.5, а $(p)$-значение равно 0.013. Это означает, что средний рост мужчин статистически значимо отличается от среднего роста женщин (при условии, что мужчины были первой группой), и вероятность случайно получить такое или более значимое различие при верной нулевой гипотезе составляет 1.3%.\n",
        "\n",
        "### Пример кода\n",
        "\n",
        "Ниже приведен пример кода на Python, который демонстрирует, как можно проверить гипотезу с использованием t-критерия:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "def t_test(data, hypothesized_mean, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Description:\n",
        "        Функция для проверки гипотезы о среднем значении с использованием t-критерия.\n",
        "\n",
        "    Args:\n",
        "        data: Список чисел, представляющий выборку.\n",
        "        hypothesized_mean: Предполагаемое среднее значение для проверки.\n",
        "        alpha: Уровень значимости (по умолчанию 0.05).\n",
        "\n",
        "    Retuens:\n",
        "        Результат проверки гипотезы.\n",
        "\n",
        "    Examples:\n",
        "        >>> t_test([10, 20, 30, 40, 50], 30)\n",
        "        'Не отклоняем нулевую гипотезу.'\n",
        "    \"\"\"\n",
        "    if not data:              # Проверка на пустую выборку\n",
        "        raise ValueError(\"Выборка не должна быть пустой.\")\n",
        "    \n",
        "    n = len(data)                                                                # Размер выборки\n",
        "    sample_mean = np.mean(data)                                                  # Выборочная средняя\n",
        "    sample_std = np.std(data, ddof=1)                                            # Выборочное стандартное отклонение\n",
        "    t_statistic = (sample_mean - hypothesized_mean) / (sample_std / np.sqrt(n))  # t-статистика\n",
        "    critical_value = stats.t.ppf(1 - alpha/2, df=n-1)                            # Критическое значение t\n",
        "\n",
        "    # Проверка условия для отклонения нулевой гипотезы\n",
        "    if abs(t_statistic) > critical_value:\n",
        "        return 'Отклоняем нулевую гипотезу.'\n",
        "    else:\n",
        "        return 'Не отклоняем нулевую гипотезу.'\n",
        "\n",
        "# Пример использования функции\n",
        "sample_data = [10, 20, 30, 40, 50]\n",
        "result = t_test(sample_data, 30)\n",
        "print(result)\n",
        "```\n",
        "\n",
        "В этом коде:\n",
        "- Функция `t_test` принимает список чисел и предполагаемое среднее значение для проверки.\n",
        "- Мы используем библиотеку NumPy для вычисления выборочной средней и стандартного отклонения, а также библиотеку SciPy для нахождения критического значения t.\n",
        "- Проверяем, что выборка не пустая, чтобы избежать деления на ноль.\n",
        "\n",
        "## **Ошибки первого и второго рода**\n",
        "\n",
        "При проверке статистических гипотез важно понимать, что процесс принятия решений может привести к ошибкам. Существует два основных типа ошибок: ошибка первого рода и ошибка второго рода.\n",
        "\n",
        "### Ошибка первого рода\n",
        "\n",
        "Ошибка первого рода происходит, когда нулевая гипотеза ($H_0$) отвергается, хотя на самом деле она верна. Это означает, что мы сделали вывод о наличии эффекта или различия, когда его на самом деле нет. Вероятность ошибки первого рода обозначается как уровень значимости ($\\alpha$). Например, если мы устанавливаем уровень значимости на уровне 0.05, это означает, что в 5% случаев мы можем ошибочно отвергнуть истинную нулевую гипотезу.\n",
        "\n",
        "### Ошибка второго рода\n",
        "\n",
        "Ошибка второго рода возникает, когда нулевая гипотеза не отвергается, хотя на самом деле она ложна. Это означает, что мы не смогли обнаружить эффект или различие, когда оно действительно существует. Вероятность ошибки второго рода обозначается как $\\beta$. Например, если $\\beta = 0.20$, это означает, что в 20% случаев мы не отвергнем ложную нулевую гипотезу.\n",
        "\n",
        "### Взаимосвязь между ошибками\n",
        "\n",
        "Существует обратная зависимость между вероятностями ошибок первого и второго рода. Увеличение уровня значимости ($\\alpha$) приводит к уменьшению вероятности ошибки второго рода ($\\beta$) и наоборот. Это связано с тем, что более строгие критерии для отклонения нулевой гипотезы могут привести к пропуску истинных эффектов.\n",
        "\n",
        "## **Оценка надежности и риски в статистических исследованиях**\n",
        "\n",
        "При проведении статистических исследований важно учитывать риски, связанные с оценкой параметров генеральной совокупности. Надежность оценок и доверительных интервалов может варьироваться в зависимости от контекста исследования и последствий ошибок.\n",
        "\n",
        "### Оценка надежности\n",
        "\n",
        "Надежность оценки — это вероятность того, что истинное значение параметра находится в заданном доверительном интервале. Например, если мы устанавливаем уровень доверия на 95%, это означает, что в 95% случаев истинное значение будет находиться в пределах этого интервала.\n",
        "\n",
        "При оценке параметров, таких как средний балл студентов или уровень образования, важно понимать, какие последствия могут возникнуть в случае ошибок. Если речь идет о критически важных решениях, таких как отчисление студентов или принятие решений в ГИБДД, необходимо устанавливать более высокий уровень надежности. В таких случаях доверительный интервал должен быть шире, чтобы минимизировать риски ошибок.\n",
        "\n",
        "### Риски ошибок\n",
        "\n",
        "1. **Ошибка первого рода**: Это риск отвергнуть нулевую гипотезу, когда она на самом деле верна. Например, если мы утверждаем, что средний балл студентов ниже определенного порога, когда на самом деле он выше, это может привести к неправильным решениям.\n",
        "\n",
        "2. **Ошибка второго рода**: Это риск не отвергнуть нулевую гипотезу, когда она ложна. Например, если мы не обнаруживаем, что средний балл студентов действительно ниже порога, это может привести к тому, что студенты не получат необходимую помощь.\n",
        "\n",
        "### Физический и геометрический смысл\n",
        "\n",
        "Представьте, что вы проводите исследование, чтобы оценить средний балл студентов на экзамене. Вы выдвигаете нулевую гипотезу, что средний балл равен 75. Используя t-критерий, вы можете проверить, есть ли статистически значимые различия между вашим выборочным средним и предполагаемым значением. Если ваше фактическое значение статистики превышает критическое значение, вы отклоняете нулевую гипотезу, что может указывать на то, что средний балл студентов отличается от 75.\n",
        "\n",
        "Таким образом, понимание статистических гипотез и методов их проверки является важным аспектом статистического анализа, позволяя делать обоснованные выводы на основе выборочных данных."
      ],
      "metadata": {
        "id": "mTkdPbKNPn0F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **Вопрос 40: Модели регрессии (линейная, парная линейная).**\n",
        "\n",
        "Регрессия - это статистический метод анализа данных, используемый для определения связи между зависимой переменной и одним или несколькими независимыми переменными (также называемыми предикторами или регрессорами).\n",
        "\n",
        "Целью регрессионного анализа является построение математической модели, которая описывает эту связь и позволяет предсказывать значение зависимой переменной на основе значений независимых переменных.\n",
        "\n",
        "Существует несколько типов регрессионного анализа, включая линейную регрессию, логарифмическую регрессию, полиномиальную регрессию, регрессию на основе деревьев решений и другие.\n",
        "\n",
        "Линейная регрессия — это статистический метод, используемый для моделирования и анализа взаимосвязей между переменными, где одна или несколько независимых переменных используются для прогнозирования значения зависимой переменной.\n",
        "\n",
        "Модель линейной регрессии предполагает линейную зависимость между независимыми и зависимыми переменными, и она строится по следующей формуле:\n",
        "\n",
        "- $ Y=β_0 + β_1 X_1 + β_2 X_2 +…+ β_n X_n + ϵ $\n",
        "\n",
        "где:\n",
        "\n",
        "- $Y$ — зависимая переменная, значение которой мы хотим предсказать;\n",
        "\n",
        "- $X_1, X_2 ,…, X_n$ — независимые переменные, которые используются для предсказания значения $Y$;\n",
        "\n",
        "- $β_0$ — это константа, которая представляет собой точку пересечения линии регрессии с осью $Y$, когда все независимые переменные равны нулю;\n",
        "\n",
        "- $β_1, β_2 ,…, β_n$ — это коэффициенты регрессии, которые представляют изменение зависимой переменной $Y$ на единицу, при изменении соответствующей независимой переменной $X$, при условии, что все остальные независимые переменные остаются неизменными;\n",
        "\n",
        "- $ϵ$ — это ошибка, которая представляет разницу между фактическим значением зависимой переменной и значением, предсказанным моделью линейной регрессии.\n",
        "\n",
        "- Задача обучения линейной регрессии сводится к поиску весов $w$ через выражение $w = (X^TX)^{-1}X^Ty$ и, что эквивалентно, к нахождению оптимальных значений коэффициентов $β_0, β_1 ,…, β_n$, которые минимизируют среднеквадратичную ошибку (MSE). Формула для `w` является результатом аналитического решения задачи минимизации MSE.\n",
        "\n",
        "#### **Оценка параметров**\n",
        "\n",
        "Для оценки параметров $β_0, β_1 ,…, β_n$ обычно используется метод наименьших квадратов (OLS, Ordinary Least Squares). Этот метод минимизирует сумму квадратов ошибок [SSE, Sum of Squared Errors](https://verimot-e.ru/summa-kvadratov-osibok-sse), которые представляют собой разницу между фактическими и предсказанными значениями зависимой переменной.\n",
        "\n",
        "- $ [ SSE = \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2] $\n",
        "\n",
        "где:\n",
        "\n",
        "- $Y_i$ — фактическое значение зависимой переменной для $i$-го наблюдения;\n",
        "\n",
        "- $\\hat{Y}_i$— значение зависимой переменной, предсказанное моделью линейной регрессии для $i$-го наблюдения.\n",
        "\n",
        "#### **Подбор значений наименьших квадратов**\n",
        "\n",
        "- **Метод наименьших квадратов (OLS)**: Это статистический метод, используемый для оценки параметров линейной регрессии. OLS находит линию (или плоскость в многомерном пространстве), которая минимизирует сумму квадратов расстояний между наблюдаемыми значениями и значениями, предсказанными моделью. Минимизация суммы квадратов ошибок является разумным критерием, особенно при предположении о нормальном распределении ошибок.\n",
        "\n",
        "\n",
        "- **Сумма квадратов ошибок (SSE)** - это сумма квадратов разностей между фактическими значениями зависимой переменной и предсказанными значениями, полученными с помощью модели. SSE часто используется в контексте метода наименьших квадратов (OLS) при оценке качества подгонки модели.\n",
        "SSE измеряет общую сумму квадратов отклонений всех точек данных от их предсказанных значений, представляя собой меру общей ошибки модели. Чем меньше значение SSE, тем лучше модель соответствует наблюдаемым данным.\n",
        "\n",
        "В контексте линейной регрессии, MSE часто используется как функция потерь, которую метод наименьших квадратов пытается минимизировать. При использовании OLS для построения модели линейной регрессии, результаты оценки коэффициентов обычно получаются путем минимизации среднеквадратичной ошибки.\n",
        "\n",
        "Метод наименьших квадратов (OLS) минимизирует сумму квадратов ошибок (SSE), находя значения параметров $β_0, β_1, …, β_n$, которые минимизируют SSE. Для этого, необходимо взять производные SSE по каждому параметру, приравнять их к нулю и решить полученные уравнения. Это приведет к аналитическому решению, описанному ниже.\n",
        "\n",
        "#### **Аналитическое решение**\n",
        "\n",
        "Линейная регрессия может быть представлена через аналитическое решение. Аналитическое решение можно получить, минимизируя сумму квадратов ошибок (SSE), что приводит к системе линейных уравнений, известной как уравнения нормального состояния. Решение этой системы уравнений находится путем взятия частных производных SSE по каждому параметру ($β_0, β_1$) и приравнивания их к нулю. Для простой линейной регрессии с одной независимой переменной аналитическое решение может быть найдено из следующих формул:\n",
        "\n",
        "- $ β_1 = \\frac{n(\\sum xy) - (\\sum x)(\\sum y)}{n\\sum x^2 - (\\sum x)^2} $\n",
        "\n",
        "- $ β_0 = \\frac{\\sum y - β_1\\sum x}{n} $\n",
        "\n",
        "* *(Дополнительно, для продвинутого уровня):* \"В матричной форме модель линейной регрессии записывается как $Y = Xβ + ϵ$, а аналитическое решение для вектора коэффициентов $β$ имеет вид $β = (X^TX)^{-1}X^Ty$.\n",
        "\n",
        "#### **Предпосылки линейной регрессии**\n",
        "\n",
        "Для того чтобы модель линейной регрессии была действительной, должны выполняться следующие предпосылки:\n",
        "\n",
        "- Линейность: Зависимость между зависимой и независимыми переменными должна быть линейной;\n",
        "\n",
        "- Нормальность ошибок: Ошибки должны быть нормально распределены;\n",
        "\n",
        "- Гомоскедастичность ошибок: Дисперсия ошибок должна быть одинаковой для всех значений независимых переменных;\n",
        "\n",
        "- Независимость ошибок: Ошибки должны быть независимыми друг от друга;\n",
        "\n",
        "- Отсутствие мультиколлинеарности: Независимые переменные не должны быть сильно коррелированными друг с другом.\n",
        "\n",
        "### Метод наименьших квадратов\n",
        "\n",
        "Метод наименьших квадратов (МНК) используется для нахождения коэффициентов линейной регрессии, минимизируя сумму квадратов отклонений между предсказанными значениями $y'$ и фактическими значениями $y$. Формально это можно записать как:\n",
        "\n",
        "$$\n",
        "\\text{minimize} \\quad S = \\sum_{i=1}^{n} (y_i' - \\hat{y}_i)^2\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $S$ — сумма квадратов отклонений;\n",
        "- $y_i'$ — предсказанное значение;\n",
        "- $\\hat{y}_i$ — фактическое значение.\n",
        "\n",
        "Коэффициенты регрессии ($\\beta_0$ и $\\beta_1$) подбираются таким образом, чтобы минимизировать $S$. В случае простой линейной регрессии уравнение модели выглядит следующим образом:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\beta_0 + \\beta_1 x\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $\\hat{y}$ — предсказанное значение зависимой переменной;\n",
        "- $\\beta_0$ — свободный член;\n",
        "- $\\beta_1$ — коэффициент наклона."
      ],
      "metadata": {
        "id": "icMlkV8aBwCi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **Вопрос 41: Коэффициент детерминации в модели регрессии.**\n",
        "\n",
        "### 10. R² Score (Коэффициент детерминации)\n",
        "\n",
        "Коэффициент детерминации $R^2$ является одной из ключевых метрик для оценки качества регрессионных моделей. Он показывает, какая доля общей дисперсии зависимой переменной ($Y$) объясняется построенной моделью. Давайте разберем эту концепцию более детально.\n",
        "\n",
        "**1. Цель коэффициента детерминации $R^2$**\n",
        "\n",
        "Основная цель $R^2$ — дать интуитивно понятную оценку того, насколько хорошо регрессионная модель соответствует наблюдаемым данным. Другими словами, он показывает, насколько точно модель предсказывает изменения зависимой переменной на основе изменений независимых переменных.\n",
        "\n",
        "Коэффициент детерминации $R^2$ определяется как:\n",
        "\n",
        "$$\n",
        "R^2 = 1 - \\frac{\\text{Var}(\\text{необъясненная})}{\\text{Var}(Y)}\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $\\text{Var}(\\text{необъясненная})$ — дисперсия, не объясненная моделью,\n",
        "- $\\text{Var}(Y)$ — общая дисперсия зависимой переменной.\n",
        "\n",
        "**2. Разбор знаменателя: Общая дисперсия зависимой переменной ($\\text{Var}(Y)$)**\n",
        "\n",
        "Знаменатель формулы, $\\text{Var}(Y)$, представляет собой общую дисперсию зависимой переменной $Y$. Дисперсия измеряет разброс значений $Y$ относительно их среднего значения. Формально, общая дисперсия рассчитывается как:\n",
        "\n",
        "$$\n",
        "\\text{Var}(Y) = \\frac{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}{n-1} \\quad \\text{или} \\quad \\frac{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}{n}\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $y_i$ — индивидуальное значение зависимой переменной,\n",
        "- $\\bar{y}$ — среднее значение зависимой переменной,\n",
        "- $n$ — количество наблюдений.\n",
        "\n",
        "В контексте $R^2$, $\\text{Var}(Y)$ представляет собой **полную изменчивость** зависимой переменной, которую мы хотим объяснить с помощью нашей модели. Это мера того, насколько значения $Y$ отличаются друг от друга.\n",
        "\n",
        "**3. Разбор числителя: Необъясненная дисперсия ($\\text{Var}(\\text{необъясненная})$)**\n",
        "\n",
        "Числитель формулы, $\\text{Var}(\\text{необъясненная})$, представляет собой дисперсию ошибок или остатков модели. Ошибки (или остатки) — это разница между фактическими значениями зависимой переменной ($y_i$) и значениями, предсказанными моделью ($\\hat{y}_i$). Формально, необъясненная дисперсия рассчитывается как:\n",
        "\n",
        "$$\n",
        "\\text{Var}(\\text{необъясненная}) = \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{n-p-1} \\quad \\text{или} \\quad \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{n-p} \\quad \\text{или} \\quad \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{n}\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $y_i$ — фактическое значение зависимой переменной,\n",
        "- $\\hat{y}_i$ — значение зависимой переменной, предсказанное моделью,\n",
        "- $n$ — количество наблюдений,\n",
        "- $p$ — количество независимых переменных в модели.\n",
        "\n",
        "$\\text{Var}(\\text{необъясненная})$ показывает, какая часть изменчивости зависимой переменной **не была учтена** моделью. Чем меньше эта величина, тем лучше модель соответствует данным. Сумма квадратов ошибок (SSE, Sum of Squared Errors) в числителе отражает суммарное отклонение предсказанных значений от фактических.\n",
        "\n",
        "**4. Интерпретация отношения $\\frac{\\text{Var}(\\text{необъясненная})}{\\text{Var}(Y)}$**\n",
        "\n",
        "Отношение $\\frac{\\text{Var}(\\text{необъясненная})}{\\text{Var}(Y)}$ представляет собой долю общей дисперсии зависимой переменной, которая **не объясняется** моделью. Это значение всегда находится в диапазоне от 0 до 1.\n",
        "\n",
        "- Если это отношение близко к 1, это означает, что большая часть дисперсии $Y$ не объясняется моделью, и модель плохо соответствует данным.\n",
        "- Если это отношение близко к 0, это означает, что модель объясняет большую часть дисперсии $Y$, и модель хорошо соответствует данным.\n",
        "\n",
        "**5. Интерпретация $1 - \\frac{\\text{Var}(\\text{необъясненная})}{\\text{Var}(Y)}$**\n",
        "\n",
        "Вычитание отношения необъясненной дисперсии к общей дисперсии из 1 дает нам долю общей дисперсии зависимой переменной, которая **объясняется** моделью. Это и есть коэффициент детерминации $R^2$.\n",
        "\n",
        "$$\n",
        "R^2 = 1 - \\frac{\\text{Var}(\\text{необъясненная})}{\\text{Var}(Y)} = \\frac{\\text{Var}(Y) - \\text{Var}(\\text{необъясненная})}{\\text{Var}(Y)}\n",
        "$$\n",
        "\n",
        "**6. Диапазон значений $R^2$ и его интерпретация**\n",
        "\n",
        "Коэффициент детерминации $R^2$ принимает значения в диапазоне от 0 до 1:\n",
        "\n",
        "- **$R^2 = 0$**: Модель не объясняет никакой дисперсии зависимой переменной. Это означает, что модель не лучше, чем простое использование среднего значения $Y$ для предсказаний.\n",
        "- **$0 < R^2 < 1$**: Модель объясняет некоторую долю дисперсии зависимой переменной. Чем ближе $R^2$ к 1, тем лучше модель соответствует данным.\n",
        "- **$R^2 = 1$**: Модель идеально объясняет всю дисперсию зависимой переменной. Это означает, что все наблюдаемые значения $Y$ точно соответствуют значениям, предсказанным моделью.\n",
        "\n",
        "**Важные моменты и ограничения:**\n",
        "\n",
        "* **$R^2$ не говорит о причинно-следственных связях.** Высокий $R^2$ не означает, что независимые переменные вызывают изменения в зависимой переменной. Это лишь указывает на статистическую связь.\n",
        "* **$R^2$ чувствителен к добавлению новых переменных.**  Добавление новых независимых переменных в модель всегда увеличивает или, в худшем случае, не изменяет $R^2$, даже если эти переменные не имеют реальной связи с зависимой переменной. Это может привести к переобучению модели. Для решения этой проблемы используется скорректированный $R^2$ (Adjusted $R^2$).\n",
        "* **$R^2$ не оценивает адекватность модели.**  Высокий $R^2$ не гарантирует, что модель является подходящей для данных. Могут существовать другие проблемы, такие как нелинейность, гетероскедастичность или автокорреляция ошибок, которые не отражаются в $R^2$.\n",
        "\n",
        "**Пример для иллюстрации:**\n",
        "\n",
        "Предположим, вы разрабатываете модель для предсказания стоимости домов на основе их площади (в квадратных метрах). Вы собрали данные по 100 домам и построили линейную регрессионную модель. После обучения модели вы получили коэффициент детерминации $( R^2 = 0.72 )$.\n",
        "\n",
        "**Интерпретация результата:**\n",
        "\n",
        "1. **Общая дисперсия зависимой переменной $( \\text{Var}(Y) )$**:\n",
        "   - Общая дисперсия стоимости домов $( Y )$ отражает, насколько сильно цены на дома отличаются друг от друга. Например, если цены на дома варьируются от 100 000 до 500 000 долларов, это и есть общая изменчивость, которую мы хотим объяснить.\n",
        "\n",
        "2. **Необъясненная дисперсия $( \\text{Var}(\\text{необъясненная}) )$**:\n",
        "   - После построения модели вы обнаружили, что остатки (разница между фактическими ценами и предсказанными) составляют 28% от общей дисперсии. Это означает, что 28% изменчивости цен на дома не объясняется площадью дома. Возможно, это связано с другими факторами, такими как местоположение, год постройки или состояние дома, которые не были учтены в модели.\n",
        "\n",
        "3. **Объясненная дисперсия $( R^2 = 0.72 )$**:\n",
        "   - Модель объясняет 72% дисперсии цен на дома. Это означает, что площадь дома является важным фактором, влияющим на стоимость, и модель хорошо справляется с предсказанием цен на основе этого признака.\n",
        "\n",
        "4. **Практическое применение**:\n",
        "   - Если вы используете эту модель для предсказания стоимости нового дома, то в 72% случаев модель будет давать точные предсказания, основываясь на площади дома. Однако в 28% случаев предсказания будут отклоняться от реальных значений из-за неучтённых факторов.\n",
        "\n",
        "5. **Ограничения модели**:\n",
        "   - Хотя $( R^2 = 0.72 )$ является хорошим показателем, важно помнить, что модель не учитывает другие важные факторы, такие как местоположение или состояние дома. Это может ограничивать её применимость в реальных условиях. Например, если два дома имеют одинаковую площадь, но один находится в престижном районе, а другой — в менее привлекательном, модель может недооценить или переоценить стоимость.\n",
        "\n",
        "6. **Дополнительные улучшения**:\n",
        "   - Чтобы улучшить модель, можно добавить дополнительные независимые переменные, такие как количество комнат, год постройки или расстояние до центра города. Это может увеличить $( R^2 )$ и уменьшить необъяснённую дисперсию.\n",
        "\n",
        "**Заключение:**\n",
        "Коэффициент детерминации $( R^2 = 0.72 )$ показывает, что модель хорошо объясняет изменчивость цен на дома на основе площади, но всё же остаётся значительная часть дисперсии, которая не объясняется моделью. Это подчеркивает важность учёта дополнительных факторов и использования других метрик для оценки качества модели.\n",
        "\n",
        "Коэффициент детерминации $R^2$ является полезным инструментом для оценки качества регрессионных моделей, позволяющим понять, какая доля изменчивости зависимой переменной объясняется моделью. Однако важно помнить о его ограничениях и использовать его в сочетании с другими метриками и методами анализа.\n",
        "\n",
        "### Пример:\n",
        "\n",
        "```python\n",
        "def calculate_r2_score(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Description:\n",
        "      Вычисляет коэффициент детерминации R².\n",
        "\n",
        "    Args:\n",
        "    :param y_true: Список истинных значений.\n",
        "    :param y_pred: Список предсказанных значений.\n",
        "    :return: Значение R².\n",
        "    \"\"\"\n",
        "    mean_true = sum(y_true) / len(y_true)\n",
        "    ss_tot = sum((true - mean_true) ** 2 for true in y_true)\n",
        "    ss_res = sum((true - pred) ** 2 for true, pred in zip(y_true, y_pred))\n",
        "    return 1 - ss_res / ss_tot\n",
        "\n",
        "# Пример использования\n",
        "y_true = [3, 5, 2.5, 7]\n",
        "y_pred = [2.5, 5, 4, 8]\n",
        "print(f\"R² Score: {calculate_r2_score(y_true, y_pred)}\")\n",
        "```"
      ],
      "metadata": {
        "id": "kxoV5M99Vnzo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **Вопрос 42: Модели кластерного анализа (ближнего соседа).**\n",
        "\n",
        "## **Кластерный анализ и его применение**\n",
        "\n",
        "Кластерный анализ — это метод статистического анализа, который используется для группировки объектов в кластеры на основе их схожести. В отличие от факторного анализа, который фокусируется на группировке признаков, кластерный анализ направлен на классификацию объектов. Это позволяет выявлять структуры в данных и находить аномальные распределения.\n",
        "\n",
        "### Основные концепции кластерного анализа\n",
        "\n",
        "Кластерный анализ выполняет задачу классификации объектов, которые могут быть представлены как точки в пространстве признаков. Количество кластеров может быть заранее неизвестным, и существуют различные методы для определения оптимального числа кластеров. Например, метод локтя позволяет визуально оценить, при каком количестве кластеров качество кластеризации начинает стабилизироваться.\n",
        "\n",
        "Каждый объект в кластерном анализе представляется как точка в многомерном пространстве, где каждое измерение соответствует одному из признаков. Например, если мы анализируем регионы по распределению баллов на ЕГЭ, мы можем использовать такие признаки, как процент двоек, процент пятерок и средний балл.\n",
        "\n",
        "### Определение оптимального количества кластеров: метод локтя\n",
        "\n",
        "Одним из распространенных методов для определения оптимального количества кластеров является **метод локтя** (Elbow Method). Этот метод основан на анализе того, как изменяется **внутрикластерная дисперсия** (или другая метрика качества кластеризации) при увеличении количества кластеров.\n",
        "\n",
        "**Как работает метод локтя:**\n",
        "\n",
        "1. **Запуск алгоритма кластеризации с разным количеством кластеров:**  Алгоритм кластеризации (например, K-средних) запускается несколько раз, каждый раз с разным количеством кластеров, например, от 1 до некоторого максимального значения $K_{max}$.\n",
        "\n",
        "2. **Расчет метрики качества кластеризации:** Для каждого количества кластеров рассчитывается метрика, отражающая качество кластеризации. Наиболее часто используемой метрикой является **сумма квадратов расстояний от точек до центроидов своих кластеров** (Within-Cluster Sum of Squares, WCSS).\n",
        "\n",
        "   Пусть $C_i$ — $i$-й кластер, а $\\mu_i$ — его центроид (среднее значение признаков объектов в кластере). WCSS для $k$ кластеров определяется как:\n",
        "   $$WCSS(k) = \\sum_{i=1}^{k} \\sum_{x \\in C_i} d(x_i, \\mu_i)^2$$\n",
        "   где $d(x_i, \\mu_i)$ — расстояние между объектом $x_i$ и центроидом его кластера $\\mu_i$ (обычно используется евклидово расстояние).\n",
        "\n",
        "3. **Построение графика зависимости метрики от количества кластеров:** Строится график, где по оси X отложено количество кластеров, а по оси Y — значение метрики качества (например, WCSS).\n",
        "\n",
        "4. **Поиск \"локтя\" на графике:**  На полученном графике ищется точка, где снижение значения метрики замедляется, образуя визуальный \"изгиб\" или \"локоть\".\n",
        "\n",
        "**Что означает стабилизация качества кластеризации:**\n",
        "\n",
        "При увеличении количества кластеров, как правило, значение WCSS уменьшается, поскольку каждый объект может быть ближе к центроиду своего собственного, меньшего кластера. Однако, на определенном этапе добавление новых кластеров начинает приносить все меньше и меньше \"выгоды\" в плане снижения WCSS.\n",
        "\n",
        "**Стабилизация качества кластеризации означает, что при дальнейшем увеличении количества кластеров, уменьшение метрики качества становится незначительным.**  Это происходит потому, что уже выделены основные, наиболее естественные группы в данных, и добавление новых кластеров приводит лишь к разделению уже существующих, хорошо сформированных кластеров на более мелкие, без существенного улучшения внутренней однородности.\n",
        "\n",
        "**Визуально, это проявляется как \"изгиб\" на графике.**  До точки \"локтя\" снижение метрики происходит достаточно быстро, что говорит о том, что добавление кластеров эффективно выделяет новые структуры в данных. После точки \"локтя\" снижение метрики становится более плавным, что указывает на то, что новые кластеры не добавляют значительной информации о структуре данных.\n",
        "\n",
        "**Пример:**\n",
        "\n",
        "Представьте, что вы кластеризуете данные на 2 кластера. Объекты внутри каждого кластера достаточно близки друг к другу. Если вы увеличите количество кластеров до 3, возможно, один из исходных кластеров разделится на два, и объекты внутри новых кластеров станут еще более однородными, что приведет к значительному снижению WCSS. Однако, если вы продолжите увеличивать количество кластеров, например, до 10, новые кластеры могут начать выделять лишь небольшие группы очень похожих объектов, не отражая общую структуру данных, и снижение WCSS будет незначительным.\n",
        "\n",
        "Таким образом, метод локтя позволяет визуально оценить, при каком количестве кластеров достигается баланс между уменьшением внутрикластерной дисперсии и сложностью модели (количеством кластеров). Точка \"локтя\" считается эвристической оценкой оптимального количества кластеров.\n",
        "\n",
        "В контексте метода локтя, термин \"эвристический\" означает, что оценка оптимального количества кластеров является **приблизительной** и основана на **практическом опыте и интуиции**, а не на строгих математических доказательствах или гарантиях оптимальности.\n",
        "\n",
        "* **Эвристика в общем смысле:**  Это метод решения проблемы, который использует практические правила, догадки или \"эмпирические\" знания для нахождения решения, которое, скорее всего, будет хорошим, но не обязательно оптимальным. Эвристики часто используются, когда точное решение найти сложно или требует слишком много вычислительных ресурсов.\n",
        "\n",
        "### Нормализация данных\n",
        "\n",
        "Перед проведением кластерного анализа важно нормализовать данные, чтобы признаки, имеющие разные шкалы, были сопоставимы. Например, процент двоек и пятерок могут иметь разные диапазоны значений, и их необходимо привести к единой шкале.\n",
        "\n",
        "### Методы измерения расстояния\n",
        "\n",
        "Кластерный анализ основывается на понятии расстояния между объектами. Расстояние определяет, насколько близки объекты друг к другу в пространстве признаков. Наиболее распространенные метрики расстояния включают:\n",
        "\n",
        "- **Евклидово расстояние**: используется для измерения прямого расстояния между двумя точками в пространстве.\n",
        "\n",
        "Пусть даны два объекта (точки) в n-мерном пространстве признаков:  $x = (x_1, x_2, ..., x_n)$ и $y = (y_1, y_2, ..., y_n)$.\n",
        "\n",
        "Евклидово расстояние между этими двумя объектами обозначается как $d(x, y)$ и вычисляется по формуле:\n",
        "\n",
        "$d(x, y) = \\sqrt{\\sum_{i=1}^{n}(x_i - y_i)^2}$\n",
        "\n",
        "**Детальное пояснение:**\n",
        "\n",
        "*   **$d(x, y)$**:  Обозначение евклидова расстояния между объектами $x$ и $y$.\n",
        "*   **$\\sum_{i=1}^{n}$**:  Символ суммы, указывающий на то, что мы будем складывать результаты для каждого измерения (признака).\n",
        "*   **$i=1$**:  Начальный индекс суммирования, указывающий на первый признак.\n",
        "*   **$n$**:  Количество измерений (признаков) у каждого объекта.\n",
        "*   **$(x_i - y_i)$**:  Разница между значениями $i$-го признака у объектов $x$ и $y$.\n",
        "*   **$(x_i - y_i)^2$**:  Квадрат этой разницы. Возведение в квадрат гарантирует, что разница всегда будет положительной и придает больше веса большим различиям.\n",
        "*   **$\\sqrt{...}$**:  Квадратный корень из суммы квадратов разностей. Эта операция возвращает расстояние к исходным единицам измерения и соответствует геометрическому понятию прямого расстояния \"как есть\".\n",
        "\n",
        "**Интуитивное понимание:**\n",
        "\n",
        "Евклидово расстояние представляет собой длину прямой линии, соединяющей две точки в многомерном пространстве. Это наиболее интуитивно понятная мера расстояния, особенно в двумерном или трехмерном пространстве, где мы можем визуализировать прямую линию между точками.\n",
        "\n",
        "- **Манхэттенское расстояние**: измеряет расстояние по осям, как если бы вы перемещались по сетке.\n",
        "\n",
        "Пусть даны два объекта (точки) в n-мерном пространстве признаков:  $x = (x_1, x_2, ..., x_n)$ и $y = (y_1, y_2, ..., y_n)$.\n",
        "\n",
        "Манхэттенское расстояние между этими двумя объектами обозначается как $d(x, y)$ и вычисляется по формуле:\n",
        "\n",
        "$d(x, y) = \\sum_{i=1}^{n}|x_i - y_i|$\n",
        "\n",
        "**Детальное пояснение:**\n",
        "\n",
        "*   **$d(x, y)$**: Обозначение манхэттенского расстояния между объектами $x$ и $y$.\n",
        "*   **$\\sum_{i=1}^{n}$**: Символ суммы, указывающий на то, что мы будем складывать результаты для каждого измерения (признака).\n",
        "*   **$i=1$**: Начальный индекс суммирования, указывающий на первый признак.\n",
        "*   **$n$**: Количество измерений (признаков) у каждого объекта.\n",
        "*   **$|x_i - y_i|$**: Абсолютное значение разницы между значениями $i$-го признака у объектов $x$ и $y$. Абсолютное значение используется для того, чтобы разница всегда была положительной, независимо от того, какое значение больше.\n",
        "\n",
        "**Интуитивное понимание:**\n",
        "\n",
        "Манхэттенское расстояние можно представить как расстояние, которое нужно пройти между двумя точками в городе с прямоугольной сеткой улиц. Вы можете двигаться только вдоль осей координат (как по улицам и проспектам), а не напрямую. Это расстояние также называют расстоянием \"городских кварталов\" или L1-нормой.\n",
        "\n",
        "- **Расстояние Хемминга**: используется для категориальных данных и измеряет количество различий между двумя объектами.\n",
        "\n",
        "Пусть даны два объекта (обычно строки или векторы) одинаковой длины $n$, состоящие из категориальных признаков: $x = (x_1, x_2, ..., x_n)$ и $y = (y_1, y_2, ..., y_n)$.\n",
        "\n",
        "Расстояние Хемминга между этими двумя объектами обозначается как $H(x, y)$ и вычисляется по формуле:\n",
        "\n",
        "$H(x, y) = \\sum_{i=1}^{n} I(x_i \\neq y_i)$\n",
        "\n",
        "где $I(condition)$ - индикаторная функция, которая равна 1, если условие истинно, и 0, если ложно.\n",
        "\n",
        "**Детальное пояснение:**\n",
        "\n",
        "*   **$H(x, y)$**: Обозначение расстояния Хемминга между объектами $x$ и $y$.\n",
        "*   **$\\sum_{i=1}^{n}$**: Символ суммы, указывающий на то, что мы будем складывать результаты для каждого признака.\n",
        "*   **$i=1$**: Начальный индекс суммирования, указывающий на первый признак.\n",
        "*   **$n$**: Количество признаков у каждого объекта (длина строк или векторов).\n",
        "*   **$I(x_i \\neq y_i)$**: Индикаторная функция.\n",
        "    *   Если значение $i$-го признака у объекта $x$ **не равно** значению $i$-го признака у объекта $y$ ($x_i \\neq y_i$), то индикаторная функция возвращает 1.\n",
        "    *   Если значение $i$-го признака у объекта $x$ **равно** значению $i$-го признака у объекта $y$ ($x_i = y_i$), то индикаторная функция возвращает 0.\n",
        "\n",
        "**Интуитивное понимание:**\n",
        "\n",
        "Расстояние Хемминга подсчитывает количество позиций, в которых соответствующие символы или значения в двух объектах различаются. Оно идеально подходит для сравнения категориальных данных, таких как последовательности ДНК (A, T, C, G), бинарные векторы (0 и 1), или ответы на вопросы (да/нет). Чем больше различий между двумя объектами, тем больше расстояние Хемминга.\n",
        "\n",
        "### Пример кода для кластерного анализа\n",
        "\n",
        "Вот пример кода, который демонстрирует, как использовать метод K-средних для кластеризации данных с использованием библиотеки `scikit-learn`:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Генерация случайных данных\n",
        "np.random.seed(0)\n",
        "X = np.random.rand(100, 2)     # 100 случайных точек в 2D пространстве\n",
        "\n",
        "# Применение метода K-средних\n",
        "kmeans = KMeans(n_clusters=3)  # Задаем количество кластеров\n",
        "kmeans.fit(X)                  # Обучаем модель\n",
        "\n",
        "# Получение меток кластеров\n",
        "labels = kmeans.labels_\n",
        "\n",
        "# Визуализация результатов\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')  # Цвета по меткам кластеров\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], color='red', marker='X', s=200, label='Центры кластеров')\n",
        "plt.title('Кластеризация методом K-средних')\n",
        "plt.xlabel('Признак 1')\n",
        "plt.ylabel('Признак 2')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "В этом коде:\n",
        "- Мы генерируем случайные данные в двумерном пространстве.\n",
        "- Применяем метод K-средних для кластеризации данных на 3 кластера.\n",
        "- Визуализируем результаты, показывая центры кластеров.\n",
        "\n",
        "### Физический и геометрический смысл кластерного анализа\n",
        "\n",
        "Кластерный анализ можно проиллюстрировать на примере группировки студентов по их успеваемости. Если мы имеем данные о баллах студентов по различным предметам, мы можем использовать кластерный анализ для выделения групп студентов с похожими результатами. Это может помочь в выявлении аномальных групп, например, студентов, которые показывают значительно более низкие или высокие результаты по сравнению с остальными.\n",
        "\n",
        "Таким образом, кластерный анализ является мощным инструментом для анализа данных, позволяя выявлять структуры и закономерности в больших объемах информации."
      ],
      "metadata": {
        "id": "uxhxC3ygBC5Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **Вопрос 43: Модели кластерного анализа (ближнего соседа).**\n",
        "\n",
        "## **Кластерный анализ и его применение**\n",
        "\n",
        "Кластерный анализ — это метод статистического анализа, который используется для группировки объектов в кластеры на основе их схожести. В отличие от факторного анализа, который фокусируется на группировке признаков, кластерный анализ направлен на классификацию объектов. Это позволяет выявлять структуры в данных и находить аномальные распределения.\n",
        "\n",
        "### Основные концепции кластерного анализа\n",
        "\n",
        "Кластерный анализ выполняет задачу классификации объектов, которые могут быть представлены как точки в пространстве признаков. Количество кластеров может быть заранее неизвестным, и существуют различные методы для определения оптимального числа кластеров. Например, метод локтя позволяет визуально оценить, при каком количестве кластеров качество кластеризации начинает стабилизироваться.\n",
        "\n",
        "Каждый объект в кластерном анализе представляется как точка в многомерном пространстве, где каждое измерение соответствует одному из признаков. Например, если мы анализируем регионы по распределению баллов на ЕГЭ, мы можем использовать такие признаки, как процент двоек, процент пятерок и средний балл.\n",
        "\n",
        "### Определение оптимального количества кластеров: метод локтя\n",
        "\n",
        "Одним из распространенных методов для определения оптимального количества кластеров является **метод локтя** (Elbow Method). Этот метод основан на анализе того, как изменяется **внутрикластерная дисперсия** (или другая метрика качества кластеризации) при увеличении количества кластеров.\n",
        "\n",
        "**Как работает метод локтя:**\n",
        "\n",
        "1. **Запуск алгоритма кластеризации с разным количеством кластеров:**  Алгоритм кластеризации (например, K-средних) запускается несколько раз, каждый раз с разным количеством кластеров, например, от 1 до некоторого максимального значения $K_{max}$.\n",
        "\n",
        "2. **Расчет метрики качества кластеризации:** Для каждого количества кластеров рассчитывается метрика, отражающая качество кластеризации. Наиболее часто используемой метрикой является **сумма квадратов расстояний от точек до центроидов своих кластеров** (Within-Cluster Sum of Squares, WCSS).\n",
        "\n",
        "   Пусть $C_i$ — $i$-й кластер, а $\\mu_i$ — его центроид (среднее значение признаков объектов в кластере). WCSS для $k$ кластеров определяется как:\n",
        "   $$WCSS(k) = \\sum_{i=1}^{k} \\sum_{x \\in C_i} d(x_i, \\mu_i)^2$$\n",
        "   где $d(x_i, \\mu_i)$ — расстояние между объектом $x_i$ и центроидом его кластера $\\mu_i$ (обычно используется евклидово расстояние).\n",
        "\n",
        "3. **Построение графика зависимости метрики от количества кластеров:** Строится график, где по оси X отложено количество кластеров, а по оси Y — значение метрики качества (например, WCSS).\n",
        "\n",
        "4. **Поиск \"локтя\" на графике:**  На полученном графике ищется точка, где снижение значения метрики замедляется, образуя визуальный \"изгиб\" или \"локоть\".\n",
        "\n",
        "**Что означает стабилизация качества кластеризации:**\n",
        "\n",
        "При увеличении количества кластеров, как правило, значение WCSS уменьшается, поскольку каждый объект может быть ближе к центроиду своего собственного, меньшего кластера. Однако, на определенном этапе добавление новых кластеров начинает приносить все меньше и меньше \"выгоды\" в плане снижения WCSS.\n",
        "\n",
        "**Стабилизация качества кластеризации означает, что при дальнейшем увеличении количества кластеров, уменьшение метрики качества становится незначительным.**  Это происходит потому, что уже выделены основные, наиболее естественные группы в данных, и добавление новых кластеров приводит лишь к разделению уже существующих, хорошо сформированных кластеров на более мелкие, без существенного улучшения внутренней однородности.\n",
        "\n",
        "**Визуально, это проявляется как \"изгиб\" на графике.**  До точки \"локтя\" снижение метрики происходит достаточно быстро, что говорит о том, что добавление кластеров эффективно выделяет новые структуры в данных. После точки \"локтя\" снижение метрики становится более плавным, что указывает на то, что новые кластеры не добавляют значительной информации о структуре данных.\n",
        "\n",
        "**Пример:**\n",
        "\n",
        "Представьте, что вы кластеризуете данные на 2 кластера. Объекты внутри каждого кластера достаточно близки друг к другу. Если вы увеличите количество кластеров до 3, возможно, один из исходных кластеров разделится на два, и объекты внутри новых кластеров станут еще более однородными, что приведет к значительному снижению WCSS. Однако, если вы продолжите увеличивать количество кластеров, например, до 10, новые кластеры могут начать выделять лишь небольшие группы очень похожих объектов, не отражая общую структуру данных, и снижение WCSS будет незначительным.\n",
        "\n",
        "Таким образом, метод локтя позволяет визуально оценить, при каком количестве кластеров достигается баланс между уменьшением внутрикластерной дисперсии и сложностью модели (количеством кластеров). Точка \"локтя\" считается эвристической оценкой оптимального количества кластеров.\n",
        "\n",
        "В контексте метода локтя, термин \"эвристический\" означает, что оценка оптимального количества кластеров является **приблизительной** и основана на **практическом опыте и интуиции**, а не на строгих математических доказательствах или гарантиях оптимальности.\n",
        "\n",
        "* **Эвристика в общем смысле:**  Это метод решения проблемы, который использует практические правила, догадки или \"эмпирические\" знания для нахождения решения, которое, скорее всего, будет хорошим, но не обязательно оптимальным. Эвристики часто используются, когда точное решение найти сложно или требует слишком много вычислительных ресурсов.\n",
        "\n",
        "### Нормализация данных\n",
        "\n",
        "Перед проведением кластерного анализа важно нормализовать данные, чтобы признаки, имеющие разные шкалы, были сопоставимы. Например, процент двоек и пятерок могут иметь разные диапазоны значений, и их необходимо привести к единой шкале.\n",
        "\n",
        "### Методы измерения расстояния\n",
        "\n",
        "Кластерный анализ основывается на понятии расстояния между объектами. Расстояние определяет, насколько близки объекты друг к другу в пространстве признаков. Наиболее распространенные метрики расстояния включают:\n",
        "\n",
        "- **Евклидово расстояние**: используется для измерения прямого расстояния между двумя точками в пространстве.\n",
        "\n",
        "Пусть даны два объекта (точки) в n-мерном пространстве признаков:  $x = (x_1, x_2, ..., x_n)$ и $y = (y_1, y_2, ..., y_n)$.\n",
        "\n",
        "Евклидово расстояние между этими двумя объектами обозначается как $d(x, y)$ и вычисляется по формуле:\n",
        "\n",
        "$d(x, y) = \\sqrt{\\sum_{i=1}^{n}(x_i - y_i)^2}$\n",
        "\n",
        "**Детальное пояснение:**\n",
        "\n",
        "*   **$d(x, y)$**:  Обозначение евклидова расстояния между объектами $x$ и $y$.\n",
        "*   **$\\sum_{i=1}^{n}$**:  Символ суммы, указывающий на то, что мы будем складывать результаты для каждого измерения (признака).\n",
        "*   **$i=1$**:  Начальный индекс суммирования, указывающий на первый признак.\n",
        "*   **$n$**:  Количество измерений (признаков) у каждого объекта.\n",
        "*   **$(x_i - y_i)$**:  Разница между значениями $i$-го признака у объектов $x$ и $y$.\n",
        "*   **$(x_i - y_i)^2$**:  Квадрат этой разницы. Возведение в квадрат гарантирует, что разница всегда будет положительной и придает больше веса большим различиям.\n",
        "*   **$\\sqrt{...}$**:  Квадратный корень из суммы квадратов разностей. Эта операция возвращает расстояние к исходным единицам измерения и соответствует геометрическому понятию прямого расстояния \"как есть\".\n",
        "\n",
        "**Интуитивное понимание:**\n",
        "\n",
        "Евклидово расстояние представляет собой длину прямой линии, соединяющей две точки в многомерном пространстве. Это наиболее интуитивно понятная мера расстояния, особенно в двумерном или трехмерном пространстве, где мы можем визуализировать прямую линию между точками.\n",
        "\n",
        "- **Манхэттенское расстояние**: измеряет расстояние по осям, как если бы вы перемещались по сетке.\n",
        "\n",
        "Пусть даны два объекта (точки) в n-мерном пространстве признаков:  $x = (x_1, x_2, ..., x_n)$ и $y = (y_1, y_2, ..., y_n)$.\n",
        "\n",
        "Манхэттенское расстояние между этими двумя объектами обозначается как $d(x, y)$ и вычисляется по формуле:\n",
        "\n",
        "$d(x, y) = \\sum_{i=1}^{n}|x_i - y_i|$\n",
        "\n",
        "**Детальное пояснение:**\n",
        "\n",
        "*   **$d(x, y)$**: Обозначение манхэттенского расстояния между объектами $x$ и $y$.\n",
        "*   **$\\sum_{i=1}^{n}$**: Символ суммы, указывающий на то, что мы будем складывать результаты для каждого измерения (признака).\n",
        "*   **$i=1$**: Начальный индекс суммирования, указывающий на первый признак.\n",
        "*   **$n$**: Количество измерений (признаков) у каждого объекта.\n",
        "*   **$|x_i - y_i|$**: Абсолютное значение разницы между значениями $i$-го признака у объектов $x$ и $y$. Абсолютное значение используется для того, чтобы разница всегда была положительной, независимо от того, какое значение больше.\n",
        "\n",
        "**Интуитивное понимание:**\n",
        "\n",
        "Манхэттенское расстояние можно представить как расстояние, которое нужно пройти между двумя точками в городе с прямоугольной сеткой улиц. Вы можете двигаться только вдоль осей координат (как по улицам и проспектам), а не напрямую. Это расстояние также называют расстоянием \"городских кварталов\" или L1-нормой.\n",
        "\n",
        "- **Расстояние Хемминга**: используется для категориальных данных и измеряет количество различий между двумя объектами.\n",
        "\n",
        "Пусть даны два объекта (обычно строки или векторы) одинаковой длины $n$, состоящие из категориальных признаков: $x = (x_1, x_2, ..., x_n)$ и $y = (y_1, y_2, ..., y_n)$.\n",
        "\n",
        "Расстояние Хемминга между этими двумя объектами обозначается как $H(x, y)$ и вычисляется по формуле:\n",
        "\n",
        "$H(x, y) = \\sum_{i=1}^{n} I(x_i \\neq y_i)$\n",
        "\n",
        "где $I(condition)$ - индикаторная функция, которая равна 1, если условие истинно, и 0, если ложно.\n",
        "\n",
        "**Детальное пояснение:**\n",
        "\n",
        "*   **$H(x, y)$**: Обозначение расстояния Хемминга между объектами $x$ и $y$.\n",
        "*   **$\\sum_{i=1}^{n}$**: Символ суммы, указывающий на то, что мы будем складывать результаты для каждого признака.\n",
        "*   **$i=1$**: Начальный индекс суммирования, указывающий на первый признак.\n",
        "*   **$n$**: Количество признаков у каждого объекта (длина строк или векторов).\n",
        "*   **$I(x_i \\neq y_i)$**: Индикаторная функция.\n",
        "    *   Если значение $i$-го признака у объекта $x$ **не равно** значению $i$-го признака у объекта $y$ ($x_i \\neq y_i$), то индикаторная функция возвращает 1.\n",
        "    *   Если значение $i$-го признака у объекта $x$ **равно** значению $i$-го признака у объекта $y$ ($x_i = y_i$), то индикаторная функция возвращает 0.\n",
        "\n",
        "**Интуитивное понимание:**\n",
        "\n",
        "Расстояние Хемминга подсчитывает количество позиций, в которых соответствующие символы или значения в двух объектах различаются. Оно идеально подходит для сравнения категориальных данных, таких как последовательности ДНК (A, T, C, G), бинарные векторы (0 и 1), или ответы на вопросы (да/нет). Чем больше различий между двумя объектами, тем больше расстояние Хемминга.\n",
        "\n",
        "### Пример кода для кластерного анализа\n",
        "\n",
        "Вот пример кода, который демонстрирует, как использовать метод K-средних для кластеризации данных с использованием библиотеки `scikit-learn`:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Генерация случайных данных\n",
        "np.random.seed(0)\n",
        "X = np.random.rand(100, 2)     # 100 случайных точек в 2D пространстве\n",
        "\n",
        "# Применение метода K-средних\n",
        "kmeans = KMeans(n_clusters=3)  # Задаем количество кластеров\n",
        "kmeans.fit(X)                  # Обучаем модель\n",
        "\n",
        "# Получение меток кластеров\n",
        "labels = kmeans.labels_\n",
        "\n",
        "# Визуализация результатов\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')  # Цвета по меткам кластеров\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], color='red', marker='X', s=200, label='Центры кластеров')\n",
        "plt.title('Кластеризация методом K-средних')\n",
        "plt.xlabel('Признак 1')\n",
        "plt.ylabel('Признак 2')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "В этом коде:\n",
        "- Мы генерируем случайные данные в двумерном пространстве.\n",
        "- Применяем метод K-средних для кластеризации данных на 3 кластера.\n",
        "- Визуализируем результаты, показывая центры кластеров.\n",
        "\n",
        "### Физический и геометрический смысл кластерного анализа\n",
        "\n",
        "Кластерный анализ можно проиллюстрировать на примере группировки студентов по их успеваемости. Если мы имеем данные о баллах студентов по различным предметам, мы можем использовать кластерный анализ для выделения групп студентов с похожими результатами. Это может помочь в выявлении аномальных групп, например, студентов, которые показывают значительно более низкие или высокие результаты по сравнению с остальными.\n",
        "\n",
        "Таким образом, кластерный анализ является мощным инструментом для анализа данных, позволяя выявлять структуры и закономерности в больших объемах информации.\n",
        "\n",
        "## Модель кластерного анализа ближайшего соседа (Nearest Neighbor Clustering)\n",
        "\n",
        "Модель кластерного анализа ближайшего соседа, также известная как **метод связных компонент** или **метод плотностной кластеризации на основе связности**, является непараметрическим алгоритмом кластеризации. В отличие от методов, требующих предварительного задания количества кластеров (например, K-средних), этот подход формирует кластеры на основе локальной плотности данных. Основная идея заключается в том, что точки данных, находящиеся в непосредственной близости друг к другу, принадлежат к одному и тому же кластеру.\n",
        "\n",
        "### Алгоритм работы\n",
        "\n",
        "Алгоритм кластеризации ближайшего соседа работает следующим образом:\n",
        "\n",
        "1. **Выбор параметра близости (ε):**  Определяется максимальное расстояние между двумя точками, при котором они считаются \"соседями\". Этот параметр является ключевым и влияет на размер и форму формируемых кластеров.\n",
        "\n",
        "2. **Поиск соседей для каждой точки:** Для каждой точки данных в наборе определяется множество ее ε-соседей. Точка `q` является ε-соседом точки `p`, если расстояние между ними `dist(p, q)` не превышает ε.\n",
        "\n",
        "3. **Формирование кластеров:**\n",
        "   - Начинаем с произвольной необработанной точки данных.\n",
        "   - Если у этой точки есть хотя бы один ε-сосед, формируется новый кластер, включающий эту точку и всех ее ε-соседей.\n",
        "   - Затем рассматриваются ε-соседи добавленных точек, и если у них есть еще необработанные ε-соседи, они также добавляются в текущий кластер. Этот процесс продолжается рекурсивно, пока не будут добавлены все точки, связанные отношением ε-соседства.\n",
        "   - Если у текущей рассматриваемой точки нет ε-соседей, она считается шумовой точкой и не включается ни в один кластер.\n",
        "   - Процесс повторяется для всех необработанных точек данных, пока все точки не будут отнесены к кластеру или помечены как шум.\n",
        "\n",
        "### Математическая формализация\n",
        "\n",
        "Для формализации модели кластерного анализа ближайшего соседа необходимо определить понятие расстояния между точками и критерий включения точек в один кластер.\n",
        "\n",
        "**1. Пространство данных:**\n",
        "\n",
        "Пусть задано множество точек данных $X = \\{x_1, x_2, ..., x_n\\}$, где каждая точка $x_i \\in \\mathbb{R}^d$, где $d$ - размерность пространства признаков.\n",
        "\n",
        "**2. Функция расстояния:**\n",
        "\n",
        "Для определения \"близости\" между точками необходимо задать функцию расстояния $dist(x_i, x_j)$. Наиболее распространенные метрики расстояния включают:\n",
        "\n",
        "* **Евклидово расстояние (Euclidean distance):**\n",
        "   $dist(x_i, x_j) = \\sqrt{\\sum_{k=1}^{d} (x_{ik} - x_{jk})^2}$\n",
        "   где $x_{ik}$ и $x_{jk}$ - значения $k$-го признака для точек $x_i$ и $x_j$ соответственно.\n",
        "\n",
        "* **Манхэттенское расстояние (Manhattan distance) или L1-норма:**\n",
        "   $dist(x_i, x_j) = \\sum_{k=1}^{d} |x_{ik} - x_{jk}|$\n",
        "\n",
        "* **Расстояние Минковского (Minkowski distance):**\n",
        "   $dist(x_i, x_j) = \\left(\\sum_{k=1}^{d} |x_{ik} - x_{jk}|^p\\right)^{1/p}$\n",
        "   Евклидово и Манхэттенское расстояния являются частными случаями расстояния Минковского при $p=2$ и $p=1$ соответственно.\n",
        "\n",
        "* **Косинусное расстояние (Cosine distance):**\n",
        "   $dist(x_i, x_j) = 1 - \\frac{x_i \\cdot x_j}{\\|x_i\\| \\|x_j\\|}$\n",
        "   где $\\cdot$ обозначает скалярное произведение, а $\\|x\\|$ - норму вектора. Косинусное расстояние часто используется для данных высокой размерности, таких как текстовые данные.\n",
        "\n",
        "Выбор конкретной метрики расстояния зависит от природы данных и задачи кластеризации.\n",
        "\n",
        "**3. Параметр близости (ε):**\n",
        "\n",
        "Задается пороговое значение $\\epsilon > 0$, определяющее максимальное расстояние, при котором две точки считаются соседями.\n",
        "\n",
        "**4. Множество ε-соседей:**\n",
        "\n",
        "Для каждой точки $x_i \\in X$ определяется множество ее ε-соседей $N_\\epsilon(x_i)$:\n",
        "$N_\\epsilon(x_i) = \\{x_j \\in X \\mid dist(x_i, x_j) \\leq \\epsilon\\}$\n",
        "\n",
        "**5. Критерий кластеризации:**\n",
        "\n",
        "Две точки $x_i$ и $x_j$ принадлежат к одному и тому же кластеру, если существует последовательность точек $p_1, p_2, ..., p_k$, где $p_1 = x_i$, $p_k = x_j$, и каждая точка $p_{i+1}$ является ε-соседом точки $p_i$. Это означает, что точки связаны отношением ε-соседства.\n",
        "\n",
        "**6. Формализация кластера:**\n",
        "\n",
        "Кластер $C$ представляет собой максимальное множество связанных точек, где каждая пара точек в $C$ связана через цепочку ε-соседей. Точки, не имеющие ε-соседей, не принадлежат ни к одному кластеру и считаются шумом.\n",
        "\n",
        "### Детальное пояснение\n",
        "\n",
        "**Параметр ε:**  Выбор параметра ε критически важен. Слишком маленькое значение приведет к тому, что многие точки будут считаться шумом, и кластеры будут фрагментированы. Слишком большое значение объединит в один кластер точки, которые на самом деле могут принадлежать к разным группам. Часто для определения оптимального значения ε используют анализ распределения расстояний до k-го ближайшего соседа для каждой точки.\n",
        "\n",
        "**Формирование кластеров как поиск связных компонент:**  Процесс формирования кластеров можно рассматривать как поиск связных компонент в графе, где вершины - это точки данных, а ребра соединяют точки, находящиеся на расстоянии не более ε друг от друга.\n",
        "\n",
        "**Шум:**  Точки, не имеющие ε-соседей, не формируют кластеры и рассматриваются как шум или выбросы. Это является одним из преимуществ метода, так как он способен идентифицировать аномалии.\n",
        "\n",
        "**Преимущества:**\n",
        "\n",
        "* **Не требует предварительного задания количества кластеров:**  Количество кластеров определяется структурой данных.\n",
        "* **Способен обнаруживать кластеры произвольной формы:**  В отличие от K-средних, которые стремятся к формированию сферических кластеров, метод ближайшего соседа может находить кластеры сложной геометрической формы.\n",
        "* **Устойчив к выбросам:**  Выбросы, как правило, не имеют плотных окрестностей и идентифицируются как шум.\n",
        "* **Прост в реализации и понимании.**\n",
        "\n",
        "**Недостатки:**\n",
        "\n",
        "* **Чувствительность к параметру ε:**  Выбор оптимального значения ε может быть сложной задачей и сильно влияет на результат кластеризации.\n",
        "* **Проблемы с кластерами разной плотности:**  Если в данных присутствуют кластеры с существенно различающейся плотностью, выбор единого значения ε может привести к тому, что плотные кластеры будут разбиты, а разреженные - объединены.\n",
        "* **Вычислительная сложность:**  Для каждой точки необходимо найти ее соседей, что в наивном подходе требует $O(n^2)$ операций, где $n$ - количество точек. Использование пространственных индексов (например, KD-деревьев или Ball-деревьев) может снизить сложность, но эффективно только для данных невысокой размерности.\n",
        "\n",
        "### Практические соображения\n",
        "\n",
        "* **Выбор метрики расстояния:**  Важно выбрать метрику расстояния, которая соответствует природе данных и задаче кластеризации.\n",
        "* **Определение оптимального ε:**  Можно использовать эвристические методы, такие как анализ графика расстояний до k-го ближайшего соседа, или проводить эксперименты с различными значениями ε.\n",
        "* **Масштабирование признаков:**  Если признаки имеют разные масштабы, рекомендуется их нормализовать или стандартизировать перед применением алгоритма, чтобы избежать доминирования признаков с большими значениями.\n",
        "* **Применение для больших наборов данных:**  Для больших наборов данных необходимо использовать эффективные структуры данных для поиска ближайших соседей, такие как KD-деревья или Ball-деревья (хотя их эффективность снижается с ростом размерности). Для очень больших наборов данных могут потребоваться более продвинутые методы, такие как DBSCAN (Density-Based Spatial Clustering of Applications with Noise), который является развитием идеи кластеризации ближайшего соседа и более устойчив к проблемам с разной плотностью кластеров.\n",
        "\n",
        "В заключение, модель кластерного анализа ближайшего соседа является мощным и интуитивно понятным методом кластеризации, особенно полезным для обнаружения кластеров произвольной формы и идентификации выбросов. Однако, правильный выбор параметра ε и учет особенностей данных являются ключевыми для успешного применения этого алгоритма."
      ],
      "metadata": {
        "id": "24kBfHNJUoy6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **Вопрос 44: Модели кластерного анализа (дальнего соседа).**\n",
        "\n",
        "## Модели кластерного анализа (дальнего соседа) или Иерархическая кластеризация с полной связью\n",
        "\n",
        "Иерархическая кластеризация с полной связью является агломеративным (восходящим) методом кластеризации. Это означает, что алгоритм начинает с того, что каждая точка данных рассматривается как отдельный кластер, а затем последовательно объединяет наиболее близкие кластеры до тех пор, пока не будет достигнут критерий остановки. В случае полной связи, \"близость\" между кластерами определяется как максимальное расстояние между любой парой точек, где каждая точка принадлежит одному из кластеров.\n",
        "\n",
        "### Алгоритм работы\n",
        "\n",
        "Алгоритм иерархической кластеризации с полной связью работает следующим образом:\n",
        "\n",
        "1. **Начало:** Каждая точка данных рассматривается как отдельный кластер. Если у вас есть $N$ точек данных, на начальном этапе у вас будет $N$ кластеров, каждый размером 1.\n",
        "\n",
        "2. **Вычисление матрицы расстояний:** Вычисляется матрица попарных расстояний между всеми точками данных. Выбор метрики расстояния (например, евклидово, манхэттенское, косинусное) зависит от природы данных.\n",
        "\n",
        "3. **Итеративное объединение:**\n",
        "   - На каждой итерации алгоритм находит два ближайших кластера и объединяет их в один новый кластер.\n",
        "   - Расстояние между двумя кластерами $C_i$ и $C_j$ при использовании полной связи определяется как:\n",
        "     $D(C_i, C_j) = \\max_{x \\in C_i, y \\in C_j} dist(x, y)$\n",
        "     где $dist(x, y)$ - расстояние между точками $x$ и $y$.\n",
        "\n",
        "4. **Обновление матрицы расстояний:** После объединения двух кластеров необходимо обновить матрицу расстояний. Расстояние от нового кластера до всех остальных существующих кластеров вычисляется с использованием критерия полной связи.\n",
        "\n",
        "5. **Критерий остановки:** Процесс объединения продолжается до тех пор, пока не будет достигнут критерий остановки. Возможные критерии:\n",
        "   - Достижение желаемого количества кластеров.\n",
        "   - Достижение определенного порога расстояния между кластерами.\n",
        "   - Объединение всех точек в один кластер (формирование полной иерархии).\n",
        "\n",
        "6. **Дендрограмма:** Результаты иерархической кластеризации часто визуализируются с помощью дендрограммы, которая показывает последовательность объединений кластеров и расстояния, при которых происходили эти объединения.\n",
        "\n",
        "### Математическая формализация\n",
        "\n",
        "Пусть задано множество точек данных $X = \\{x_1, x_2, ..., x_n\\}$.\n",
        "\n",
        "**1. Матрица расстояний:**\n",
        "\n",
        "Вычисляется матрица попарных расстояний $D$, где $D_{ij} = dist(x_i, x_j)$.\n",
        "\n",
        "**2. Расстояние между кластерами (полная связь):**\n",
        "\n",
        "Для двух кластеров $C_i$ и $C_j$, расстояние между ними $D(C_i, C_j)$ определяется как максимальное расстояние между любой парой точек, принадлежащих разным кластерам:\n",
        "\n",
        "$D(C_i, C_j) = \\max \\{ dist(x, y) \\mid x \\in C_i, y \\in C_j \\}$\n",
        "\n",
        "**3. Алгоритм объединения:**\n",
        "\n",
        "На каждой итерации находятся два кластера $C_p$ и $C_q$, для которых расстояние $D(C_p, C_q)$ минимально среди всех пар кластеров. Эти два кластера объединяются в новый кластер.\n",
        "\n",
        "**4. Обновление расстояний:**\n",
        "\n",
        "Если кластеры $C_p$ и $C_q$ объединены в новый кластер $C_{pq} = C_p \\cup C_q$, расстояние от нового кластера $C_{pq}$ до любого другого кластера $C_k$ вычисляется как:\n",
        "\n",
        "$D(C_{pq}, C_k) = \\max \\{ D(C_p, C_k), D(C_q, C_k) \\}$\n",
        "\n",
        "Это следует из определения полной связи: максимальное расстояние между точками в $C_{pq}$ и $C_k$ будет либо максимальным расстоянием между точками в $C_p$ и $C_k$, либо максимальным расстоянием между точками в $C_q$ и $C_k$.\n",
        "\n",
        "### Пример работы\n",
        "\n",
        "Предположим, у нас есть четыре точки данных: A, B, C, D с попарными расстояниями:\n",
        "\n",
        "|       | A   | B   | C   | D   |\n",
        "| :---- | :-- | :-- | :-- | :-- |\n",
        "| **A** | 0   | 3   | 6   | 7   |\n",
        "| **B** | 3   | 0   | 5   | 6   |\n",
        "| **C** | 6   | 5   | 0   | 2   |\n",
        "| **D** | 7   | 6   | 2   | 0   |\n",
        "\n",
        "**Шаг 1:** Начальные кластеры: {A}, {B}, {C}, {D}.\n",
        "\n",
        "**Шаг 2:** Находим пару кластеров с минимальным расстоянием. Это C и D с расстоянием 2. Объединяем их в кластер {C, D}.\n",
        "\n",
        "**Шаг 3:** Обновляем матрицу расстояний, используя полную связь:\n",
        "   - $D(\\{A\\}, \\{C, D\\}) = \\max(dist(A, C), dist(A, D)) = \\max(6, 7) = 7$\n",
        "   - $D(\\{B\\}, \\{C, D\\}) = \\max(dist(B, C), dist(B, D)) = \\max(5, 6) = 6$\n",
        "\n",
        "Новая матрица расстояний между кластерами:\n",
        "\n",
        "|           | {A} | {B} | {C, D} |\n",
        "| :-------- | :-- | :-- | :------- |\n",
        "| **{A}**   | 0   | 3   | 7        |\n",
        "| **{B}**   | 3   | 0   | 6        |\n",
        "| **{C, D}** | 7   | 6   | 0        |\n",
        "\n",
        "**Шаг 4:** Находим пару кластеров с минимальным расстоянием. Это {A} и {B} с расстоянием 3. Объединяем их в кластер {A, B}.\n",
        "\n",
        "**Шаг 5:** Обновляем матрицу расстояний:\n",
        "   - $D(\\{A, B\\}, \\{C, D\\}) = \\max(D(\\{A\\}, \\{C, D\\}), D(\\{B\\}, \\{C, D\\})) = \\max(7, 6) = 7$\n",
        "\n",
        "Новая матрица расстояний между кластерами:\n",
        "\n",
        "|             | {A, B} | {C, D} |\n",
        "| :---------- | :----- | :------- |\n",
        "| **{A, B}**  | 0      | 7        |\n",
        "| **{C, D}**  | 7      | 0        |\n",
        "\n",
        "**Шаг 6:** Находим пару кластеров с минимальным расстоянием. Это {A, B} и {C, D} с расстоянием 7. Объединяем их в кластер {A, B, C, D}.\n",
        "\n",
        "Дендрограмма будет отражать эту последовательность объединений.\n",
        "\n",
        "### Преимущества и недостатки\n",
        "\n",
        "**Преимущества:**\n",
        "\n",
        "* **Тенденция к формированию компактных кластеров:** Полная связь стремится минимизировать максимальное расстояние между точками в разных кластерах, что приводит к формированию более плотных и компактных кластеров.\n",
        "* **Менее подвержена \"эффекту цепи\":** В отличие от односвязной кластеризации, полная связь менее склонна к объединению кластеров через отдельные \"мосты\" из близко расположенных точек.\n",
        "* **Детерминированный алгоритм:** При заданных данных и метрике расстояния результат всегда будет одинаковым.\n",
        "* **Предоставляет иерархическую структуру:** Дендрограмма позволяет анализировать структуру данных на разных уровнях гранулярности.\n",
        "\n",
        "**Недостатки:**\n",
        "\n",
        "* **Чувствительность к выбросам:** Выбросы могут сильно влиять на расстояние между кластерами, поскольку расстояние определяется максимальной парой.\n",
        "* **Может разбивать естественные кластеры:** Если кластеры имеют \"рыхлую\" структуру или соединены тонкими перемычками, полная связь может их разделить.\n",
        "* **Вычислительная сложность:**  Наивная реализация требует $O(N^3)$ времени, где $N$ - количество точек данных. Использование оптимизированных подходов может снизить сложность до $O(N^2 \\log N)$.\n",
        "* **Сложность выбора количества кластеров:**  Хотя дендрограмма помогает визуализировать процесс, выбор оптимального количества кластеров остается субъективным.\n",
        "\n",
        "### Сравнение с другими методами иерархической кластеризации\n",
        "\n",
        "* **Односвязная кластеризация (Single-linkage):** Расстояние между кластерами определяется как минимальное расстояние между любой парой точек из разных кластеров. Склонна к \"эффекту цепи\" и формированию вытянутых кластеров.\n",
        "* **Средневзвешенная кластеризация (Average-linkage):** Расстояние между кластерами определяется как среднее расстояние между всеми парами точек из разных кластеров. Представляет собой компромисс между полной и одиночной связью.\n",
        "\n",
        "Выбор метода связывания зависит от структуры данных и целей анализа. Полная связь подходит для выявления компактных, хорошо разделенных кластеров.\n",
        "\n",
        "### Практические соображения\n",
        "\n",
        "* **Выбор метрики расстояния:**  Критически важен и зависит от типа данных и задачи.\n",
        "* **Масштабирование признаков:**  Рекомендуется масштабировать признаки, чтобы избежать доминирования признаков с большими значениями.\n",
        "* **Интерпретация дендрограммы:**  Высота слияния на дендрограмме показывает расстояние между объединяемыми кластерами. Большие скачки в высоте могут указывать на хорошее разделение между кластерами.\n",
        "* **Выбор количества кластеров:**  Можно использовать различные эвристические методы для определения оптимального количества кластеров на основе дендрограммы (например, \"правило локтя\").\n",
        "* **Применение для больших наборов данных:**  Для больших наборов данных вычислительная сложность может быть проблемой. Рассмотрите возможность использования более эффективных алгоритмов или методов сэмплирования.\n",
        "\n",
        "В заключение, иерархическая кластеризация с полной связью является мощным инструментом для анализа структуры данных и выявления компактных кластеров. Понимание ее принципов работы, преимуществ и ограничений позволяет эффективно применять этот метод в различных областях."
      ],
      "metadata": {
        "id": "wVtUELfgX8kO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **Вопрос 45: Модели машинного обучения. Основные алгоритмы.**\n",
        "\n",
        "**Модели машинного обучения: Основные алгоритмы**\n",
        "\n",
        "Модель машинного обучения – это математическое представление закономерностей в данных, которое позволяет делать прогнозы или принимать решения без явного программирования. Вместо того чтобы задавать конкретные правила, модели обучаются на данных, выявляя скрытые зависимости и паттерны.\n",
        "\n",
        "Основная цель машинного обучения – научить компьютер выполнять задачи, для которых он не был явно запрограммирован. Это достигается путем предоставления алгоритму обучающих данных, на основе которых он строит свою модель.\n",
        "\n",
        "Модели машинного обучения можно классифицировать по различным критериям, но наиболее распространенным является разделение на основе типа обучения:\n",
        "\n",
        "1. **Обучение с учителем (Supervised Learning):** Модель обучается на размеченных данных, где для каждого входного примера известен правильный выходной результат (метка). Цель – научиться сопоставлять входные данные с выходными.\n",
        "2. **Обучение без учителя (Unsupervised Learning):** Модель обучается на неразмеченных данных, где нет информации о правильных ответах. Цель – выявить скрытые структуры, закономерности или группировки в данных.\n",
        "3. **Обучение с подкреплением (Reinforcement Learning):** Модель обучается путем взаимодействия с окружающей средой. Она получает награды или штрафы за свои действия и стремится выработать оптимальную стратегию поведения для максимизации общей награды.\n",
        "\n",
        "Рассмотрим основные алгоритмы, относящиеся к каждой из этих категорий.\n",
        "\n",
        "**1. Обучение с учителем (Supervised Learning)**\n",
        "\n",
        "В задачах обучения с учителем целью является построение модели, которая может предсказывать значения выходной переменной на основе входных переменных. Задачи обучения с учителем делятся на два основных типа:\n",
        "\n",
        "*   **Регрессия:** Прогнозирование непрерывной числовой переменной.\n",
        "*   **Классификация:** Прогнозирование дискретной категориальной переменной.\n",
        "\n",
        "**Основные алгоритмы регрессии:**\n",
        "\n",
        "*   **Линейная регрессия (Linear Regression):** Один из самых простых и фундаментальных алгоритмов. Предполагает линейную зависимость между входными переменными и выходной переменной. Модель стремится найти наилучшую прямую (или гиперплоскость в многомерном пространстве), которая минимизирует сумму квадратов разностей между предсказанными и фактическими значениями.\n",
        "    *   **Принцип работы:** Поиск коэффициентов линейного уравнения (y = b0 + b1\\*x1 + b2\\*x2 + ... + bn\\*xn), которые наилучшим образом соответствуют данным.\n",
        "    *   **Применение:** Прогнозирование цен на недвижимость, спроса на товары, температуры и т.д.\n",
        "*   **Полиномиальная регрессия (Polynomial Regression):** Расширение линейной регрессии, позволяющее моделировать нелинейные зависимости путем добавления полиномиальных признаков (например, x^2, x^3).\n",
        "    *   **Принцип работы:** Поиск коэффициентов полиномиального уравнения (y = b0 + b1\\*x + b2\\*x^2 + ... + bn\\*x^n).\n",
        "    *   **Применение:** Моделирование роста, распространения заболеваний, зависимости урожайности от количества удобрений.\n",
        "*   **Метод опорных векторов для регрессии (Support Vector Regression, SVR):**  Использует принципы метода опорных векторов для решения задач регрессии. Вместо минимизации ошибки, как в линейной регрессии, SVR стремится найти функцию, которая лежит в пределах заданного \"коридора\" вокруг фактических значений.\n",
        "    *   **Принцип работы:** Поиск оптимальной гиперплоскости, которая содержит как можно больше точек данных внутри заданного \"эпсилон-трубки\".\n",
        "    *   **Применение:** Прогнозирование временных рядов, финансовое прогнозирование.\n",
        "*   **Решающие деревья для регрессии (Decision Tree Regression):** Строят дерево решений, где каждый внутренний узел представляет собой проверку на значение одного из признаков, каждая ветвь представляет собой результат проверки, а каждый лист представляет собой предсказанное значение.\n",
        "    *   **Принцип работы:** Рекурсивное разделение пространства признаков на области, внутри которых значения целевой переменной относительно однородны.\n",
        "    *   **Применение:** Прогнозирование цен, оценка рисков.\n",
        "*   **Случайный лес для регрессии (Random Forest Regression):** Ансамблевый метод, который строит множество решающих деревьев на различных подмножествах данных и признаков, а затем усредняет их прогнозы. Это помогает уменьшить переобучение и повысить точность.\n",
        "    *   **Принцип работы:**  Создание множества независимых решающих деревьев и усреднение их предсказаний.\n",
        "    *   **Применение:** Прогнозирование в сложных и зашумленных данных, где отдельные деревья могут быть нестабильными.\n",
        "\n",
        "**Основные алгоритмы классификации:**\n",
        "\n",
        "*   **Логистическая регрессия (Logistic Regression):** Несмотря на наличие слова \"регрессия\" в названии, это алгоритм классификации. Используется для прогнозирования вероятности принадлежности объекта к определенному классу (обычно бинарная классификация).\n",
        "    *   **Принцип работы:** Применяет сигмоидную функцию к линейной комбинации входных признаков, чтобы получить вероятность принадлежности к классу.\n",
        "    *   **Применение:** Определение спама, диагностика заболеваний, кредитный скоринг.\n",
        "*   **Метод k-ближайших соседей (K-Nearest Neighbors, KNN):**  Простой алгоритм, который классифицирует новый объект на основе класса большинства из его k ближайших соседей в пространстве признаков.\n",
        "    *   **Принцип работы:** Поиск k ближайших объектов в обучающем наборе данных и присвоение новому объекту класса, который наиболее часто встречается среди его соседей.\n",
        "    *   **Применение:** Рекомендательные системы, распознавание образов.\n",
        "*   **Метод опорных векторов для классификации (Support Vector Machines, SVM):**  Ищет оптимальную гиперплоскость, которая наилучшим образом разделяет объекты разных классов. Стремится максимизировать \"зазор\" между классами. Может использовать \"ядерные трюки\" для работы с нелинейно разделимыми данными.\n",
        "    *   **Принцип работы:** Поиск гиперплоскости с максимальным отступом от ближайших точек каждого класса (опорных векторов).\n",
        "    *   **Применение:** Классификация изображений, текстовая классификация, биоинформатика.\n",
        "*   **Наивный байесовский классификатор (Naive Bayes):**  Основан на теореме Байеса и предполагает, что признаки являются условно независимыми друг от друга при заданном классе. Прост в реализации и эффективен для задач с высокой размерностью.\n",
        "    *   **Принцип работы:** Вычисление вероятности принадлежности объекта к каждому классу на основе теоремы Байеса и выбора класса с наибольшей вероятностью.\n",
        "    *   **Применение:** Фильтрация спама, классификация текстов.\n",
        "*   **Решающие деревья для классификации (Decision Tree Classification):** Аналогично регрессионным деревьям, но предсказывают класс, а не числовое значение.\n",
        "    *   **Принцип работы:** Рекурсивное разделение пространства признаков на области, внутри которых объекты принадлежат к одному и тому же классу.\n",
        "    *   **Применение:** Классификация клиентов, диагностика заболеваний.\n",
        "*   **Случайный лес для классификации (Random Forest Classification):** Ансамблевый метод, аналогичный регрессионному случайному лесу, но предназначен для задач классификации.\n",
        "    *   **Принцип работы:** Создание множества независимых решающих деревьев и выбор класса, за который проголосовало большинство деревьев.\n",
        "    *   **Применение:** Классификация изображений, распознавание объектов.\n",
        "\n",
        "**2. Обучение без учителя (Unsupervised Learning)**\n",
        "\n",
        "В задачах обучения без учителя целью является выявление скрытых закономерностей и структур в неразмеченных данных.\n",
        "\n",
        "**Основные алгоритмы кластеризации:**\n",
        "\n",
        "*   **Метод k-средних (K-Means):**  Разбивает данные на k кластеров, где каждый объект относится к кластеру с ближайшим средним значением (центроидом).\n",
        "    *   **Принцип работы:** Итеративное назначение объектов кластерам и пересчет центроидов до сходимости.\n",
        "    *   **Применение:** Сегментация клиентов, анализ изображений, обнаружение аномалий.\n",
        "*   **Иерархическая кластеризация (Hierarchical Clustering):**  Строит иерархию кластеров. Может быть агломеративной (снизу вверх) или дивизивной (сверху вниз).\n",
        "    *   **Принцип работы:**  Последовательное объединение или разделение кластеров на основе меры близости.\n",
        "    *   **Применение:** Биология (классификация видов), анализ социальных сетей.\n",
        "*   **DBSCAN (Density-Based Spatial Clustering of Applications with Noise):**  Основан на плотности данных. Группирует вместе точки, которые плотно расположены, отмечая выбросы как шум.\n",
        "    *   **Принцип работы:**  Определение основных точек (с достаточным количеством соседей в заданном радиусе) и расширение кластеров от этих точек.\n",
        "    *   **Применение:** Обнаружение аномалий, пространственный анализ данных.\n",
        "\n",
        "**Основные алгоритмы снижения размерности:**\n",
        "\n",
        "*   **Метод главных компонент (Principal Component Analysis, PCA):**  Линейное преобразование, которое проецирует данные на новое пространство меньшей размерности, сохраняя при этом наибольшую дисперсию.\n",
        "    *   **Принцип работы:**  Нахождение главных компонент (направлений с наибольшей дисперсией) и проецирование данных на эти компоненты.\n",
        "    *   **Применение:** Визуализация данных, сжатие данных, уменьшение шума.\n",
        "*   **t-SNE (t-distributed Stochastic Neighbor Embedding):**  Нелинейный метод снижения размерности, особенно хорошо подходящий для визуализации многомерных данных в 2D или 3D пространстве.\n",
        "    *   **Принцип работы:**  Сохранение локальной структуры данных при проецировании в пространство меньшей размерности.\n",
        "    *   **Применение:** Визуализация кластеров в данных высокой размерности.\n",
        "\n",
        "**Основные алгоритмы поиска ассоциативных правил:**\n",
        "\n",
        "*   **Apriori:**  Находит часто встречающиеся наборы элементов в наборе данных и на их основе генерирует ассоциативные правила.\n",
        "    *   **Принцип работы:**  Итеративный поиск часто встречающихся наборов элементов, начиная с отдельных элементов и постепенно увеличивая их размер.\n",
        "    *   **Применение:** Анализ корзины покупок, рекомендательные системы.\n",
        "*   **Eclat (Equivalence Class Transformation):**  Альтернативный алгоритм поиска ассоциативных правил, который часто оказывается более эффективным, чем Apriori, особенно для больших наборов данных.\n",
        "    *   **Принцип работы:**  Использует вертикальное представление данных и пересечение наборов транзакций для поиска часто встречающихся наборов элементов.\n",
        "    *   **Применение:** Анализ корзины покупок.\n",
        "\n",
        "**3. Обучение с подкреплением (Reinforcement Learning)**\n",
        "\n",
        "В задачах обучения с подкреплением агент учится принимать решения, взаимодействуя с окружающей средой. Он получает обратную связь в виде наград или штрафов за свои действия и стремится выработать стратегию, максимизирующую общую награду.\n",
        "\n",
        "**Основные алгоритмы обучения с подкреплением:**\n",
        "\n",
        "*   **Q-обучение (Q-learning):**  Алгоритм обучения без модели, который учит функцию Q-значений, представляющую ожидаемую награду за выполнение определенного действия в определенном состоянии.\n",
        "    *   **Принцип работы:**  Итеративное обновление Q-значений на основе полученных наград и будущих ожидаемых наград.\n",
        "    *   **Применение:** Игры, робототехника.\n",
        "*   **SARSA (State-Action-Reward-State-Action):**  Алгоритм обучения без модели, похожий на Q-обучение, но использует политику, которой следует агент, для оценки будущих наград.\n",
        "    *   **Принцип работы:**  Обновление Q-значений на основе текущего состояния, действия, полученной награды, следующего состояния и следующего действия, которое будет выполнено в соответствии с текущей политикой.\n",
        "    *   **Применение:** Игры, робототехника.\n",
        "\n",
        "**Выбор модели и оценка качества**\n",
        "\n",
        "Выбор подходящей модели машинного обучения зависит от типа задачи, объема и характеристик данных. Важно также оценивать качество построенных моделей с помощью различных метрик, таких как точность, полнота, F1-мера для задач классификации и средняя квадратичная ошибка, средняя абсолютная ошибка для задач регрессии.\n",
        "\n",
        "**Заключение**\n",
        "\n",
        "Мир моделей машинного обучения обширен и постоянно развивается. Понимание основных алгоритмов и принципов их работы является фундаментом для успешного применения машинного обучения в различных областях. Выбор конкретного алгоритма зависит от поставленной задачи и характеристик доступных данных. Экспериментирование и глубокое понимание сильных и слабых сторон каждого алгоритма играют ключевую роль в достижении желаемых результатов."
      ],
      "metadata": {
        "id": "_-QDhEgGVlrE"
      }
    }
  ]
}