# Оглавление

**I. Введение в итерацию по политике в марковских процессах**
    *   **Определение итерации по политике** как метода поиска оптимальной стратегии
    *   Два основных шага: **оценка текущей политики и её улучшение**
    *   **Оценка политики**: вычисление функции ценности $V^\pi(s)$
        *   Формула функции ценности $V^\pi(s) = \mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^t r_t | s_0 = s, \pi\right]$
        *   Значение коэффициента дисконтирования $\gamma$
    *   **Улучшение политики**: использование Q-функции $Q^\pi(s, a)$ для выбора действий
        *   Формула Q-функции $Q^\pi(s, a) = \mathbb{E}\left[r + \gamma V^\pi(s') | s, a\right]$
        *   Выбор действия, максимизирующего Q-функцию: $\pi(s) = \arg\max_a Q^\pi(s, a)$
    *   **Физический и геометрический смысл** итерации по политике на примере обучения робота

**II. Улучшение политики и сходимость**
    *   Процесс улучшения политики как **тестирование и выбор наиболее эффективных стратегий**
    *   **Пример улучшения политики**: изменение стратегии робота на основе опыта
    *   **Теорема об улучшении**: новая политика, лучше старой в каком-то состоянии, будет лучше в целом
    *   **Сходимость** алгоритма к наилучшему способу действий
    *   Математическая формализация функции ценности $V^\pi(s)$ и Q-функции $Q^\pi(s, a)$
    *   **Физический и геометрический смысл** улучшения политики и сходимости на примере робота-пылесоса

**III. Реализация алгоритма обучения в лабиринте**
    *   Основные шаги: определение действий, наград, начальной политики и оценка состояния
    *   **Определение действий**: вверх, вниз, влево, вправо и их координаты
    *   **Установление наград**: пример штрафа в клетке (1, 1)
    *   **Коэффициент важности будущих наград** ($\gamma$)
    *   **Создание начальной политики** (случайным образом) и матрицы ценностей (инициализация нулями)
    *   **Проверка допустимости состояния**: учет границ лабиринта
    *   **Оценка и улучшение политики** на основе полученных наград
    *   Математическая формализация награды за состояние $R(s)$ и обновления ценности состояния $V(s) = R(s) + \gamma \max_a Q(s, a)$
    *   **Физический и геометрический смысл** обучения в лабиринте как ориентирование в новом месте

**IV. Визуализация обучения агента**
    *   Цель визуализации: лучше понять, как агент адаптирует свою стратегию
    *   **Структура лабиринта** 4x4 с целью в правом нижнем углу
    *   **Начальное состояние и политика**: случайная политика в начале
    *   Наблюдение за изменением ценности состояний и адаптацией стратегии
    *   **Оптимальный путь**: нахождение пути к цели, избегая препятствий
    *   Математическая формализация обновления ценности состояния
    *   **Физический и геометрический смысл** визуализации как аналогия ориентирования в новом пространстве

**V. Итерация по ценности как метод оптимизации**
    *   Отличие от итерации по политике: **оценка и улучшение в одном шаге**, отсутствие явного хранения политики
    *   **Основная идея**: оценка ценности каждой клетки в реальном времени
    *   **Алгоритм действий**: инициализация ценностей, проверка ходов, выбор хода с максимальной ценностью, обновление ценности текущей клетки
    *   **Уравнение Беллмана**: $V(s) = R(s) + \gamma \max_{a} V(s')$
    *   **Преимущества**: быстрая сходимость, меньшие требования к памяти, простота реализации
    *   **Недостатки**: отсутствие промежуточных политик, меньшая стабильность, чувствительность к параметрам
    *   **Физический и геометрический смысл** итерации по ценности как оценка различных маршрутов

**VI. Метод Монте-Карло в обучении с подкреплением**
    *   Использование опыта взаимодействия со средой для оценки ценности состояний или пар состояние-действие
    *   Полезен при отсутствии модели среды, обновление после завершения полного эпизода
    *   **Основная идея**: обучение на основе **полных эпизодов** взаимодействия
    *   **Алгоритм действий**: инициализация, генерация эпизодов, обновление оценок (только первое посещение состояния)
    *   **Обновление ценностей**: $V(s) = V(s) + \alpha \left( G - V(s) \right)$
        *   Значение коэффициента обучения $\alpha$
        *   Общая награда $G$
    *   **Физический и геометрический смысл** метода Монте-Карло на примере робота-пылесоса

**VII. Применение метода Монте-Карло для оптимизации политики**
    *   Использование для **оценки Q-функции** и нахождения оптимальной политики на основе опыта
    *   Позволяет агенту учиться на ошибках и успехах при неизвестных правилах
    *   **Основные компоненты**: оценка Q-функции, улучшение политики, исследование новых действий
    *   **Пример обучения**: робот, играющий в видеоигру или покер, запоминающий награды
    *   **Преимущества метода**: не требует знания всех правил заранее, гибкость и адаптивность
    *   Математическая формализация обновления Q-функции: $Q(s, a) = Q(s, a) + \alpha \left( G - Q(s, a) \right)$
    *   **Физический и геометрический смысл** метода Монте-Карло как обучение на опыте

**VIII. Реализация класса для лабиринта и управление агентом**
    *   Моделирование лабиринта и управление перемещением агента
    *   **Структура класса**: инициализация (размер, стартовая позиция, награды, штрафы), метод `reset`, метод `step`
    *   **Перемещение агента**: учет границ лабиринта
    *   **Награды**: примеры штрафа (-1 за клетку 8, -0.1 за шаг) и награды (+1 за клетку 9), завершение эпизода
    *   **Физический и геометрический смысл** реализации класса как обучение перемещению в пространстве

**IX. Обучение агента с использованием метода Монте-Карло**
    *   Оценка ценности действий на основе опыта при отсутствии заданных правил
    *   **Основные компоненты метода**: количество эпизодов, вероятность случайного действия, коэффициент дисконтирования ($\gamma$)
    *   **Обновление Q-значений**: инициализация нулями, запоминание наград, обновление после эпизода
        *   Формула: $Q(s, a) = Q(s, a) + \alpha \left( G - Q(s, a) \right)$
    *   **Генерация эпизодов**: выбор действий на основе политики, обновление позиции, получение награды, учет только первого посещения состояния
    *   **Физический и геометрический смысл** обучения как определение ценности участков на основе опыта

**X. Обратное распространение вознаграждения в методе Монте-Карло**
    *   Ключевая концепция: **обновление оценок на основе полученных наград после эпизода**
    *   **Дисконтированное вознаграждение**: учет будущих наград с помощью $\gamma$, накопление общей награды $G$
    *   **Обновление Q-значений**: формула $Q(s, a) = Q(s, a) + \alpha \left( G - Q(s, a) \right)$
    *   **Визуализация** оптимальной политики после обучения
    *   **Физический и геометрический смысл** как запоминание результатов различных подходов

**XI. Обучение агента в лабиринте с использованием Q-таблицы**
    *   Использование Q-таблицы для оценки действий и выбора оптимальной стратегии
    *   Шаги: инициализация среды, Q-таблицы, обучение, визуализация
    *   **Инициализация среды**: создание лабиринта, определение ловушек и целей, установка параметров (вероятность случайного действия, $\gamma$)
    *   **Q-таблица**: инициализация нулями, обновление на основе наград
    *   **Обучение агента**: прохождение множества эпизодов, выбор действий (случайные или с максимальным Q), обновление Q-значений
    *   **Визуализация** оптимальной политики
    *   Математическая формализация обновления Q-значений: $Q(s, a) = Q(s, a) + \alpha \left( R + \gamma \max_{a'} Q(s', a') - Q(s, a) \right)$
    *   **Физический и геометрический смысл** как выработка оптимальной стратегии перемещения

**XII. Обучение агента с частичным наблюдением**
    *   Принятие решений на основе **неполной информации** об окружении
    *   **Основная идея**: определение границ окружения и своего положения относительно объектов
    *   **Пример с роботом-пылесосом**: реакция на появление человека
    *   Использование **частичного наблюдения** для определения границ
    *   Математическая формализация обновления Q-значений
    *   **Физический и геометрический смысл** как адаптация к среде при неполной информации

**XIII. Заключение**
    *   Сравнение **итерации по политике и метода Монте-Карло**
    *   Общие принципы обучения с подкреплением и адаптации агента

# Введение

В этой лекции мы погрузимся в увлекательный мир **обучения с подкреплением**, области искусственного интеллекта, где агенты учатся принимать оптимальные решения, взаимодействуя с окружающей средой. В основе многих алгоритмов обучения с подкреплением лежат **марковские процессы принятия решений (MDP)**, которые предоставляют математическую основу для моделирования последовательного принятия решений в условиях неопределенности. Наша цель — изучить, как агенты могут разрабатывать стратегии, позволяющие им максимизировать получаемое вознаграждение с течением времени. Мы рассмотрим два основных подхода к решению этой задачи: **итерацию по политике** и **методы Монте-Карло**.

Первый из рассматриваемых нами подходов — **итерация по политике**. Этот алгоритм итеративно улучшает стратегию (политику) агента посредством двух ключевых этапов: **оценки политики** и её **улучшения**. На этапе оценки мы вычисляем **функцию ценности** для каждого состояния, определяя ожидаемую суммарную награду при следовании текущей политике. Затем, на этапе улучшения, агент использует **Q-функцию**, чтобы выбрать действия, которые максимизируют ожидаемую награду в каждом состоянии, тем самым обновляя свою политику. Теорема об улучшении политики гарантирует, что каждая новая итерация приводит к политике, которая как минимум не хуже предыдущей.

В отличие от итерации по политике, **методы Монте-Карло** представляют собой подход к обучению с подкреплением, который опирается на **непосредственный опыт взаимодействия с окружающей средой**. Вместо того чтобы строить модель среды, агент учится, генерируя полные эпизоды взаимодействия и обновляя свои оценки ценности состояний или пар состояние-действие на основе полученных в конце эпизода наград. Ключевой концепцией здесь является **обратное распространение вознаграждения**, когда полученные награды используются для корректировки оценок на всех шагах, предпринятых в эпизоде.

Для иллюстрации этих методов мы будем часто обращаться к задаче **обучения агента в лабиринте**. Представьте себе робота, который должен найти выход из лабиринта, избегая ловушек и собирая награды. Рассмотренные нами алгоритмы позволят агенту **адаптироваться к окружающей среде**, **улучшать свою стратегию** на основе получаемого опыта и в конечном итоге находить **оптимальный путь** к цели. Визуализация этого процесса поможет нам лучше понять, как происходит обучение и как агент принимает решения.

# Глассарий терминов

*   **Итерация по политике (Policy Iteration)**: Алгоритм, используемый для нахождения оптимальной стратегии в **марковских процессах принятия решений (MDP)**. Этот метод состоит из двух основных шагов: **оценки текущей политики** и её **улучшения**.

*   **Оценка политики (Policy Evaluation)**: Этап в итерации по политике, на котором вычисляется **функция ценности** для всех состояний. Функция ценности $V^\pi(s)$ для состояния $s$ определяется как ожидаемая суммарная награда при следовании политике $\pi$ из состояния $s$.

*   **Функция ценности $V^\pi(s)$ (Value Function)**: Ожидаемая суммарная награда, которую можно получить, следуя определенной **политике** $\pi$, начиная из состояния $s$.

*   **Улучшение политики (Policy Improvement)**: Этап в итерации по политике, на котором политика улучшается путем выбора действий, максимизирующих ожидаемую награду. Это делается с помощью **Q-функции** $Q^\pi(s, a)$.

*   **Q-функция $Q^\pi(s, a)$ (Q-function)**: Функция, показывающая ожидаемую награду при выполнении действия $a$ в состоянии $s$ и дальнейшем следовании политике $\pi$.

*   **Коэффициент дисконтирования $\gamma$ (Discount Factor)**: Параметр, который определяет, насколько важны будущие награды по сравнению с немедленными.

*   **Марковский процесс принятия решений (MDP) (Markov Decision Process)**: Математическая структура для моделирования принятия решений в ситуациях, где результаты частично случайны и зависят как от действий принимающего решения, так и от текущего состояния среды (определено в предыдущем диалоге).

*   **Политика $\pi$ (Policy)**: Стратегия, определяющая, какое действие агент будет предпринимать в каждом состоянии.

*   **Награда $r_t$ или $R(s)$ (Reward)**: Сигнал, получаемый агентом от окружающей среды после выполнения действия в определенном состоянии.

*   **Метод Монте-Карло (Monte Carlo Method)**: Подход в **обучении с подкреплением**, который оценивает ценность состояний или пар состояние-действие на основе опыта, полученного в результате случайных выборок — полных эпизодов взаимодействия с окружающей средой.

*   **Эпизод (Episode)**: Полная последовательность состояний, действий и наград от начального состояния до конечного состояния или момента времени.

*   **Общая награда $G$ (Return)**: Сумма дисконтированных наград, полученных агентом в течение эпизода после первого посещения определенного состояния.

*   **Коэффициент обучения $\alpha$ (Learning Rate)**: Параметр, определяющий, насколько сильно новая информация влияет на предыдущие оценки ценности или Q-значений.

*   **Обратное распространение вознаграждения (Backward Propagation of Reward)**: Процесс в методе Монте-Карло, при котором полученная в конце эпизода награда используется для обновления оценок ценности (или Q-значений) для всех посещенных состояний (или выполненных действий) в этом эпизоде.

*   **Q-таблица (Q-table)**: Таблица, используемая для хранения оценок **Q-функции** для всех возможных пар состояние-действие.

*   **Исследование (Exploration)**: Процесс выбора действий, которые не считаются текуще оптимальными, с целью получения новой информации об окружающей среде и потенциально нахождения лучших стратегий.

*   **Использование (Exploitation)**: Процесс выбора действий, которые на данный момент кажутся наилучшими на основе имеющихся знаний (например, значений в Q-таблице), с целью максимизации немедленной награды.

*   **Итерация по ценности (Value Iteration)**: Альтернативный метод поиска оптимальной стратегии, который напрямую итеративно обновляет **функцию ценности** состояний, основываясь на **уравнении Беллмана**, без явного представления политики.

*   **Уравнение Беллмана (Bellman Equation)**: Принцип оптимальности, выраженный математически, который связывает ценность состояния с ценностями его возможных последующих состояний и ожидаемыми наградами.

*   **Частичное наблюдение (Partial Observation)**: Ситуация, в которой агент не имеет полной информации о текущем состоянии окружающей среды и должен принимать решения на основе доступных наблюдений.


---

# Summarization for Text

## Chunk 1

### **Название фрагмента: Итерация по политике в марковских процессах**

## **Итерация по политике**

Итерация по политике — это алгоритм, используемый для нахождения оптимальной стратегии в марковских процессах принятия решений (MDP). Этот метод состоит из двух основных шагов: оценки текущей политики и её улучшения, которые выполняются итеративно до сходимости.

1. **Оценка политики**: На этом этапе мы вычисляем функцию ценности для всех состояний, что позволяет понять, насколько хороша текущая политика. Функция ценности $V^\pi(s)$ для состояния $s$ определяется как ожидаемая суммарная дисконтированная награда, которую можно получить, следуя политике $\pi$ из состояния $s$:

$$
V^\pi(s) = \mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^t r_t | s_0 = s, \pi\right]
$$

где:
- $r_t$ — награда в момент времени $t$,
- $\gamma \in [0,1)$ — коэффициент дисконтирования, который определяет, насколько важны будущие награды.

Для вычисления функции ценности используется уравнение Беллмана:

$$
V^\pi(s) = \sum_a \pi(a|s) \sum_{s'} P(s'|s,a)[r(s,a,s') + \gamma V^\pi(s')]
$$

где:
- $\pi(a|s)$ — вероятность выбора действия $a$ в состоянии $s$ согласно политике $\pi$,
- $P(s'|s,a)$ — вероятность перехода в состояние $s'$ из состояния $s$ при выполнении действия $a$,
- $r(s,a,s')$ — непосредственная награда при переходе из состояния $s$ в состояние $s'$ при выполнении действия $a$.

2. **Улучшение политики**: После оценки политики мы улучшаем её, выбирая действия, которые максимизируют ожидаемую награду. Это делается с помощью Q-функции $Q^\pi(s, a)$, которая показывает ожидаемую суммарную дисконтированную награду при выполнении действия $a$ в состоянии $s$ и дальнейшем следовании политике $\pi$:

$$
Q^\pi(s, a) = \sum_{s'} P(s'|s,a)[r(s,a,s') + \gamma V^\pi(s')]
$$

Таким образом, для каждого состояния $s$ мы обновляем политику, выбирая действие $a$, которое максимизирует Q-функцию:

$$
\pi'(s) = \arg\max_a Q^\pi(s, a)
$$

Если полученная политика $\pi'$ не отличается от предыдущей политики $\pi$, то алгоритм сходится, и мы нашли оптимальную политику $\pi^*$. В противном случае мы продолжаем итерации с новой политикой.

Важно отметить, что:
1. Для конечных MDP с дисконтированным критерием оптимальности итерация по политике гарантированно сходится к оптимальной политике за конечное число итераций.
2. Хотя в общем случае политика может быть стохастической $\pi(a|s)$, оптимальная политика для MDP с полной наблюдаемостью всегда может быть представлена в детерминированной форме.
3. Практическая реализация оценки политики часто выполняется итеративно до некоторого порога сходимости, поскольку точное решение уравнения Беллмана может быть вычислительно затратным.

### Пример кода

Ниже приведен пример кода, реализующего итерацию по политике для простого MDP:

```python
import numpy as np

def policy_evaluation(policy, value_function, rewards, transitions, gamma=0.9, theta=1e-6):
    """
    Description:
    --------------------
      Оценка политики.
    
    Args:
    --------------------
        policy: Текущая политика (массив вероятностей действий для каждого состояния).
        value_function: Функция ценности для каждого состояния.
        rewards: Награды для каждого состояния.
        transitions: Вероятности переходов между состояниями.
        gamma: Коэффициент дисконтирования.
        theta: Порог сходимости.
    
    Returns:
    --------------------
        Обновленная функция ценности.
    """
    while True:
        delta = 0                                           # Изменение в функции ценности
        for s in range(len(value_function)):
            v = value_function[s]                           # Сохраняем старое значение
            value_function[s] = sum(policy[s, a] * sum(transitions[s, a, s_next] * (rewards[s] + gamma * value_function[s_next])
                                                        for s_next in range(len(value_function)))
                                     for a in range(len(policy[s])))
            delta = max(delta, abs(v - value_function[s]))  # Обновляем максимальное изменение
        if delta < theta:                                   # Проверяем сходимость
            break
    return value_function

# Пример использования
policy = np.array([[0.5, 0.5], [0.5, 0.5]])                                   # Пример политики
value_function = np.zeros(2)                                                  # Инициализация функции ценности
rewards = np.array([1, 0])                                                    # Награды
transitions = np.array([[[0.8, 0.2], [0.1, 0.9]], [[0.2, 0.8], [0.9, 0.1]]])  # Вероятности переходов

updated_value_function = policy_evaluation(policy, value_function, rewards, transitions)
print("Обновленная функция ценности:", updated_value_function)
```

В этом коде функция `policy_evaluation` оценивает текущую политику, обновляя функцию ценности для каждого состояния до тех пор, пока изменения не станут меньше заданного порога $\theta$. 

### Физический и геометрический смысл

Представьте, что вы обучаете робота играть в видеоигру. Итерация по политике — это процесс, в котором робот сначала оценивает, насколько хороша его текущая стратегия (например, как часто он выигрывает), а затем улучшает её, выбирая более выгодные действия (например, избегая врагов и собирая монетки). Этот процесс можно сравнить с обучением: сначала мы изучаем материал, а затем применяем его на практике, улучшая свои навыки.

## Chunk 2

### **Название фрагмента: Улучшение политики и сходимость в марковских процессах**

**Предыдущий контекст:** В предыдущем чанке мы обсудили итерацию по политике как метод, который включает в себя оценку и улучшение стратегии, а также теорему об улучшении политики, которая гарантирует, что новая стратегия будет как минимум не хуже старой.

## **Улучшение политики и сходимость**

Улучшение политики — это процесс, в котором агент (например, робот) изменяет свои действия на основе полученного опыта, чтобы достичь лучших результатов. Этот процесс можно представить как постоянное тестирование различных стратегий и выбор наиболее эффективной.

1.  **Пример улучшения политики**: Представьте, что робот начинает с простого плана, например, всегда двигаться вправо. После нескольких игр он замечает, что справа часто находятся враги, и решает изменить свою стратегию: если справа враг, то он будет двигаться налево. Это изменение основано на оценке его предыдущих действий и их результативности.

2.  **Теорема об улучшении**: Эта теорема утверждает, что если новая политика в каком-то состоянии лучше старой, то в целом новая политика будет лучше. Например, если новая стратегия позволяет роботу лучше справляться с врагами, то его общая производительность в игре также улучшится.

3.  **Сходимость**: Важно отметить, что с течением времени агент будет находить наилучший способ действовать. Это похоже на решение пазла: пробуя различные варианты, вы в конечном итоге найдете правильное решение.

Рассмотрим основные этапы этого процесса на пример алгоритма [**rStar-Math**](https://telegra.ph/Revolyuciya-v-matematicheskom-myshlenii-malyh-yazykovyh-modelej-s-rStar-Math-01-27):

Метод **rStar-Math** демонстрирует процесс **улучшения политики** в контексте математического рассуждения для малых языковых моделей (SLM). В rStar-Math, **модель политики (policy SLM)** выступает в роли "агента", генерирующего последовательность шагов решения математической задачи. Процесс **саморазвития** в rStar-Math является примером **итеративного улучшения политики**.

*   В каждом раунде саморазвития, **policy SLM** генерирует множество траекторий рассуждений для большого набора математических задач с использованием **поиска по дереву Монте-Карло (MCTS)**. MCTS позволяет исследовать различные "стратегии" решения, разбивая сложные задачи на более простые шаги.
*   **Модель предпочтения процессов (PPM)** оценивает каждый сгенерированный шаг рассуждения на основе **Q-значений**, присвоенных MCTS. Эти Q-значения отражают вклад шага в получение правильного окончательного ответа. **PPM, таким образом, действует как механизм оценки "результативности" каждого "действия" (шага рассуждения) модели политики**.
*   На основе этой оценки, **policy SLM** и **PPM** **итеративно обучаются и совершенствуются**. Policy SLM учится генерировать более предпочтительные шаги, а PPM - лучше оценивать их.

Этот итеративный процесс соответствует **теореме об улучшении**. С каждым раундом саморазвития, **новая версия policy SLM становится лучше в генерации шагов, ведущих к правильным решениям**, что в целом улучшает ее способность решать математические задачи. Экспериментальные результаты rStar-Math показывают **значительное улучшение точности SLM** в математических рассуждениях на различных бенчмарках, таких как MATH и AIME. Например, rStar-Math значительно повысил точность модели Qwen2.5-Math-7B с 58.8% до 90.0% на бенчмарке MATH.

В конечном итоге, целью rStar-Math является **достижение высокой точности и надежности в математическом рассуждении**, что можно рассматривать как **сходимость** к оптимальной "политике" решения математических задач для SLM. Успех rStar-Math, демонстрирующий способность SLM превосходить даже более крупные модели, такие как OpenAI o1, подтверждает **эффективность этого итеративного процесса улучшения политики**. Важно отметить, что **PPM играет критически важную роль** в этом процессе улучшения, предоставляя точную обратную связь на уровне каждого шага рассуждения.

### Математическая формализация

Сходимость и улучшение политики можно формализовать с помощью следующих уравнений:

- Функция ценности $V^\pi(s)$ для состояния $s$:

$$
V^\pi(s) = \mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^t r_t | s_0 = s, \pi\right]
$$

где:
- $r_t$ — награда в момент времени $t$,
- $\gamma$ — коэффициент дисконтирования.

- Q-функция $Q^\pi(s, a)$:

$$
Q^\pi(s, a) = \mathbb{E}\left[r + \gamma V^\pi(s') | s, a\right]
$$

где $s'$ — следующее состояние после выполнения действия $a$.

### Физический и геометрический смысл

Представьте, что вы обучаете робота-пылесоса убирать комнату. Начальный план — двигаться случайно. После оценки своего маршрута робот замечает, что в углах больше всего пыли, и начинает чаще проверять эти места. Этот процесс повторяется, пока он не найдет оптимальный маршрут, который позволит ему собрать всю пыль. Таким образом, улучшение политики и сходимость помогают агенту адаптироваться к окружающей среде и достигать лучших результатов.

## Chunk 3

### **Название фрагмента: Итерация по ценности как метод оптимизации**

**Предыдущий контекст:** В предыдущем чанке мы обсуждали визуализацию обучения агента в лабиринте, где агент адаптирует свою стратегию для достижения цели, избегая препятствий и получая награды.

## **Итерация по ценности**

Итерация по ценности — это метод поиска оптимальной стратегии, который отличается от итерации по политике тем, что не требует явного хранения политики. Вместо этого, оценка и улучшение происходят в одном шаге, что позволяет достичь сходимости быстрее.

1.  **Основная идея**: Итерация по ценности позволяет агенту (например, роботу) оценивать ценность каждой клетки лабиринта в режиме реального времени. Чем ближе клетка к цели (например, сокровищу), тем выше её ценность. Если клетка содержит ловушку, её ценность будет отрицательной.

2.  **Алгоритм действий**:
    *   Робот начинает с присвоения всем клеткам одинаковой ценности (например, ноль), за исключением клетки с сокровищем, которая получает высокую ценность (например, +1).
    *   Затем он проверяет все возможные ходы из каждой клетки и вычисляет, насколько ценными будут следующие клетки, если он сделает тот или иной ход.
    *   Робот выбирает ход с максимальной ценностью и обновляет ценность текущей клетки.

3.  **Уравнение Беллмана**: Для вычисления ценности используется уравнение Беллмана, которое формализует оптимальность:

$$
V(s) = R(s) + \gamma \max_{a} V(s')
$$

где:
- $V(s)$ — ценность состояния $s$,
- $R(s)$ — награда за состояние $s$,
- $\gamma$ — коэффициент дисконтирования,
- $s'$ — следующее состояние после выполнения действия $a$.

### Преимущества и недостатки

- **Преимущества**:
    - Быстрая сходимость.
    - Меньшие требования к памяти.
    - Проще в реализации.

- **Недостатки**:
    - Отсутствие промежуточных политик.
    - Меньшая стабильность.
    - Чувствительность к выбору параметров.

Вот как можно рассмотреть связь между rStar-Math и итерацией по ценности:

*   **Оценка "ценности" шагов:** В rStar-Math, **поиск по дереву Монте-Карло (MCTS) присваивает Q-значения каждому шагу решения**. Эти Q-значения можно рассматривать как оценку "ценности" выполнения определенного шага в процессе решения задачи, поскольку они отражают вклад этого шага в получение правильного окончательного ответа.
*   **Модель предпочтения процессов (PPM) как функция ценности:** **PPM обучается предсказывать "предпочтительность" каждого шага рассуждения на основе этих Q-значений**. Таким образом, PPM можно интерпретировать как функцию, которая оценивает "ценность" текущего шага в контексте задачи и предыдущих шагов.
*   **Отличие от уравнения Беллмана:** Однако, в rStar-Math **нет явного применения уравнения Беллмана** в том виде, в котором оно используется в классической итерации по ценности. Q-значения обновляются на основе результатов симуляций MCTS, а PPM обучается на парах предпочтений, полученных из этих Q-значений. **Основной целью не является прямое вычисление оптимальной функции ценности для всех состояний, а обучение модели, способной генерировать и оценивать последовательности шагов рассуждения.**

### Физический и геометрический смысл

Итерация по ценности можно представить как процесс, в котором агент (например, робот) учится определять, насколько хороша каждая клетка лабиринта в режиме реального времени. Это похоже на то, как человек оценивает различные маршруты, выбирая наиболее безопасный и быстрый путь к цели. Чем ближе путь к цели и чем безопаснее он, тем выше ценность этого пути. Таким образом, итерация по ценности помогает агенту находить оптимальные решения в сложных ситуациях.

## Chunk 4

### **Название фрагмента: Метод Монте-Карло в обучении с подкреплением**

**Предыдущий контекст:** В предыдущем чанке мы обсудили итерацию по ценности как метод оптимизации, который позволяет агенту оценивать ценность каждой клетки лабиринта и адаптировать свою стратегию на основе полученных значений.

## **Метод Монте-Карло**

Метод Монте-Карло — это подход, используемый в обучении с подкреплением для оценки ценности состояний или пар состояние-действие на основе опыта, полученного в ходе взаимодействия с окружающей средой. Этот метод особенно полезен, когда нет необходимости в моделировании среды, и обновление оценок происходит только после завершения полного эпизода.

1. **Основная идея**: Метод Монте-Карло основывается на том, что агент учится на основе полных эпизодов взаимодействия с окружающей средой. Это означает, что агент выполняет действия, следуя своей текущей политике, до достижения терминального состояния, и только после завершения эпизода обновляет свои оценки.

2. **Разновидности метода Монте-Карло**:
   - **Метод первого посещения (first-visit MC)**: Учитывается только первое посещение состояния в эпизоде при обновлении его ценности.
   - **Метод каждого посещения (every-visit MC)**: Учитываются все посещения состояния в эпизоде, и ценность обновляется для каждого из них.

3. **Алгоритм действий**:
   - **Инициализация**: Все состояния и их ценности инициализируются произвольными значениями. Для каждого состояния создается пустой список для хранения ценностей.
   - **Генерация эпизодов**: Агент генерирует эпизоды, следуя текущей политике. Каждый эпизод представляет собой последовательность состояний, действий и наград, которые агент выполняет и получает.
   - **Расчет возврата**: Для каждого состояния в эпизоде рассчитывается возврат (G) — сумма дисконтированных наград, полученных после посещения этого состояния:
     
     $$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$
     
     где γ (гамма) — коэффициент дисконтирования (0 ≤ γ ≤ 1), определяющий значимость будущих наград.
     
   - **Обновление оценок**: После завершения эпизода агент обновляет ценности состояний или пар состояние-действие.

4. **Обновление ценностей состояний**: Обновление ценностей происходит по формуле:

   $$V(s) = V(s) + \alpha \left( G - V(s) \right)$$

   где:
   - $V(s)$ — текущая ценность состояния $s$,
   - $G$ — возврат, полученный после посещения состояния $s$,
   - $\alpha$ — коэффициент обучения, который определяет скорость обновления ценности.

5. **Обновление Q-функции**: Для обучения без модели среды часто используется оценка ценности пар состояние-действие (Q-функция):

   $$Q(s, a) = Q(s, a) + \alpha \left( G - Q(s, a) \right)$$

   где $Q(s, a)$ — текущая оценка ценности пары состояние-действие.

6. **Баланс исследования и эксплуатации**: Для эффективного обучения агент должен как исследовать новые действия (exploration), так и использовать уже полученные знания (exploitation). Распространенные стратегии включают:
   - **ε-жадная политика**: Агент выбирает действие с наибольшей оценкой Q с вероятностью (1-ε) и случайное действие с вероятностью ε.
   - **Смягченная максимизация (softmax)**: Вероятность выбора действия пропорциональна его ожидаемой ценности.

7. **Обучение стратегии**: На основе полученных Q-значений агент может обновлять свою стратегию:
   - **Жадная политика**: $\pi(s) = \arg\max_a Q(s, a)$
   - **Улучшение политики**: Постепенное смещение в сторону действий с высокими Q-значениями с сохранением возможности исследования.

8. **Сравнение с другими методами**:
   - **В отличие от динамического программирования**: Метод Монте-Карло не требует полной модели среды.
   - **В отличие от методов временной разности (TD-обучение)**: Метод Монте-Карло обновляет оценки только после завершения полного эпизода, а не после каждого шага, что может приводить к меньшему смещению, но большей дисперсии в оценках.

Метод Монте-Карло хорошо работает в средах с непродолжительными эпизодами и является фундаментальным подходом в обучении с подкреплением, особенно когда модель среды недоступна или слишком сложна для точного моделирования.

### Пример кода

Ниже приведен пример кода, который демонстрирует метод Монте-Карло в обучении с подкреплением:

```python
from typing import List, Tuple, Optional
import numpy as np

class MonteCarloAgent:
    """
    Description:
    ---------------
        Класс для реализации агента, обучающегося методом Монте-Карло.

    Args:
    ---------------
        size: Размер лабиринта (например, 4 для 4x4).
        epsilon: Параметр исследования (вероятность случайного действия).
        gamma: Коэффициент дисконтирования будущих наград.

    Attributes:
    ---------------
        size: Размер лабиринта.
        epsilon: Параметр исследования.
        gamma: Коэффициент дисконтирования.
        values: Инициализация ценностей состояний.
        returns: Список для хранения наград.
        policy: Случайная политика.
        dx: Направления перемещения по оси X.
        dy: Направления перемещения по оси Y.
        q_values: Q-значения для каждой пары состояние-действие.
        sa_count: Счетчик посещений пар состояние-действие.
    """

    def __init__(self, size: int, epsilon: float = 0.1, gamma: float = 0.9):
        self.size = size
        self.epsilon = epsilon
        self.gamma = gamma
        self.values = np.zeros((size, size))  # Инициализация ценностей
        self.returns = {(x, y): [] for x in range(size) for y in range(size)}  # Список для хранения наград
        self.policy = np.random.choice([0, 1, 2, 3], size=(size, size))        # Случайная политика

        # Направления перемещения: 0-вверх, 1-вправо, 2-вниз, 3-влево
        self.dx = [-1, 0, 1, 0]
        self.dy = [0, 1, 0, -1]

        # Q-значения для каждой пары состояние-действие
        self.q_values = np.zeros((size, size, 4))
        self.sa_count = np.zeros((size, size, 4))  # Счетчик посещений пар состояние-действие

    def is_valid_move(self, x: int, y: int) -> bool:
        """
        Description:
        ---------------
            Проверяет, допустимо ли перемещение в данную позицию.

        Args:
        ---------------
            x: Координата X.
            y: Координата Y.

        Returns:
        ---------------
            Булево значение, указывающее, допустимо ли перемещение.
        """
        return 0 <= x < self.size and 0 <= y < self.size

    def choose_action(self, state: Tuple[int, int]) -> int:
        """
        Description:
        ---------------
            Выбор действия согласно эпсилон-жадной стратегии.

        Args:
        ---------------
            state: Текущее состояние (кортеж координат).

        Returns:
        ---------------
            Выбранное действие (целое число).
        """
        x, y = state

        if np.random.rand() < self.epsilon:
            # Исследование: выбираем случайное действие
            return np.random.choice(4)
        else:
            # Эксплуатация: выбираем лучшее действие согласно политике
            return self.policy[x, y]

    def generate_episode(self, max_steps: int = 100) -> Tuple[List[Tuple[Tuple[int, int], int, int]], bool]:
        """
        Description:
        ---------------
            Генерация эпизода, следуя текущей политике.

        Args:
        ---------------
            max_steps: Максимальное количество шагов в эпизоде.

        Returns:
        ---------------
            episode: Список кортежей (состояние, действие, награда).
            success: Был ли эпизод успешным (достигнута цель).
        """
        episode = []
        state = (0, 0)  # Начальное состояние
        steps = 0

        while state != (self.size - 1, self.size - 1) and steps < max_steps:  # Пока не достигнута цель
            action = self.choose_action(state)                                # Выбор действия

            # Определяем новое состояние
            new_x = state[0] + self.dx[action]
            new_y = state[1] + self.dy[action]

            # Если ход допустим, перемещаемся
            if self.is_valid_move(new_x, new_y):
                new_state = (new_x, new_y)

                # Базовая награда: -1 за каждый шаг для стимулирования быстрого пути
                reward = -1

                # Бонус за достижение цели
                if new_state == (self.size - 1, self.size - 1):
                    reward = 10

                episode.append((state, action, reward))
                state = new_state
            else:
                # Штраф за попытку выйти за границы
                reward = -2
                episode.append((state, action, reward))

            steps += 1

        success = state == (self.size - 1, self.size - 1)
        return episode, success

    def update_values(self, episode: List[Tuple[Tuple[int, int], int, int]]) -> None:
        """
        Description:
        ---------------
            Обновление ценностей состояний на основе эпизода.

        Args:
        ---------------
            episode: Список кортежей (состояние, действие, награда).
        """
        G = 0  # Накопленная награда
        visited_sa_pairs = set()  # Для отслеживания первого посещения

        # Проходим эпизод в обратном порядке
        for t in range(len(episode) - 1, -1, -1):
            state, action, reward = episode[t]
            x, y = state

            # Накопление награды с учетом дисконтирования
            G = reward + self.gamma * G

            # Обновляем только при первом посещении пары состояние-действие
            sa_pair = (x, y, action)
            if sa_pair not in visited_sa_pairs:
                visited_sa_pairs.add(sa_pair)

                # Обновляем счетчик посещений и Q-значение
                self.sa_count[x, y, action] += 1
                alpha = 1.0 / self.sa_count[x, y, action]  # Размер шага
                self.q_values[x, y, action] += alpha * (G - self.q_values[x, y, action])

                # Обновляем политику (жадная стратегия)
                self.policy[x, y] = np.argmax(self.q_values[x, y])

                # Обновляем значение состояния
                self.values[x, y] = np.max(self.q_values[x, y])

                # Сохраняем возврат для данного состояния
                self.returns[(x, y)].append(G)

    def train(self, num_episodes: int = 1000) -> None:
        """
        Description:
        ---------------
            Обучение агента.

        Args:
        ---------------
            num_episodes: Количество эпизодов для обучения.
        """
        successful_episodes = 0

        for i in range(num_episodes):
            episode, success = self.generate_episode()
            if success:
                successful_episodes += 1
            self.update_values(episode)

            # Постепенное уменьшение epsilon для меньшего исследования
            if i % 100 == 0 and i > 0:
                print(f"Эпизоды: {i}, успешных: {successful_episodes}")
                self.epsilon = max(0.01, self.epsilon * 0.95)

    def print_policy(self) -> None:
        """
        Description:
        ---------------
            Отображение политики в читаемом формате.
        """
        arrows = ["↑", "→", "↓", "←"]
        for i in range(self.size):
            row = ""
            for j in range(self.size):
                if (i, j) == (self.size - 1, self.size - 1):
                    row += "G  "  # Цель
                else:
                    row += arrows[self.policy[i, j]] + "  "
            print(row)

# Пример использования
agent = MonteCarloAgent(4    )  # Создаем агента для лабиринта 4x4
agent.train(num_episodes=1000)  # Обучаем агента
print("Ценности состояний:")
print(agent.values)
print("\nОптимальная политика:")
agent.print_policy()
```

В этом коде мы создаем класс `MonteCarloAgent`, который инициализирует агента, генерирует эпизоды и обновляет ценности состояний на основе полученных наград. Метод `generate_episode` создает эпизод, а метод `update_values` обновляет ценности состояний, учитывая только первое посещение каждого состояния.

### Физический и геометрический смысл

Метод Монте-Карло можно представить как процесс, в котором агент (например, робот-пылесос) учится определять ценность каждого участка комнаты, основываясь на своем опыте. Если участок грязный, его ценность будет высокой, и робот будет стремиться туда двигаться. Таким образом, метод Монте-Карло позволяет агенту адаптироваться к окружающей среде, улучшая свои действия на основе полученного опыта.

## Chunk 5

### **Название фрагмента: Обратное распространение вознаграждения в методе Монте-Карло**

**Предыдущий контекст:** В предыдущем чанке мы обсудили метод Монте-Карло как подход к обучению с подкреплением, который позволяет агенту оценивать ценность действий и обновлять свои стратегии на основе полученного опыта.

## **Обратное распространение вознаграждения**

Обратное распространение вознаграждения — это ключевая концепция в методе Монте-Карло, которая позволяет агенту обновлять свои оценки на основе полученных наград в ходе эпизодов. Этот процесс помогает агенту учиться на своих действиях и принимать более обоснованные решения в будущем.

1. **Основная идея**: После завершения эпизода агент должен распространить полученные награды обратно по своим шагам. Это позволяет ему обновить свои оценки Q-функции для каждого состояния и действия, основываясь на общей награде, полученной в эпизоде.

2. **Дисконтированное вознаграждение**: 
   - Коэффициент дисконтирования ($\gamma$) используется для учета будущих наград. Это означает, что награды, полученные позже, имеют меньшую ценность, чем немедленные награды.
   - Общая награда ($G$) накапливается в процессе выполнения действий, и агент обновляет свои Q-значения на основе этой награды.

3. **Обновление Q-значений**: 
   - Обновление Q-значений происходит по формуле:

$$
Q(s, a) = Q(s, a) + \alpha \left( G - Q(s, a) \right)
$$

где:
- $Q(s, a)$ — текущее значение для состояния $s$ и действия $a$,
- $G$ — общая награда, полученная в эпизоде,
- $\alpha$ — коэффициент обучения.

## Chunk 6

### **Название фрагмента: Обучение агента с частичным наблюдением**

**Предыдущий контекст:** В предыдущем чанке мы обсудили, как агент обучается в лабиринте с использованием Q-таблицы, обновляя свои оценки на основе полученных наград и выбирая оптимальные действия.

## **Обучение агента с частичным наблюдением**

Обучение агента с частичным наблюдением — это процесс, в котором агент (например, робот) принимает решения на основе неполной информации о своей окружающей среде. Это особенно актуально в реальных сценариях, где агент не всегда может видеть всю информацию о своем окружении.

1. **Основная идея**: Агент должен определить границы своего окружения и понять, где он находится относительно других объектов и препятствий. Это позволяет ему адаптировать свои действия и находить оптимальные стратегии.

2. **Пример с роботом**: Представьте, что робот-пылесос движется по комнате. Если мимо него проходит человек, его "карта" окружающего мира меняется. Робот должен остановиться, проанализировать ситуацию и заново определить свою стратегию, чтобы избежать столкновений и эффективно выполнять свою задачу. 

3. **Частичное наблюдение**: 
   - Агент использует частичное наблюдение, чтобы определить границы своего окружения. Это может включать в себя использование сенсоров для сканирования пространства и определения местоположения препятствий.
   - После определения границ агент начинает принимать решения о том, как двигаться, основываясь на текущем состоянии и доступной информации.

### Математическая формализация

В контексте частичного наблюдения можно использовать следующие формулы для обновления Q-значений:

$$
Q(s, a) = Q(s, a) + \alpha \left( R + \gamma \max_{a'} Q(s', a') - Q(s, a) \right)
$$

где:
- $Q(s, a)$ — текущее значение для состояния $s$ и действия $a$,
- $R$ — награда за переход в новое состояние,
- $s'$ — новое состояние после выполнения действия,
- $\gamma$ — коэффициент дисконтирования,
- $\alpha$ — коэффициент обучения.


## Final Summary

В тексте рассматриваются методы обучения с подкреплением, такие как итерация по политике и метод Монте-Карло, которые помогают агентам находить оптимальные стратегии в марковских процессах. Итерация по политике включает два основных шага: оценку текущей политики и её улучшение, что позволяет агенту вычислять функцию ценности для всех состояний и выбирать действия, максимизирующие ожидаемую награду. Метод Монте-Карло, в свою очередь, основывается на оценке ценности действий на основе опыта, полученного в ходе взаимодействия с окружающей средой, и обновляет Q-значения на основе полученных наград.

Ключевые компоненты метода Монте-Карло включают количество эпизодов, вероятность случайного действия и коэффициент дисконтирования. Агент учится на своих ошибках, проходя через множество эпизодов, и обновляет свои Q-значения, что позволяет ему адаптироваться к окружающей среде. Визуализация результатов обучения помогает понять, как агент вырабатывает оптимальную стратегию, избегая ловушек и достигая целей.

В коде представлены примеры реализации итерации по политике и метода Монте-Карло, включая создание класса для лабиринта, управление агентом и обновление Q-ценностей. Эти методы позволяют агенту эффективно перемещаться по лабиринту, минимизируя штрафы и максимизируя награды, что аналогично тому, как человек учится ориентироваться в новом пространстве.
