# Оглавление

I. **Формализация Марковских процессов принятия решений (МДП)**
*   Компоненты Марковского процесса принятия решений (МДП)
*   Математическая формализация компонентов МДП

II. **Свойство марковости в МДП**
*   Формальное определение свойства марковости
*   Пример задачи: Лабиринт

III. **Беллмановские уравнения и их значение в МДП**
*   Функция ценности состояния и действия
*   Оптимальные Беллмановские уравнения
*   Пример применения

IV. **Дискаунтирование и горизонты в обучении с подкреплением**
*   Формула дисконтированной награды
*   Интерпретация коэффициента дисконтирования
*   Пример применения
*   Конечные и бесконечные горизонты

V. **Реализация алгоритма обучения агента в среде**
*   Основные компоненты алгоритма

VI. **Визуализация и применение алгоритмов обучения с подкреплением**
*   Визуализация действий агента
*   Применение алгоритмов обучения (Q-Learning, Deep Q-Learning, PPO)

VII. **Реализация и визуализация алгоритма обучения с подкреплением**
*   Основные шаги реализации алгоритма

# Введение

Марковский процесс принятия решений (МДП) – это **математическая модель, описывающая процесс принятия решений агентом в среде, где результаты действий агента зависят от текущего состояния**. МДП позволяет формализовать задачи, в которых необходимо выбирать оптимальные действия для достижения цели, учитывая возможные последствия каждого шага. Важность изучения МДП обусловлена его широким применением в различных областях, таких как робототехника, экономика и искусственный интеллект, особенно в контексте обучения с подкреплением.

В рамках данной лекции будут рассмотрены **основные компоненты МДП**, такие как набор состояний, набор действий, функция перехода состояния, функция вознаграждения и коэффициент дисконтирования. Будет подробно изучено **свойство марковости**, которое значительно упрощает вычисления и разработку алгоритмов обучения. Также будут рассмотрены **Беллмановские уравнения**, являющиеся ключевым инструментом для определения оптимальной стратегии агента.

Кроме того, будут затронуты **вопросы дискаунтирования будущих наград и горизонтов планирования**, которые играют важную роль в процессе принятия решений. Лекция также включает в себя **примеры кода на Python**, иллюстрирующие основные концепции и алгоритмы, а также обсуждение физического и геометрического смысла рассматриваемых математических моделей. В заключение, будет рассмотрена **реализация алгоритма обучения агента в среде и визуализация результатов**, а также обсуждение применения комплексных чисел в контексте обучения с подкреплением.

# Глассарий терминов

*   **Марковский процесс принятия решений (МДП)**: Математическая модель, описывающая, как агент принимает решения в среде, где результаты его действий зависят от текущего состояния.

*   **Состояние (в МДП)**: Множество всех возможных положений, в которых может находиться агент. Например, в шахматах это текущая расстановка фигур на доске. Обозначается как $S$.

*   **Действие (в МДП)**: Множество всех возможных действий, которые может выполнять агент. В шахматах это любой ход фигуры. Обозначается как $A$.

*   **Функция перехода состояния**: Функция, определяющая вероятность перехода агента из одного состояния в другое при выполнении определенного действия. Обозначается как $P(s'|s, a)$, где $s'$ – новое состояние, $s$ – текущее состояние, $a$ – действие.

*   **Функция вознаграждения**: Функция, определяющая, какое вознаграждение получает агент за выполнение действия в определенном состоянии и переход в новое состояние. Обозначается как $R(s, a)$, где $s$ – состояние, $a$ – действие.

*   **Коэффициент дисконтирования**: Коэффициент, показывающий, насколько агент учитывает будущие вознаграждения. Обозначается как $\gamma$, где $0 \leq \gamma < 1$. Определяет, насколько сильно агент будет заботиться о будущих наградах. При $\gamma = 0$ агент оценивает только текущее вознаграждение, при $\gamma = 1$ – все будущие награды одинаково.

*   **Политика**: Стратегия, по которой агент выбирает действия в зависимости от состояния среды. Может быть детерминированной или стохастической.

*   **Свойство марковости**: Утверждает, что будущее состояние системы зависит только от текущего состояния и выбранного действия, а не от всей предшествующей истории.

*   **Беллмановские уравнения**: Ключевой инструмент в обучении с подкреплением, позволяющий формально определить оптимальную стратегию агента для максимизации ожидаемого вознаграждения.

*   **Функция ценности состояния $V(s)$**: Определяет ожидаемую суммарную награду, если агент будет действовать согласно определенной политике.

*   **Функция ценности действия $Q(s, a)$**: Показывает, насколько выгодно выполнить действие $a$ в состоянии $s$.

*   **Дискаунтирование**: Процесс, при котором будущие вознаграждения оцениваются с учетом их меньшей ценности по сравнению с текущими.

*   **Горизонт (в обучении с подкреплением)**: Определяет, как долго агент будет учитывать будущие награды. Может быть конечным или бесконечным.

    *   **Конечный горизонт**: Агент действует в среде с ограниченным числом шагов.
    *   **Бесконечный горизонт**: Агент рассматривает бесконечное количество шагов.

*   **Q-Learning**: Алгоритм обучения с подкреплением, использующий таблицу для хранения ценностей действий в каждом состоянии.

*   **Deep Q-Learning**: Алгоритм, использующий нейронные сети для оценки ценностей действий, что позволяет работать с более сложными средами.

*   **PPO (Proximal Policy Optimization)**: Алгоритм, фокусирующийся на обучении политики, а не на оценке ценностей действий.

---

# Summarization for Text

## Chunk 1

### **Название фрагмента: Формализация Марковских процессов принятия решений или Марковский Дискретный Процесс (МДП)**

**Предыдущий контекст:** В предыдущем фрагменте обсуждались основные темы семинара, включая формализацию Марковских процессов принятия решений (МДП) и их компоненты, а также важность этих процессов в контексте обучения с подкреплением.

## **Компоненты Марковского процесса принятия решений (МДП)**

Марковский процесс принятия решений (МДП) представляет собой математическую модель, которая описывает, как агент принимает решения в среде, где результаты его действий зависят от текущего состояния. Основные компоненты МДП включают:

1. **Набор состояний среды**: Это множество всех возможных состояний, в которых может находиться агент. Например, в игре в шахматы состояние будет представлять собой текущую расстановку фигур на доске.

2. **Набор действий**: Это множество всех возможных действий, которые может выполнять агент. В шахматах действием может быть любой ход фигуры.

3. **Функция перехода состояния**: Эта функция определяет вероятность перехода агента из одного состояния в другое при выполнении определенного действия. Например, если робот движется по сетке, вероятность того, что он окажется в соседней клетке при движении вперед, может составлять 90%, а вероятность остаться на месте — 10%.

4. **Функция вознаграждения**: Эта функция определяет, какое вознаграждение получает агент за выполнение действия в определенном состоянии и переход в новое состояние. В шахматах вознаграждение может быть положительным за взятие фигуры соперника или отрицательным за потерю своей фигуры.

5. **Коэффициент дисконтирования**: Этот коэффициент показывает, насколько агент учитывает будущие вознаграждения. Если коэффициент близок к единице, агент будет стремиться максимизировать долгосрочную выгоду. Если близок к нулю, он будет ориентироваться только на краткосрочные награды.

6. **Политика**: Политика — это стратегия, по которой агент выбирает действия в зависимости от состояния среды. Политика может быть детерминированной (где для каждого состояния есть строгое действие) или стохастической (где действия выбираются с определенной вероятностью).

Математическая формализация этих компонентов может быть представлена следующим образом:

- Набор состояний обозначается как $S$.
- Набор действий обозначается как $A$.
- Функция перехода состояния $P(s'|s, a)$ определяет вероятность перехода в состояние $s'$ из состояния $s$ при выполнении действия $a$.
- Функция вознаграждения $R(s, a)$ определяет вознаграждение, получаемое агентом за выполнение действия $a$ в состоянии $s$.
- Коэффициент дисконтирования обозначается как $\gamma$, где $0 \leq \gamma < 1$.

Пример формулы для расчета ожидаемого вознаграждения может выглядеть так:

$$
V(s) = R(s, a) + \gamma \sum_{s'} P(s'|s, a) V(s')
$$

### Пример кода

Ниже приведен пример кода на Python, который иллюстрирует основные компоненты МДП:

```python
# Импорт стандартных библиотек
import numpy as np

# Импорт сторонних библиотек
from typing import List, Dict, Tuple

class MarkovDecisionProcess:
    """
    Description:
        Класс для моделирования Марковского процесса принятия решений (МДП).

    Args:
        states: Список состояний.
        actions: Список действий.
        transition_probabilities: Словарь с вероятностями перехода.
        rewards: Словарь с вознаграждениями.
        discount_factor: Коэффициент дисконтирования.
    """

    def __init__(
        self,
        states: List[str],
        actions: List[str],
        transition_probabilities: Dict[Tuple[str, str, str], float],
        rewards: Dict[Tuple[str, str], float],
        discount_factor: float
    ) -> None:
        self.states = states
        self.actions = actions
        self.transition_probabilities = transition_probabilities
        self.rewards = rewards
        self.discount_factor = discount_factor

    def value_iteration(self, epsilon: float = 0.01) -> np.ndarray:
        """
        Description:
            Алгоритм итерации по значениям для нахождения оптимальной политики.

        Args:
            epsilon: Порог для остановки итераций.

        Returns:
            Оптимальные значения состояний.

        Raises:
            ValueError: Если epsilon отрицательный.

        Examples:
            >>> mdp = MarkovDecisionProcess(states, actions, transition_probabilities, rewards, discount_factor)
            >>> optimal_values = mdp.value_iteration()
            >>> print(optimal_values)
            [12.34, 56.78]
        """
        if epsilon < 0:
            raise ValueError("Epsilon не может быть отрицательным.")

        V = np.zeros(len(self.states))  # Инициализация значений состояний
        while True:
            delta = 0
            for s in range(len(self.states)):
                v = V[s]
                V[s] = max(self.calculate_value(s, a, V) for a in self.actions)
                delta = max(delta, abs(v - V[s]))
            if delta < epsilon:
                break
        return V

    def calculate_value(
        self,
        state_index: int,
        action: str,
        V: np.ndarray
    ) -> float:
        """
        Description:
            Расчет ожидаемого значения для состояния и действия.

        Args:
            state_index: Индекс состояния.
            action: Действие.
            V: Текущие значения состояний.

        Returns:
            Ожидаемое значение.

        Examples:
            >>> mdp = MarkovDecisionProcess(states, actions, transition_probabilities, rewards, discount_factor)
            >>> V = np.zeros(len(states))
            >>> value = mdp.calculate_value(0, 'a1', V)
            >>> print(value)
            10.0
        """
        value = 0
        for next_state in self.states:
            prob = self.transition_probabilities[(self.states[state_index], action, next_state)]
            reward = self.rewards[(self.states[state_index], action)]
            value += prob * (reward + self.discount_factor * V[self.states.index(next_state)])
        return value

# Пример использования
states = ['s1', 's2']
actions = ['a1', 'a2']
transition_probabilities = {
    ('s1', 'a1', 's1'): 0.9, ('s1', 'a1', 's2'): 0.1,
    ('s1', 'a2', 's1'): 0.8, ('s1', 'a2', 's2'): 0.2,
    ('s2', 'a1', 's1'): 0.4, ('s2', 'a1', 's2'): 0.6,
    ('s2', 'a2', 's1'): 0.3, ('s2', 'a2', 's2'): 0.7
}
rewards = {('s1', 'a1'): 10, ('s1', 'a2'): 5, ('s2', 'a1'): 2, ('s2', 'a2'): 3}
discount_factor = 0.9

mdp = MarkovDecisionProcess(states, actions, transition_probabilities, rewards, discount_factor)
optimal_values = mdp.value_iteration()
print("Оптимальные значения состояний:", optimal_values)
```

### Физический и геометрический смысл

В физике концепция МДП может быть применена для моделирования поведения робота в неопределенной среде. Например, если робот должен перемещаться по комнате, он может рассматривать каждое положение как состояние, а каждое движение — как действие. Функция перехода состояния будет описывать вероятность того, что робот окажется в определенной позиции после выполнения действия, а функция вознаграждения может учитывать, насколько близко он подошел к цели. Таким образом, МДП позволяет эффективно планировать действия в условиях неопределенности, что является важным аспектом в робототехнике и автоматизации.

## Chunk 2
### **Название фрагмента: Свойства МДП**

**Предыдущий контекст:** В предыдущем фрагменте обсуждались основные компоненты Марковского процесса принятия решений (МДП), включая набор состояний, действий, функции перехода, вознаграждения, коэффициент дисконтирования и политику. Эти компоненты формируют основу для понимания, как агент принимает решения в среде.

## **Свойство МДП**

Свойства МДП является ключевым аспектом Марковских процессов принятия решений (МДП). Оно утверждает, что будущее состояние системы зависит только от текущего состояния и выбранного действия, а не от всей предшествующей истории. Это свойство позволяет значительно упростить вычисления и разработку алгоритмов обучения.

Формально, свойства МДП можно выразить следующим образом:

$$
P(s_{t+1} | s_t, a_t) = P(s_{t+1} | s_{t-1}, a_{t-1}, s_{t-2}, a_{t-2}, \ldots, s_0, a_0)
$$

где:
- $P(s_{t+1} | s_t, a_t)$ — вероятность перехода в состояние $s_{t+1}$ из состояния $s_t$ при выполнении действия $a_t$;
- $P(s_{t+1} | s_{t-1}, a_{t-1}, s_{t-2}, a_{t-2}, \ldots, s_0, a_0)$ — вероятность перехода в состояние $s_{t+1}$, учитывая всю предшествующую историю.

Это упрощение позволяет строить эффективные алгоритмы обучения, так как агенту не нужно хранить всю историю состояний и действий, а достаточно знать текущее состояние и действие.

### Пример задачи: Лабиринт

Рассмотрим пример, в котором агент должен пройти через лабиринт. В этом случае:
- Каждая клетка лабиринта представляет собой состояние.
- Действия включают движение вверх, вниз, влево и вправо.
- Функция перехода может быть задана так, что с вероятностью 90% агент движется в заданном направлении, а с 10% остается на месте (например, из-за скользкого пола).
- Функция вознаграждения может быть следующей: +10 за выход из лабиринта, -1 за каждое движение и -100 за попадание в ловушку.
- Коэффициент дисконтирования может быть равен 0.9, что означает, что агент ценит долгосрочные вознаграждения.

Таким образом, агент должен обучиться максимизировать свои награды, используя марковские принципы.

### Пример кода

Ниже приведен пример кода на Python, который иллюстрирует простую реализацию лабиринта с использованием свойства марковости:

```python
from typing import Dict, Tuple, List

import numpy as np

class MazeAgent:
    """
    Description:
        Класс MazeAgent представляет агента, который перемещается по лабиринту.

    Args:
        maze: Матрица лабиринта, где 0 - проходимая клетка, 1 - стена.
        rewards: Словарь с вознаграждениями для каждой клетки.
        discount_factor: Коэффициент дисконтирования.
    """

    def __init__(self, maze: List[List[int]], rewards: Dict[Tuple[int, int], int], discount_factor: float) -> None:
        """
        Description:
            Инициализация агента в лабиринте.

        Args:
            maze: Матрица лабиринта, где 0 - проходимая клетка, 1 - стена.
            rewards: Словарь с вознаграждениями для каждой клетки.
            discount_factor: Коэффициент дисконтирования.
        """
        self.maze = maze
        self.rewards = rewards
        self.discount_factor = discount_factor
        self.state = (0, 0)  # Начальное состояние (0, 0)

    def move(self, action: str) -> None:
        """
        Description:
            Перемещение агента в зависимости от действия.

        Args:
            action: Действие (вверх, вниз, влево, вправо).
        """
        x, y = self.state
        if action == 'up' and x > 0 and self.maze[x-1][y] == 0:
            self.state = (x-1, y)
        elif action == 'down' and x < len(self.maze)-1 and self.maze[x+1][y] == 0:
            self.state = (x+1, y)
        elif action == 'left' and y > 0 and self.maze[x][y-1] == 0:
            self.state = (x, y-1)
        elif action == 'right' and y < len(self.maze[0])-1 and self.maze[x][y+1] == 0:
            self.state = (x, y+1)

    def get_reward(self) -> int:
        """
        Description:
            Получение вознаграждения за текущее состояние.

        Returns:
            Вознаграждение для текущего состояния.
        """
        return self.rewards.get(self.state, 0)

# Пример использования
if __name__ == "__main__":
    maze = [
        [0, 0, 1, 0],
        [0, 1, 1, 0],
        [0, 0, 0, 0],
        [1, 1, 0, 0]
    ]
    rewards = {
        (0, 0): -1, (0, 1): -1, (1, 0): -1,
        (2, 0): -1, (2, 1): -1, (2, 2): 10,
        (3, 2): -1, (3, 3): 10
    }
    discount_factor = 0.9

    agent = MazeAgent(maze, rewards, discount_factor)
    actions = ['down', 'down', 'right', 'right']  # Пример действий
    for action in actions:
        agent.move(action)
        print(f"Текущее состояние: {agent.state}, Вознаграждение: {agent.get_reward()}")
```

### Физический и геометрический смысл

В физике свойство марковости можно применить для моделирования движения частиц в газе. Например, если частица движется в пространстве, ее будущее положение зависит только от текущего положения и скорости, а не от того, как она туда попала. Это позволяет использовать марковские модели для предсказания поведения частиц и анализа их взаимодействий в различных условиях.

## Chunk 3

### **Название фрагмента: Беллмановские уравнения и их значение в МДП**

**Предыдущий контекст:** В предыдущем фрагменте обсуждалось свойство марковости, которое утверждает, что будущее состояние системы зависит только от текущего состояния и выбранного действия. Это свойство позволяет упростить вычисления и разработку алгоритмов обучения, таких как МДП.

## **Беллмановские уравнения**

Беллмановские уравнения являются ключевым инструментом в обучении с подкреплением, позволяя формально определить, как рассчитывать оптимальную стратегию (политику) агента для максимизации ожидаемого вознаграждения. Эти уравнения связывают ценности состояний и действий, что позволяет агенту принимать обоснованные решения.

Существует две основные функции ценности:
1. **Функция ценности состояния** $V(s)$, которая определяет ожидаемую суммарную награду, если агент будет действовать согласно определенной политике $P$:
   $$
   V(s) = \sum_{a \in A} \pi(a|s) \sum_{s'} P(s'|s, a) [R(s, a) + \gamma V(s')]
   $$
   где:
   - $V(s)$ — ценность состояния $s$;
   - $\pi(a|s)$ — вероятность выбора действия $a$ в состоянии $s$ согласно политике;
   - $P(s'|s, a)$ — вероятность перехода в состояние $s'$ из состояния $s$ при выполнении действия $a$;
   - $R(s, a)$ — вознаграждение за выполнение действия $a$ в состоянии $s$;
   - $\gamma$ — коэффициент дисконтирования.

2. **Функция ценности действия** $Q(s, a)$, которая показывает, насколько выгодно выполнить действие $a$ в состоянии $s$:
   $$
   Q(s, a) = \sum_{s'} P(s'|s, a) [R(s, a) + \gamma V(s')]
   $$
   где:
   - $Q(s, a)$ — ценность выполнения действия $a$ в состоянии $s$.

### Оптимальные Беллмановские уравнения

Когда агент стремится найти оптимальную политику, функции ценности записываются следующим образом:
- Оптимальная ценность состояния:
  $$
  V^*(s) = \max_{a \in A} Q^*(s, a)
  $$
- Оптимальная ценность действия:
  $$
  Q^*(s, a) = \sum_{s'} P(s'|s, a) [R(s, a) + \gamma V^*(s')]
  $$

Эти уравнения позволяют агенту выбирать действия, которые максимизируют его ожидаемую награду, основываясь на ценностях состояний и действий.

### Пример применения

Рассмотрим задачу, в которой агент находится в сетке 3 на 3 и должен добраться до выхода, получая вознаграждение -1 за каждый шаг и +10 за достижение выхода. Применяя Беллмановские уравнения, можно вычислить ценности всех состояний и определить, какие действия являются наиболее выгодными.

### Пример кода

Ниже приведен пример кода на Python, который иллюстрирует использование Беллмановских уравнений для вычисления ценностей состояний в сетке:

```python
import numpy as np

class ValueIteration:
    def __init__(self, grid_size, rewards, discount_factor):
        """
        Description:
            Инициализация алгоритма итерации по значениям.

        Args:
            grid_size: Размер сетки (например, 3 для 3x3).
            rewards: Словарь с вознаграждениями для каждой клетки.
            discount_factor: Коэффициент дисконтирования.
        """
        self.grid_size = grid_size
        self.rewards = rewards
        self.discount_factor = discount_factor
        self.values = np.zeros((grid_size, grid_size))  # Инициализация ценностей

    def iterate(self, epsilon=0.01):
        """
        Description:
            Итерация по значениям для нахождения оптимальных ценностей.

        Args:
            epsilon: Порог для остановки итераций.

        Returns:
            Ценности состояний.
        """
        while True:
            delta = 0
            for x in range(self.grid_size):
                for y in range(self.grid_size):
                    v = self.values[x, y]
                    self.values[x, y] = self.calculate_value(x, y)
                    delta = max(delta, abs(v - self.values[x, y]))
            if delta < epsilon:
                break
        return self.values

    def calculate_value(self, x, y):
        """
        Description:
            Расчет ожидаемого значения для состояния (x, y).

        Args:
            x: Координата по оси x.
            y: Координата по оси y.

        Returns:
            Ожидаемое значение.
        """
        value = 0
        # Действия: вправо, вниз, влево, вверх
        actions = [(0, 1), (1, 0), (0, -1), (-1, 0)]
        for dx, dy in actions:
            new_x, new_y = x + dx, y + dy
            if 0 <= new_x < self.grid_size and 0 <= new_y < self.grid_size:
                value += 0.25 * (self.rewards.get((new_x, new_y), 0) + self.discount_factor * self.values[new_x, new_y])
        return value

# Пример использования
grid_size = 3
# Награда за выход
rewards = {(2, 2): 10}
discount_factor = 0.9

vi = ValueIteration(grid_size, rewards, discount_factor)
optimal_values = vi.iterate()
print("Оптимальные ценности состояний:\n", optimal_values)
```

### Физический и геометрический смысл

В физике Беллмановские уравнения могут быть использованы для моделирования движения частиц в потенциальных полях. Например, если частица движется в поле силы, ее будущее положение зависит не только от текущего положения, но и от силы, действующей на нее. Используя Беллмановские уравнения, можно оценить, какие действия (например, изменение направления движения) приведут к максимальному уменьшению потенциальной энергии, что аналогично максимизации вознаграждения в задачах обучения с подкреплением.

## Chunk 4
### **Название фрагмента: Дискаунтирование и горизонты в обучении с подкреплением**

**Предыдущий контекст:** В предыдущем фрагменте обсуждались Беллмановские уравнения, которые позволяют агенту оценивать ценность состояний и действий, а также находить оптимальную стратегию взаимодействия с окружающей средой. Эти уравнения являются основой для динамического программирования и методов обучения с подкреплением.

## **Дискаунтирование будущих наград**

Дискаунтирование — это процесс, при котором будущие вознаграждения оцениваются с учетом их меньшей ценности по сравнению с текущими. Это важно в обучении с подкреплением, поскольку в реальной жизни будущие награды могут быть менее надежными из-за различных факторов, таких как риск, неопределенность и временная стоимость денег.

### Формула дисконтированной награды

Общая формула для суммарной дисконтированной награды, получаемой начиная с момента $t$ и до конца, может быть записана следующим образом:

$$
R_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \ldots = \sum_{k=0}^{\infty} \gamma^k r_{t+k}
$$

Разберем каждую переменную и компоненту по порядку:

- **$R_t$ — суммарная дисконтированная награда (возврат) с момента времени $t$.**
    - Это значение представляет собой *общую* награду, которую агент ожидает получить, начиная с текущего момента времени $t$ и до конца эпизода (или бесконечности, в зависимости от задачи).
    - Важно, что это *дисконтированная* награда, то есть будущие награды "ценятся" меньше, чем немедленные, из-за коэффициента дисконтирования $\gamma$.

- **$r_t$ — немедленная награда, полученная на шаге $t$.**
    - Это награда, которую агент получает непосредственно в момент времени $t$, совершив какое-то действие в состоянии $s_t$ и перейдя в состояние $s_{t+1}$.
    - Например, в игре, $r_t$ может быть очками, полученными за ход, или штрафом за ошибку.

- **$r_{t+1}, r_{t+2}, \ldots, r_{t+k}, \ldots$ —  последующие немедленные награды.**
    - $r_{t+1}$ — это награда, полученная на следующем шаге, $t+1$.
    - $r_{t+2}$ — награда на шаге $t+2$, и так далее.
    - В общем, $r_{t+k}$ — это награда, полученная через $k$ шагов от текущего момента $t$.

- **$\gamma$ — коэффициент дисконтирования (гамма), $0 \le \gamma \le 1$.**
    - Этот коэффициент определяет, насколько мы ценим будущие награды по сравнению с текущими.
    - Если $\gamma = 0$, то агент заботится только о немедленной награде ($R_t = r_t$). Будущие награды полностью игнорируются.
    - Если $\gamma = 1$, то будущие награды ценятся так же, как и текущие. Агент стремится максимизировать *сумму всех* будущих наград.
    - Обычно $\gamma$ устанавливают в диапазоне $(0, 1)$, например, 0.9 или 0.99. Это позволяет агенту учитывать будущие награды, но при этом отдавать предпочтение более близким во времени наградам.  Дисконтирование часто используется, потому что:
        - В реальных задачах будущие награды могут быть менее определенными или надежными.
        - Дисконтирование помогает избежать бесконечных сумм наград в эпизодах бесконечной длительности.
        - Математически, дисконтирование обеспечивает сходимость некоторых алгоритмов обучения с подкреплением.

- **$\gamma^k$ — степень дисконтирования для награды, полученной через $k$ шагов в будущем.**
    - Чем больше $k$, тем больше степень, в которую возводится $\gamma$, и тем меньше становится значение $\gamma^k$ (если $\gamma < 1$).
    - Это означает, что награды, полученные через большее количество шагов в будущем, вносят меньший вклад в суммарную дисконтированную награду $R_t$.

- **$\sum_{k=0}^{\infty} \gamma^k r_{t+k}$ —  запись в виде суммы.**
    - Символ $\sum$ (сигма) обозначает суммирование.
    - $k=0$ — начальное значение индекса суммирования.
    - $\infty$ — конечное значение (в данном случае, до бесконечности, подразумевая, что эпизод может быть бесконечным или достаточно длинным).
    - $k$ — индекс, который пробегает значения от 0 до бесконечности.
        - При $k=0$:  $\gamma^0 r_{t+0} = 1 \cdot r_t = r_t$ (немедленная награда)
        - При $k=1$:  $\gamma^1 r_{t+1} = \gamma r_{t+1}$ (награда на следующем шаге, дисконтированная на $\gamma$)
        - При $k=2$:  $\gamma^2 r_{t+2}$ (награда через два шага, дисконтированная на $\gamma^2$), и так далее.

**Таким образом, формула показывает, что суммарная дисконтированная награда $R_t$ является взвешенной суммой всех будущих наград, начиная с момента $t$, где вес каждой будущей награды уменьшается экспоненциально с увеличением каждого следующего шага, благодаря коэффициенту дисконтирования $\gamma$.**

### Интерпретация коэффициента дисконтирования

Коэффициент дисконтирования $\gamma$ определяет, насколько сильно агент будет заботиться о будущих наградах:
- Если $\gamma = 0$, агент будет оценивать только текущее вознаграждение и игнорировать будущее.
- Если $\gamma = 1$, агент будет оценивать все будущие награды одинаково, что может привести к бесконечным вычислениям.
- Значения $\gamma$ от 0.1 до 0.3 ориентируют агента на краткосрочные выгоды, что подходит для задач с немедленными решениями.
- Значения $\gamma$ от 0.9 до 0.99 показывают, что агент должен учитывать долгосрочные награды, что лучше подходит для стратегического планирования.

### Пример применения

Представим робота, который собирает ресурсы. Если его коэффициент дисконтирования равен 0.3, он будет предпочитать более короткие маршруты для сбора ресурсов. Если же коэффициент равен 0.99, он может выбрать более долгий путь, если это приведет к большей награде в будущем.

### Конечные и бесконечные горизонты

Горизонт в обучении с подкреплением определяет, как долго агент будет учитывать будущие награды:
- **Конечный горизонт**: агент действует в среде с ограниченным числом шагов. Например, в игре с фиксированным количеством ходов или в управлении ракетой, которая должна приземлиться через 100 секунд. В этом случае будущие награды учитываются только до установленного шага.
- **Бесконечный горизонт**: агент рассматривает бесконечное количество шагов, что может привести к сложным вычислениям и нестабильности.

### Пример кода

Ниже приведен пример кода на Python, который иллюстрирует, как агент может использовать коэффициент дисконтирования для оценки суммарной награды:

```python
class DiscountedReward:
    def __init__(self, rewards, discount_factor):
        """
        Description:
            Инициализация класса для расчета дисконтированной награды.

        Args:
            rewards: Список наград на каждом шаге.
            discount_factor: Коэффициент дисконтирования.
        """
        self.rewards = rewards
        self.discount_factor = discount_factor

    def calculate_discounted_reward(self):
        """
        Description:
            Расчет суммарной дисконтированной награды.

        Returns:
            Дисконтированная награда.
        """
        total_reward = 0
        for t, reward in enumerate(self.rewards):
            total_reward += (self.discount_factor ** t) * reward
        return total_reward

# Пример использования
rewards = [10, -1, -1, 10]
discount_factor = 0.9

dr = DiscountedReward(rewards, discount_factor)
total_discounted_reward = dr.calculate_discounted_reward()
print("Суммарная дисконтированная награда:", total_discounted_reward)
```

### Физический и геометрический смысл

Например, в финансах принцип временной стоимости денег утверждает, что 100 рублей сегодня ценнее, чем 100 рублей через год. Это связано с рисками и неопределенностью, которые увеличиваются со временем. Таким образом, дискаунтирование позволяет учитывать эти факторы при принятии решений, что аналогично тому, как агент в обучении с подкреплением оценивает свои действия и награды.

## Chunk 5

### **Название фрагмента: Реализация алгоритма обучения агента в среде**

**Предыдущий контекст:** В предыдущем фрагменте обсуждались горизонты в обучении с подкреплением, включая конечные и бесконечные горизонты, а также их влияние на принятие решений агентом. Также рассматривалась важность коэффициента дисконтирования для оценки будущих наград.

## **Алгоритм обучения агента в среде**

В этом фрагменте рассматривается реализация алгоритма, который позволяет агенту обучаться в заданной среде, используя методы, основанные на принципах обучения с подкреплением. Агент должен достигнуть цели, находясь в определенной области, избегая при этом запрещенных зон и получая награды за свои действия.

### Основные компоненты алгоритма

1. **Определение среды**: Агент находится в сетке, где его цель — добраться до определенной точки (например, 4,4). В процессе он может столкнуться с запрещенными зонами, которые необходимо избегать.

2. **Награды и штрафы**: Агент получает положительные награды за достижение цели и отрицательные за попадание в запрещенные зоны или за неэффективные действия. Например, штраф может составлять -20 за попадание в запрещенную зону.

3. **Коэффициент дисконтирования**: Используется для оценки будущих наград. Например, если коэффициент дисконтирования $\gamma = 0.9$, это означает, что агент будет учитывать будущие награды, но с уменьшением их ценности.

4. **Обновление ценностей состояний**: Агент обновляет свои оценки ценности состояний на основе текущей награды и ожидаемой награды в следующем состоянии. Это делается с помощью формулы:

$$
V(s) = R(s) + \gamma \max_{a} V(s')
$$

где:
- $V(s)$ — ценность текущего состояния;
- $R(s)$ — награда за текущее состояние;
- $\gamma$ — коэффициент дисконтирования;
- $s'$ — следующее состояние.

### Пример кода

Ниже приведен пример кода на Python, который иллюстрирует, как агент может обучаться в среде, используя описанные принципы:

```python
from typing import Tuple
import numpy as np

class Agent:
    def __init__(self, grid_size: int, discount_factor: float):
        """
        Description:
            Инициализация агента.

        Args:
            grid_size: Размер сетки (например, 5 для 5x5).
            discount_factor: Коэффициент дисконтирования.
        """
        self.grid_size = grid_size
        self.discount_factor = discount_factor
        self.values = np.zeros((grid_size, grid_size))   # Инициализация ценностей
        self.rewards = np.zeros((grid_size, grid_size))  # Инициализация наград

    def set_rewards(self, goal: Tuple[int, int], penalty: Tuple[int, int]):
        """
        Description:
            Установка наград для целей и штрафов.

        Args:
            goal: Координаты цели.
            penalty: Координаты запрещенной зоны.
        """
        self.rewards[goal]    = 10    # Награда за достижение цели
        self.rewards[penalty] = -20   # Штраф за попадание в запрещенную зону
        self.values[goal]     = 10    # Инициализация ценности цели
        self.values[penalty]  = -20   # Инициализация ценности штрафа

    def update_values(self, iterations: int = 100):
        """
        Description:
            Обновление ценностей состояний на основе наград и коэффициента дисконтирования.

        Args:
            iterations: Количество итераций для обновления ценностей.
        """
        for _ in range(iterations):
            new_values = np.copy(self.values)
            for x in range(self.grid_size):
                for y in range(self.grid_size):
                    # Если есть награда или штраф
                    if self.rewards[x, y] != 0:
                        continue
                    # Обновление ценности состояния
                    future_rewards = []
                    # Действия: вверх, вниз, влево, вправо
                    for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1)]:
                        new_x, new_y = x + dx, y + dy
                        if 0 <= new_x < self.grid_size and 0 <= new_y < self.grid_size:
                            future_rewards.append(self.values[new_x, new_y])
                    if future_rewards:
                        new_values[x, y] = self.discount_factor * max(future_rewards) + self.rewards[x, y]
            self.values = new_values

# Пример использования
if __name__ == "__main__":
    grid_size = 5
    discount_factor = 0.9
    agent = Agent(grid_size, discount_factor)

    # Установка наград и штрафов
    agent.set_rewards(goal=(4, 4), penalty=(2, 2))

    # Обновление ценностей состояний
    # Увеличиваем количество итераций для лучшей сходимости
    agent.update_values(iterations=1000)
    print("Ценности состояний:\n", agent.values)
```

### Физический и геометрический смысл

В физике аналогичные принципы могут быть применены для моделирования движения объектов в пространстве. Например, в задачах автономного вождения, где автомобиль должен избегать препятствий и достигать заданной точки, агент (автомобиль) должен оценивать свои действия, основываясь на текущих наградах (например, безопасность) и будущих наградах (например, достижение цели). Использование коэффициента дисконтирования позволяет автомобилю оптимизировать свои действия, принимая во внимание как краткосрочные, так и долгосрочные цели, что аналогично тому, как в реальной жизни мы принимаем решения, основываясь на ожидаемых результатах в будущем.

## Chunk 6

### **Название фрагмента: Визуализация и применение алгоритмов обучения с подкреплением**

**Предыдущий контекст:** В предыдущем фрагменте обсуждались основные принципы работы агента в среде, включая обновление ценностей состояний и использование коэффициента дисконтирования для оценки наград. Также рассматривались действия агента и их влияние на обучение.

## **Визуализация действий агента и применение алгоритмов**

В этом фрагменте рассматривается, как визуализировать действия агента в среде и как это связано с алгоритмами обучения с подкреплением, такими как Q-Learning и Deep Q-Learning. Визуализация помогает лучше понять, как агент принимает решения и какие действия он выбирает в зависимости от текущих наград и штрафов.

### Визуализация действий агента

1. **Гистограммы и тепловые карты**: Визуализация результатов работы агента может быть выполнена с помощью гистограмм и тепловых карт. Гистограммы показывают распределение наград, а тепловые карты отображают ценности состояний, что позволяет увидеть, какие области среды агент считает более выгодными.

2. **Запрещенные зоны**: Визуализация также может включать обозначение запрещенных зон, где агент не может находиться. Это помогает понять, как агент избегает определенных областей и какие действия он выбирает для достижения своей цели.

3. **Пример визуализации**: Например, если агент должен добраться до точки (4,4), а запрещенная зона находится в (2,2), то визуализация может показать, как агент избегает этой зоны и выбирает оптимальные пути.

### Применение алгоритмов обучения

#### 1. Q-Learning

**Математическая формализация:**

Q-Learning является алгоритмом *внеполитического* (off-policy) обучения, основанным на *временной разнице* (Temporal Difference, TD).  Основная идея заключается в обучении *Q-функции*, которая оценивает ожидаемое кумулятивное вознаграждение за выполнение действия $a$ в состоянии $s$ и далее действуя оптимально.

Обновление Q-значения происходит по следующей формуле:

$$Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma \max_{a'} Q(s', a') - Q(s, a)]$$

Где:

*   $Q(s, a)$:  Q-значение для состояния $s$ и действия $a$. Это оценка того, насколько хорошо выполнить действие $a$ в состоянии $s$.
*   $s$: Текущее состояние среды.
*   $a$: Действие, выполненное агентом в состоянии $s$.
*   $\alpha$ (альфа): *Скорость обучения* (learning rate),  параметр, определяющий, насколько сильно новые знания будут влиять на старые Q-значения. Значение $\alpha$ находится в диапазоне $[0, 1]$.  Чем выше $\alpha$, тем быстрее агент адаптируется к новым наградам, но тем больше вероятность нестабильности.
*   $r$: *Награда* (reward), полученная агентом после выполнения действия $a$ в состоянии $s$ и перехода в новое состояние $s'$.
*   $\gamma$ (гамма): *Фактор дисконтирования* (discount factor), параметр в диапазоне $[0, 1]$, определяющий, насколько важны будущие награды по сравнению с текущими.  Значение $\gamma$ близкое к 0 делает агента близоруким, фокусирующимся на немедленных наградах. Значение $\gamma$ близкое к 1 делает агента дальновидным, учитывающим будущие награды.
*   $s'$: Новое состояние, в которое агент переходит после выполнения действия $a$ в состоянии $s$.
*   $a'$:  Действие в следующем состоянии $s'$. В Q-Learning мы рассматриваем *максимальное* Q-значение среди всех возможных действий $a'$ в следующем состоянии $s'$. Это отражает стремление агента к оптимальной политике.
*   $\max_{a'} Q(s', a')$: Максимальное Q-значение среди всех возможных действий $a'$ в следующем состоянии $s'$. Это оценка наилучшего возможного будущего вознаграждения, которое агент может получить, начиная с состояния $s'$.

**Детальное пояснение:**

Формула обновления Q-значения представляет собой итеративный процесс.  На каждом шаге агент:

1.  Находится в состоянии $s$.
2.  Выбирает действие $a$ (например, используя ε-жадную стратегию, которая балансирует между исследованием новых действий и использованием уже известных выгодных действий).
3.  Выполняет действие $a$ в среде.
4.  Получает награду $r$ и переходит в новое состояние $s'$.
5.  Обновляет Q-значение для пары $(s, a)$ согласно приведенной выше формуле.

**Интуитивное понимание обновления:**

Обновление Q-значения - это коррекция текущей оценки $Q(s, a)$ на основе *ошибки временной разницы* (TD error):  $[r + \gamma \max_{a'} Q(s', a') - Q(s, a)]$.

*   $r + \gamma \max_{a'} Q(s', a')$:  Это *целевое* значение,  оценка того, каким должно быть $Q(s, a)$ на основе полученного опыта. Оно состоит из немедленной награды $r$ и дисконтированного максимального Q-значения в следующем состоянии $s'$.
*   $Q(s, a)$: Это текущая оценка Q-значения.
*   **Разница между целевым значением и текущей оценкой** представляет собой ошибку, которую нужно скорректировать.  Скорость обучения $\alpha$ контролирует, насколько сильно мы корректируем текущую оценку в направлении целевого значения.

**Связь с текстовым описанием:**

Текст описывает Q-Learning как алгоритм, использующий таблицу для хранения ценностей действий.  В математической формализации Q-таблица - это и есть таблица, где строками являются состояния $s$, столбцами - действия $a$, а ячейками - значения $Q(s, a)$.  Алгоритм обновляет эти значения на основе получаемых наград, что и отражено в формуле обновления.

#### 2. Deep Q-Learning (DQN)

**Математическая формализация:**

Deep Q-Learning (DQN) является расширением Q-Learning, которое использует *глубокие нейронные сети* для аппроксимации Q-функции.  Вместо Q-таблицы, DQN использует нейронную сеть, параметризованную весами $\theta$, для оценки Q-значений:  $Q(s, a; \theta)$.

Обучение DQN происходит путем минимизации *функции потерь* (loss function).  Функция потерь в DQN обычно основана на *среднеквадратичной ошибке* (Mean Squared Error, MSE) между *целевым Q-значением* и *предсказанным Q-значением*.

**Функция потерь (Loss):**  $$L(\theta) = E [(y_i - Q(s_i, a_i; \theta))^2]$$

Где:

*   $L(\theta)$: Функция потерь, которую нужно минимизировать, чтобы обучить параметры нейронной сети $\theta$.
*   $E$: Математическое ожидание (среднее значение) по набору обучающих примеров.
*   $(s_i, a_i, r_i, s'_i)$:  Опыт агента, собранный в виде *переходов* (transitions), обычно хранящихся в *буфере воспроизведения* (replay buffer).  $s_i$ - состояние, $a_i$ - действие, $r_i$ - награда, $s'_i$ - следующее состояние.
*   $Q(s_i, a_i; \theta)$:  Предсказанное Q-значение нейронной сетью с параметрами $\theta$ для состояния $s_i$ и действия $a_i$.
*   $y_i$: *Целевое Q-значение* для перехода $(s_i, a_i, r_i, s'_i)$.  В DQN целевое Q-значение рассчитывается с использованием *целевой нейронной сети* (target network) с параметрами $\theta^-$ (которые периодически обновляются весами основной сети $\theta$).

**Целевое Q-значение ($y_i$):**  $$y_i = r_i + \gamma \max_{a'} Q(s'_i, a'; \theta^-)$$

*   $\theta^-$: Параметры *целевой нейронной сети*. Целевая сеть является копией основной сети, но ее веса обновляются реже (например, каждые N шагов), чтобы стабилизировать обучение.
*   $Q(s'_i, a'; \theta^-)$:  Q-значение, предсказанное *целевой нейронной сетью* для следующего состояния $s'_i$ и действия $a'$.  Использование целевой сети помогает уменьшить корреляцию между целевым значением и текущими параметрами сети, что делает обучение более стабильным.
*   $\max_{a'} Q(s'_i, a'; \theta^-)$: Максимальное Q-значение, предсказанное целевой сетью для следующего состояния $s'_i$ среди всех возможных действий $a'$.

**Детальное пояснение:**

DQN решает проблему масштабируемости Q-Learning для задач с большим количеством состояний и действий, используя нейронную сеть для аппроксимации Q-функции.

1.  **Нейронная сеть как Q-функция:**  Вместо таблицы DQN использует нейронную сеть, которая принимает на вход состояние $s$ (и, возможно, действие $a$) и выдает оценку $Q(s, a)$.  Нейронная сеть позволяет обобщать знания, полученные в одних состояниях, на другие, похожие состояния.
2.  **Буфер воспроизведения (Replay Buffer):** DQN использует буфер воспроизведения для хранения переходов $(s, a, r, s')$.  При обучении мини-пакеты переходов случайным образом выбираются из буфера воспроизведения. Это позволяет:
    *   **Устранить корреляцию между последовательными переходами:**  Последовательные переходы в RL часто сильно коррелированы, что может привести к нестабильности обучения. Случайная выборка из буфера воспроизведения уменьшает эту корреляцию.
    *   **Повысить эффективность использования данных:**  Каждый переход может быть использован для обучения несколько раз.
3.  **Целевая нейронная сеть (Target Network):**  Использование целевой сети стабилизирует обучение.  Целевая сеть предоставляет более стабильные целевые значения для обучения основной сети, поскольку ее параметры обновляются реже.

**Связь с текстовым описанием:**

Текст объясняет, что Deep Q-Learning использует нейронные сети для оценки ценностей действий и эффективен в сложных средах с большим количеством состояний. Математическая формализация показывает, как именно нейронная сеть используется для аппроксимации Q-функции и как функция потерь и целевая сеть помогают обучить эту нейронную сеть.

#### 3. Proximal Policy Optimization (PPO)

**Математическая формализация:**

Proximal Policy Optimization (PPO) - это *политический* (policy-based) алгоритм обучения с подкреплением, который напрямую оптимизирует *политику* агента, а не Q-функцию.  Политика $\pi(a|s)$ определяет вероятность выбора действия $a$ в состоянии $s$.

PPO относится к классу алгоритмов *доверительной области* (Trust Region Policy Optimization, TRPO), но является более простым в реализации и более эффективным.  Основная идея PPO - обновлять политику итеративно, но при этом ограничивать размер шага обновления, чтобы избежать резких изменений в политике, которые могут привести к нестабильности обучения.

**Целевая функция PPO (упрощенная версия):**

$$J(\theta) = E_t [ \min( r_t(\theta) \hat{A}_t,  clip(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t ) ]$$

Где:

*   $J(\theta)$: Целевая функция, которую нужно максимизировать, чтобы обучить параметры политики $\theta$.
*   $E_t$: Математическое ожидание (среднее значение) по времени (по траекториям, собранным во время взаимодействия агента со средой).
*   $\theta$: Параметры текущей политики $\pi_\theta$ (обычно нейронная сеть).
*   $r_t(\theta)$: *Отношение вероятностей* (probability ratio) между новой политикой $\pi_\theta(a_t|s_t)$ и старой политикой $\pi_{\theta_{old}}(a_t|s_t)$ для действия $a_t$ в состоянии $s_t$ в момент времени $t$:

    $$r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$$

*   $\pi_{\theta_{old}}$: Параметры *старой политики*, использованной для сбора данных (траекторий).  Старая политика фиксируется на несколько итераций обучения.
*   $\hat{A}_t$: *Оценка функции преимущества* (advantage function) в момент времени $t$.  Функция преимущества оценивает, насколько действие $a_t$ лучше, чем среднее действие в состоянии $s_t$.  Существуют разные способы оценки функции преимущества, например, Generalized Advantage Estimation (GAE).
*   $clip(x, 1-\epsilon, 1+\epsilon)$: Функция *отсечения* (clipping).  Она ограничивает значение $x$ диапазоном $[1-\epsilon, 1+\epsilon]$.  В PPO параметр $\epsilon$ (обычно $\epsilon = 0.2$) определяет размер доверительной области.
*   $\epsilon$ (эпсилон): *Параметр отсечения*, определяющий размер доверительной области.

**Детальное пояснение:**

PPO стремится оптимизировать политику, максимизируя целевую функцию $J(\theta)$.  Ключевые особенности PPO:

1.  **Оптимизация политики напрямую:** PPO напрямую учит политику $\pi(a|s)$, которая определяет, как агент должен действовать в каждом состоянии.  В отличие от Q-Learning и DQN, которые учат Q-функцию, а затем выводят политику из Q-функции.
2.  **Ограничение размера шага обновления политики:**  PPO использует функцию отсечения в целевой функции, чтобы ограничить изменение политики на каждом шаге обновления.  Это гарантирует, что новая политика не слишком сильно отличается от старой политики, от которой были собраны данные.  Это делает обучение более стабильным и надежным.
3.  **Отношение вероятностей (Probability Ratio) $r_t(\theta)$:**  Отношение вероятностей измеряет, насколько изменилась вероятность действия при переходе от старой политики к новой.  Если отношение вероятностей слишком велико или слишком мало, функция отсечения ограничивает вклад этого перехода в целевую функцию.
4.  **Функция преимущества (Advantage Function) $\hat{A}_t$:**  Использование функции преимущества позволяет уменьшить дисперсию градиентов и ускорить обучение.  Функция преимущества показывает, насколько действие лучше, чем среднее действие в данном состоянии, что позволяет агенту фокусироваться на более выгодных действиях.

**Интуитивное понимание целевой функции PPO:**

Целевая функция PPO максимизирует ожидаемое преимущество, но при этом штрафует за слишком большие изменения в политике.  Функция $min( r_t(\theta) \hat{A}_t,  clip(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t )$ выбирает минимум между двумя значениями:

*   $r_t(\theta) \hat{A}_t$:  Неограниченное преимущество, умноженное на отношение вероятностей.  Если отношение вероятностей находится в пределах доверительной области $[1-\epsilon, 1+\epsilon]$, то используется это значение.
*   $clip(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t$:  Ограниченное преимущество.  Если отношение вероятностей выходит за пределы доверительной области, то используется это значение, которое ограничивает изменение политики.

**Связь с текстовым описанием:**

Текст описывает PPO как алгоритм, фокусирующийся на обучении политики и позволяющий агенту адаптироваться к изменениям в среде. Математическая формализация показывает, как PPO оптимизирует политику, используя целевую функцию с отсечением, чтобы обеспечить стабильность и эффективность обучения.  Оптимизация политики напрямую позволяет агенту более гибко адаптироваться к сложным и меняющимся средам.

## Final Summary

### **Сводка текста**

В данной статье рассматриваются основные компоненты Марковских процессов принятия решений (МДП), которые включают набор состояний, действий, функцию перехода состояния, функцию вознаграждения, коэффициент дисконтирования и политику. МДП представляет собой математическую модель, описывающую, как агент принимает решения в среде, где результаты его действий зависят от текущего состояния. Основные формулы, такие как функция ценности состояния и функция ценности действия, помогают агенту оценивать свои действия и выбирать оптимальные стратегии.

Далее обсуждается свойство марковости, которое утверждает, что будущее состояние зависит только от текущего состояния и действия, что упрощает вычисления. Приводится пример задачи с лабиринтом, где агент должен избегать запрещенных зон и получать награды за достижения.

Также рассматриваются Беллмановские уравнения, которые являются ключевым инструментом в обучении с подкреплением, позволяя формально определить оптимальную стратегию агента. Обсуждаются дискаунтирование будущих наград и горизонты, которые влияют на принятие решений агентом.

В заключение, статья описывает реализацию алгоритма обучения агента в среде, включая визуализацию действий агента и использование комплексных чисел для обработки сигналов. Обсуждаются идеи применения комплексных чисел в задачах обучения с подкреплением и обратная связь от участников семинара, что помогает адаптировать содержание обучения под их потребности.
