# Обучение с подкреплением для игры "Крестики-нолики"

## Описание проекта

Этот проект представляет собой реализацию алгоритма обучения с подкреплением (Reinforcement Learning) для игры "Крестики-нолики" на базе OpenAI Gym. Агент обучается оптимальной стратегии игры против различных типов оппонентов: случайного, основанного на правилах и минимакс-оппонента.

## Обоснование выбора задачи

Игра "Крестики-нолики" идеально подходит для демонстрации базовых принципов RL по следующим причинам:
- **Четкие правила и ограниченное пространство состояний** - позволяет сосредоточиться на алгоритмах RL;
- **Дискретные действия** - упрощает реализацию и понимание алгоритмов;
- **Простая визуализация** - делает процесс обучения наглядным;
- **Быстрое обучение** - позволяет получить результаты за короткое время;
- **Наличие оптимальной стратегии** - позволяет оценить эффективность обучения.

## Требования к окружению

- Python 3.7+
- OpenAI Gym 0.21.0+
- NumPy 1.19.0+
- Matplotlib 3.3.0+
- Seaborn
- Pandas
- tqdm

## Установка

```bash
# Клонирование репозитория
git clone https://github.com/Verbasik/tic-tac-toe-rl.git
cd tic-tac-toe-rl

# Создание виртуального окружения (опционально)
python -m venv venv
source venv/bin/activate  # Для Linux/Mac
venv\Scripts\activate     # Для Windows

# Установка зависимостей
pip install -r requirements.txt
```

## Структура проекта

```
tic_tac_toe_rl/
├── environment/
│   ├── __init__.py
│   ├── tic_tac_toe_env.py       # Реализация среды
│   └── opponent.py              # Реализация оппонентов
├── agents/
│   ├── __init__.py
│   ├── random_agent.py          # Случайный агент
│   └── q_learning_agent.py      # Q-learning агент
├── utils/
│   ├── __init__.py
│   ├── visualization.py         # Визуализация
│   └── metrics.py               # Оценка производительности
├── training/
│   ├── __init__.py
│   └── train.py                 # Скрипт для обучения
├── models/                      # Директория для сохранения моделей
├── results/                     # Директория для сохранения результатов
├── main.py                      # Основной скрипт
├── requirements.txt             # Зависимости
└── README.md                    # Документация
```

## Использование

### Обучение агента

```bash
python main.py --mode train --opponent random --episodes 10000
```

Параметры:
- `--mode`: Режим работы (`train`, `evaluate`, `play`, `analyze`)
- `--opponent`: Тип оппонента (`random`, `rule_based`, `minimax`)
- `--episodes`: Количество эпизодов для обучения
- `--learning-rate`: Скорость обучения (по умолчанию 0.1)
- `--discount-factor`: Коэффициент дисконтирования (по умолчанию 0.9)
- `--exploration-rate`: Начальная вероятность исследования (по умолчанию 1.0)
- `--exploration-decay`: Коэффициент снижения вероятности исследования (по умолчанию 0.995)
- `--exploration-min`: Минимальная вероятность исследования (по умолчанию 0.01)
- `--render`: Флаг для включения визуализации

### Оценка агента

```bash
python main.py --mode evaluate --opponent rule_based --model-path models/q_agent_final.pkl
```

### Игра против обученного агента

```bash
python main.py --mode play --model-path models/q_agent_final.pkl
```

### Анализ обученного агента

```bash
python main.py --mode analyze --model-path models/q_agent_final.pkl
```

## Компоненты системы

### Среда (TicTacToeEnv)

Среда представляет собой игровое поле 3x3 и реализует интерфейс OpenAI Gym. Ключевые методы:
- `reset()`: Сброс среды в начальное состояние
- `step(action)`: Выполнение хода агента и ответного хода оппонента
- `render(mode)`: Визуализация игрового поля

### Оппоненты

Реализованы три типа оппонентов:
1. **RandomOpponent**: Выбирает случайное доступное действие
2. **RuleBasedOpponent**: Использует простые правила для выбора хода:
   - Завершает свои выигрышные комбинации
   - Блокирует выигрышные комбинации агента
   - При отсутствии очевидных ходов действует по приоритету: центр → углы → стороны
3. **MinimaxOpponent**: Использует алгоритм минимакс для оптимальной игры

### Агенты

1. **RandomAgent**: Агент, выбирающий случайные действия
2. **QLearningAgent**: Агент на основе алгоритма Q-learning

### Функции обучения и оценки

- `train_agent()`: Обучение агента через взаимодействие со средой
- `evaluate_agent()`: Оценка производительности агента
- `plot_training_stats()`: Визуализация статистики обучения

### Функции визуализации и анализа

- `plot_q_values_heatmap()`: Визуализация Q-значений для текущего состояния
- `visualize_game()`: Визуализация текущего состояния игры
- `plot_q_distribution()`: Визуализация распределения Q-значений
- `animate_game()`: Анимация игры с использованием обученного агента
- `compare_agents()`: Сравнение производительности нескольких агентов
- `analyze_opponent_types()`: Анализ производительности агента против различных типов оппонентов
- `plot_first_move_heatmap()`: Визуализация предпочтений агента для первого хода
- `learning_curve_analysis()`: Анализ кривой обучения агента

## Алгоритм Q-learning

Алгоритм Q-learning использует следующие параметры:
- Скорость обучения (learning_rate): Определяет, насколько быстро обновляются Q-значения
- Коэффициент дисконтирования (discount_factor): Определяет важность будущих наград
- Вероятность исследования (exploration_rate): Определяет вероятность выбора случайного действия
- Коэффициент снижения вероятности исследования (exploration_decay): Определяет скорость снижения вероятности исследования
- Минимальная вероятность исследования (exploration_min): Определяет минимальную вероятность исследования

Формула обновления Q-значений:
```
Q(s,a) = Q(s,a) + learning_rate * (reward + discount_factor * max(Q(s')) - Q(s,a))
```

## Результаты обучения

После обучения агент достигает следующих результатов:
- Против случайного оппонента: >90% побед
- Против оппонента на основе правил: >70% побед, >20% ничьих
- Против минимакс-оппонента: >90% ничьих

## Возможные улучшения

1. **Расширенные алгоритмы обучения**:
   - Реализация SARSA
   - Реализация Deep Q-Network (DQN) с использованием нейронных сетей

2. **Самообучение**:
   - Реализация обучения агента через игру против самого себя
   - Сравнение эффективности различных схем обучения

3. **Расширение на другие игры**:
   - Адаптация фреймворка для игр Connect Four, Nim и др.
   - Исследование масштабируемости алгоритмов RL

---

# Workflow процесс обучения Q-learning агента для игры "Крестики-нолики"

## 1. Подготовка компонентов системы

### 1.1. Создание среды игры
Для обучения агента предоставляется игровая среда на базе OpenAI Gym:
- Метод `reset()` - инициализация новой игры;
- Метод `step(action)` - выполнение действия и возврат нового состояния, награды, флаг завершения игры;
- Метод `render()` - визуализация текущего состояния игры.

### 1.2. Инициализация агента
```python
from q_learning_agent import QLearningAgent

# Инициализация Q-learning агента
agent = QLearningAgent(
    action_space=list(range(9)),  # Действия от 0 до 8 (клетки игрового поля)
    learning_rate=0.1,            # Скорость обучения (альфа)
    discount_factor=0.9,          # Коэффициент дисконтирования (гамма)
    exploration_rate=1.0,         # Начальная вероятность исследования (эпсилон)
    exploration_decay=0.995,      # Коэффициент снижения вероятности исследования
    exploration_min=0.01          # Минимальная вероятность исследования
)
```

### 1.3. Выбор оппонента
```python
from opponent import RandomOpponent, RuleBasedOpponent, MinimaxOpponent

# Выбор типа оппонента (от простого к сложному)
opponent = RandomOpponent()     # Случайные ходы
opponent = RuleBasedOpponent()  # Оппонент на основе правил
opponent = MinimaxOpponent()    # Оппонент на основе минимакс алгоритма
```

## 2. Процесс обучения

### 2.1. Настройка параметров обучения
```python
from train_agent import train_agent

# Настройка параметров обучения
n_episodes = 10000           # Общее количество эпизодов обучения
evaluate_every = 100         # Частота оценки производительности
n_eval_episodes = 100        # Количество эпизодов для оценки
save_dir = './models'        # Директория для сохранения моделей
save_every = 1000            # Частота сохранения модели
render_training = False      # Флаг отображения процесса обучения
```

### 2.2. Запуск процесса обучения
```python
# Запуск процесса обучения
training_stats = train_agent(
    env=env,
    agent=agent,
    n_episodes=n_episodes,
    evaluate_every=evaluate_every,
    n_eval_episodes=n_eval_episodes,
    save_dir=save_dir,
    save_every=save_every,
    render_training=render_training
)
```

## 3. Анализ и визуализация результатов

### 3.1. Визуализация статистики обучения
```python
from train_agent import plot_training_stats

# Визуализация результатов обучения
plot_training_stats(
    stats=training_stats,
    save_path='training_results.png'
)
```

## 4. Подробный алгоритм обучения Q-learning агента

### 4.1. Инициализация 
- Создание пустой Q-таблицы (словарь, где ключами являются строковые представления состояний, а значениями - массивы Q-значений для каждого действия);
- Установка начальных параметров обучения (скорость обучения, коэффициент дисконтирования, вероятность исследования).

### 4.2. Цикл обучения
Для каждого эпизода:
1. Сброс среды и получение начального состояния;
2. Цикл до завершения эпизода:
   - Определение допустимых действий (пустые клетки);
   - Выбор действия агентом (с использованием ε-greedy стратегии):
     - С вероятностью ε выбрать случайное действие (исследование);
     - С вероятностью (1-ε) выбрать действие с максимальным Q-значением (использование).
   - Выполнение действия в среде и получение нового состояния, награды и флага завершения;
   - Обновление Q-значения по формуле:
     $$
     Q(s,a) = Q(s,a) + α * (r + γ * max(Q(s',a')) - Q(s,a))
     $$
     где:
     - Q(s,a) - текущее Q-значение для состояния s и действия a;
     - α - скорость обучения;
     - r - полученная награда;
     - γ - коэффициент дисконтирования;
     - max(Q(s',a')) - максимальное Q-значение для следующего состояния.
   - Переход к новому состоянию;
3. Уменьшение вероятности исследования (ε) после каждого эпизода.

### 4.3. Периодическая оценка и сохранение модели
- Через каждые `evaluate_every` эпизодов:
  - Оценка текущей производительности агента на `n_eval_episodes` эпизодах;
  - Сохранение статистики (процент побед, ничьих, поражений);
  - Вывод прогресса обучения.
- Через каждые `save_every` эпизодов:
  - Сохранение текущего состояния Q-таблицы.

## 5. Внутренний процесс обучения агента

### 5.1. Выбор действия (метод `choose_action`)
- Преобразование состояния игрового поля в строковый ключ для Q-таблицы;
- Инициализация Q-значений для нового состояния, если его нет в таблице;
- Применение ε-greedy стратегии:
  - Если сгенерированное случайное число < ε, выбрать случайное действие;
  - Иначе выбрать действие с максимальным Q-значением среди допустимых.

### 5.2. Обновление Q-значений (метод `learn`)
- Получение ключей для текущего и следующего состояний;
- Инициализация Q-значений для новых состояний;
- Определение максимального Q-значения для следующего состояния;
- Обновление Q-значения для текущего состояния и выбранного действия по формуле Q-learning;
- Уменьшение вероятности исследования, если эпизод завершен.

## 6. Особенности реализации

### 6.1. Представление состояний
- Состояние игрового поля (матрица 3x3) преобразуется в одномерный список и затем в строку;
- Это позволяет использовать строковые ключи для индексации Q-таблицы.

### 6.2. Обработка недопустимых действий
- Для недопустимых действий (занятые клетки) устанавливается Q-значение -∞;
- Это гарантирует, что агент никогда не выберет недопустимое действие.

### 6.3. Управление исследованием
- Вероятность исследования (ε) начинается с высокого значения (обычно 1.0);
- После каждого эпизода она умножается на коэффициент снижения;
- Это обеспечивает баланс между исследованием и использованием знаний.

Этот процесс позволяет агенту постепенно изучать оптимальную стратегию игры через взаимодействие со средой и постоянное обновление своих оценок ценности различных действий в различных состояниях игры.