# **ОТЧЕТ ПО ПРОЕКТУ**

## **Тема: реализация и анализ алгоритма Proximal Policy Optimization (PPO) на задаче навигации в сеточной среде**

### **1. Введение с описанием цели проекта**

Обучение с подкреплением (Reinforcement Learning, RL) является одной из наиболее динамично развивающихся областей машинного обучения, позволяющей агентам обучаться оптимальному поведению в сложных средах путем взаимодействия и получения обратной связи в виде наград или штрафов. Среди множества RL-алгоритмов, Proximal Policy Optimization (PPO) выделяется своей эффективностью, стабильностью и относительной простотой реализации, что делает его популярным выбором для широкого круга задач.

**Цель данного проекта** – реализовать алгоритм PPO с нуля, применить его для обучения агента навигации в кастомной сеточной среде ("Grid World") и провести анализ его производительности. Проект направлен на демонстрацию понимания принципов работы PPO, включая архитектуру актор-критик, использование обобщенной оценки преимущества (GAE), механизма отсечения (clipping) для стабилизации обучения, а также роли энтропийного бонуса.

Основные задачи проекта:
1.  Разработать и реализовать симуляционную среду "Grid World".
2.  Реализовать компоненты алгоритма PPO: нейросетевые модели актора и критика, функцию расчета GAE, и основной цикл обновления PPO.
3.  Провести серию экспериментов по обучению агента в созданной среде.
4.  Собрать, визуализировать и проанализировать метрики обучения: кривые средней награды, длины эпизода, потерь актора и критика, а также энтропии политики.
5.  Оценить качество обученной политики визуально.

### **2. Обзор литературы**

Обучение с подкреплением (RL) – это парадигма машинного обучения, в которой агент учится принимать последовательность решений (действий) в некоторой среде для максимизации кумулятивной награды. Ключевыми элементами RL являются:
*   **Агент (Agent):** сущность, принимающая решения.
*   **Среда (Environment):** внешний мир, с которым взаимодействует агент.
*   **Состояние (State, $S$):** описание текущей ситуации в среде.
*   **Действие (Action, $A$):** выбор, который может сделать агент.
*   **Награда (Reward, $R$):** сигнал обратной связи от среды, оценивающий "хорошесть" действия в данном состоянии.
*   **Политика (Policy, $\pi$):** стратегия агента, отображающая состояния в действия ($\pi(a|s)$ – вероятность выбора действия $a$ в состоянии $s$).
*   **Функция ценности (Value Function, $V(s)$ или $Q(s,a)$):** ожидаемая кумулятивная награда из состояния $s$ (или при выполнении действия $a$ в состоянии $s$) при следовании определенной политике.

**Методы градиента политики (Policy Gradient Methods)**, такие как REINFORCE, напрямую оптимизируют параметризованную политику, двигаясь в направлении градиента ожидаемой награды. Однако они часто страдают от высокой дисперсии оценок градиента, что замедляет и дестабилизирует обучение.

**Актор-Критик (Actor-Critic) методы** комбинируют преимущества методов градиента политики и методов, основанных на функции ценности. "Актор" отвечает за выбор действий (политика), а "Критик" оценивает эти действия (функция ценности), предоставляя более стабильный сигнал для обучения актора. Примерами являются A2C (Advantage Actor-Critic) и A3C (Asynchronous Advantage Actor-Critic).

**Proximal Policy Optimization (PPO)** [Schulman et al., 2017] является развитием идей Trust Region Policy Optimization (TRPO), но с более простой реализацией. PPO стремится делать наибольший возможный шаг по улучшению политики, не выходя при этом слишком далеко от старой политики, что может привести к коллапсу производительности. Это достигается за счет введения "отсеченной" (clipped) суррогатной целевой функции:

$$ L^{CLIP}(\theta) = \mathbb{E}_t \left[ \min\left(r_t(\theta) A_t, \text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) A_t\right) \right] $$

где $r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$ – отношение вероятностей новой и старой политики, $A_t$ – оценка преимущества (часто используется Generalized Advantage Estimation, GAE), а $\epsilon$ – гиперпараметр отсечения (обычно 0.1-0.2).  
GAE [Schulman et al., 2015] предоставляет эффективный способ оценки преимущества, балансируя между смещением и дисперсией:

$$ A_t^{GAE} = \sum (\gamma\lambda)^l \delta_{t+l} $$

где $\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$ – ошибка временных различий (TD error), $\gamma$ – фактор дисконтирования, $\lambda$ – параметр GAE.

PPO также часто включает в функцию потерь штраф за ошибку оценки ценности состояния (для критика) и бонус за энтропию политики (для поощрения исследования):

$$ L(\theta) = L^{CLIP}(\theta) - c_1 L^{VF}(\theta) + c_2 S[\pi_\theta](s_t) $$

где $L^{VF}$ – ошибка квадрата среднего для критика, $S$ – энтропия, $c_1$ и $c_2$ – коэффициенты.

### **3. Описание методологии**

#### **3.1. Среда (GridEnvironment)**

Для экспериментов была реализована кастомная сеточная среда `GridEnvironment` размером 10x10 клеток:
*   **Состояние:** координаты агента (строка, столбец), нормализованные в диапазон [0, 1] путем деления на (размер-1). Размерность вектора состояния – 2.
*   **Действия:** 4 дискретных действия: 0 (вверх), 1 (вниз), 2 (влево), 3 (вправо).
*   **Начальное состояние:** (0, 0).
*   **Целевое состояние:** (9, 9).
*   **Награды:**
    *   `+10.0` за достижение цели.
    *   `-1.0` за столкновение со стеной (агент остается на месте).
    *   `-0.1` за каждый выполненный шаг (штраф за время).
    *   `0.0` если агент пытается сделать шаг, уже находясь в целевой точке.
*   **Завершение эпизода:** эпизод завершается при достижении цели или по истечении максимального числа шагов (`MAX_STEPS_PER_EPISODE_PPO`).

#### **3.2. Агент и Алгоритм PPO**

Агент реализован с использованием архитектуры актор-критик.

*   **Актор (PolicyNetwork):**
    *   Многослойный перцептрон (MLP) с двумя скрытыми слоями по 128 нейронов и функцией активации ReLU.
    *   Вход: нормализованное состояние (2 нейрона).
    *   Выход: логиты для 4 действий, которые преобразуются в категориальное распределение вероятностей действий.
*   **Критик (ValueNetwork):**
    *   Аналогичная MLP архитектура (2 скрытых слоя по 128 нейронов, ReLU).
    *   Вход: нормализованное состояние (2 нейрона).
    *   Выход: одно скалярное значение – оценка ценности текущего состояния (V(s)).

#### **Процесс обучения PPO:**

1.  **Сбор данных (Rollout):** агент взаимодействует со средой на протяжении `STEPS_PER_ITERATION_PPO` шагов, используя текущую политику актора. Собираются траектории: состояния, действия, награды, логарифмы вероятностей действий (от старой политики), оценки ценности состояний (от критика) и флаги завершения эпизода.
2.  **Расчет преимуществ (Advantages) и целевых значений для критика (Returns-to-go):**
    *   Для каждого шага в собранном батче вычисляется оценка преимущества с помощью GAE:
        `advantages_tensor = compute_gae(rewards, values, next_values, dones, gamma=GAMMA_PPO, lambda_gae=GAE_LAMBDA_PPO, standardize=STANDARDIZE_ADV_PPO)`
    *   Целевые значения для обучения критика (returns-to-go) вычисляются как: `returns_to_go_tensor = advantages_tensor + values_tensor`.
3.  **Обновление сетей:** в течение `PPO_EPOCHS` эпох на собранном батче данных производятся обновления параметров актора и критика:
    *   **Потери актора (Policy Loss):**
        `ratio = exp(log_probs_new - log_probs_old)`
        `surr1 = ratio * advantages`
        `surr2 = clamp(ratio, 1 - PPO_CLIP_EPSILON, 1 + PPO_CLIP_EPSILON) * advantages`
        `policy_loss = -min(surr1, surr2).mean() - ENTROPY_COEFF * entropy.mean()`
    *   **Потери критика (Value Loss):**
        `value_loss = MSE(critic(states), returns_to_go)`
    *   **Общая потеря (для оптимизации):** оптимизаторы актора и критика минимизируют свои соответствующие функции потерь (с учетом `VALUE_LOSS_COEFF` для потерь критика).
4.  Шаги 1-3 повторяются `NUM_ITERATIONS_PPO` раз.

**3.3. Гиперпараметры**
*   `GAMMA_PPO`: 0.99 (коэффициент дисконтирования)
*   `GAE_LAMBDA_PPO`: 0.95 (параметр λ для GAE)
*   `PPO_CLIP_EPSILON`: 0.2 (эпсилон для отсечения PPO)
*   `ACTOR_LR`: 3e-4 (скорость обучения актора)
*   `CRITIC_LR_PPO`: 1e-3 (скорость обучения критика)
*   `PPO_EPOCHS`: 10 (количество эпох PPO на одну итерацию)
*   `VALUE_LOSS_COEFF`: 0.5 (вес потерь критика)
*   `ENTROPY_COEFF`: 0.01 (вес энтропийного бонуса)
*   `STANDARDIZE_ADV_PPO`: True (стандартизация преимуществ)
*   `NUM_ITERATIONS_PPO`: 150 (число итераций обучения)
*   `STEPS_PER_ITERATION_PPO`: 1000 (шагов для сбора данных на итерацию)
*   `MAX_STEPS_PER_EPISODE_PPO`: 200 (максимальная длина эпизода)

**3.4. Инструменты**
*   Язык программирования: Python 3.
*   Библиотеки: PyTorch (для нейронных сетей и автоматического дифференцирования), NumPy (для численных операций), Matplotlib (для визуализации).
*   Устройство для вычислений: `DEVICE` (CUDA, если доступно, иначе CPU).

### **4. Результаты экспериментов**

Обучение проводилось на протяжении 150 итераций. Ниже представлены графики ключевых метрик (см. предоставленное изображение "PPO Custom Grid Plots"):

*   **PPO Custom Grid: Avg Ep Reward / Iteration (Средняя награда за эпизод):**
    Наблюдается быстрый рост средней награды за эпизод. Уже примерно к 20-й итерации награда достигает значения около 7.0-7.5 и далее стабилизируется в районе 8.0. Это свидетельствует об эффективном обучении агента находить путь к цели. Скользящее среднее (10-iter MA) подтверждает эту тенденцию и сглаживает флуктуации.

*   **PPO Custom Grid: Avg Ep Length / Iteration (Средняя длина эпизода):**
    Коррелируя с ростом награды, средняя длина эпизода быстро уменьшается. С начальных значений (более 40-60 шагов, судя по быстрому падению оранжевой линии MA) она снижается до примерно 18 шагов к 20-30 итерации и остается на этом уровне. Минимально возможная длина пути в сетке 10x10 из (0,0) в (9,9) составляет 18 шагов (9 шагов вправо + 9 шагов вниз). Это означает, что агент научился находить оптимальный или близкий к оптимальному путь.

*   **PPO Custom Grid: Avg Value Loss / Iteration (Средняя ошибка критика):**
    Ошибка критика (MSE Loss) после начального периода стабилизируется и колеблется в диапазоне примерно 0.9-1.05. Отсутствие расхождения или резкого роста потерь указывает на то, что критик адекватно обучается предсказывать ценность состояний.

*   **PPO Custom Grid: Avg Policy Objective / Iteration (Средняя целевая функция политики):**
    На графике показана целевая функция политики (Policy Loss, но со знаком минус, так как мы максимизируем эту функцию). Наблюдается устойчивый рост этого значения (что соответствует уменьшению потерь политики), особенно на начальных этапах обучения. Это говорит об успешной оптимизации политики актором.

*   **PPO Custom Grid: Avg Policy Entropy / Iteration (Средняя энтропия политики):**
    Энтропия политики плавно уменьшается на протяжении всего процесса обучения, с начальных значений около 1.3-1.4 до примерно 0.3 к концу. Это ожидаемое поведение: в начале обучения политика более стохастична (высокая энтропия, больше исследования), а по мере нахождения оптимальных действий она становится более детерминированной (низкая энтропия, больше эксплуатации). Плавное снижение без резких падений говорит о том, что агент не "застревает" преждевременно в субоптимальной политике.

**Вывод консоли во время обучения:**
```
Iter 10/150 | Avg Reward: 6.36 | Avg Len: 21.3 | P_Loss: -0.0145 | V_Loss: 0.9804 | Entropy: 0.8628
...
Iter 150/150 | Avg Reward: 8.07 | Avg Len: 17.9 | P_Loss: -0.0067 | V_Loss: 1.0051 | Entropy: 0.2982
```
Данные из консоли подтверждают наблюдения по графикам: рост средней награды, уменьшение средней длины эпизода до близкой к оптимальной, стабилизация потерь и постепенное снижение энтропии.

**Визуализация обученной политики (PPO Learned Policy - Most Likely Action):**
(См. предоставленное изображение "PPO Learned Policy Grid")
На сетке отображены наиболее вероятные действия, выбираемые обученным актором в каждом состоянии. Видно, что агент выучил осмысленную стратегию:
*   В большинстве клеток верхней левой части сетки агент стремится двигаться вправо.
*   Приближаясь к правому краю или находясь в правой части сетки, агент начинает двигаться вниз.
*   Вблизи целевой клетки 'G' (правый нижний угол) действия четко направлены к цели.
Политика выглядит логичной и ведет агента из начальной точки (0,0) к цели (9,9) по одному из оптимальных путей.

### **5. Обсуждение и выводы**

**5.1. Обсуждение результатов**
Результаты экспериментов демонстрируют успешное применение алгоритма PPO для решения задачи навигации в сеточной среде.

*   **Эффективность обучения:** Агент быстро (в течение первых 20-30 итераций) научился достигать цели, что видно по резкому росту средней награды и снижению длины эпизода. Достигнутая средняя награда (около 8.0) близка к максимально возможной для оптимального пути (10 (цель) - 0.1 * 18 (шаги) = 10 - 1.8 = 8.2). Небольшое расхождение может быть связано со случайными ошибками на ранних этапах эпизодов, которые все еще усредняются, или с тем, что не все эпизоды в батче идеальны.
*   **Стабильность:** Алгоритм PPO показал стабильное обучение. Кривые обучения не демонстрируют резких провалов или расхождений, что является одним из ключевых преимуществ PPO, достигаемым за счет механизма отсечения (clipping) суррогатной функции потерь.
*   **Работа критика:** Потери критика оставались в разумных пределах, указывая на то, что он успешно обучался оценивать состояния. Это важно, так как качественные оценки преимущества (GAE), зависящие от критика, являются основой для обновления политики актора.
*   **Оптимизация политики:** Целевая функция политики актора улучшалась, что говорит об успешной оптимизации стратегии агента.
*   **Исследование vs. Эксплуатация:** Плавное снижение энтропии политики указывает на здоровый баланс между исследованием новых действий на ранних этапах и эксплуатацией уже известных хороших стратегий на более поздних этапах. Коэффициент энтропии `ENTROPY_COEFF = 0.01` способствовал поддержанию исследования.
*   **Качество итоговой политики:** Визуализация политики подтверждает, что агент выучил эффективный путь к цели.

По сравнению с более простыми методами градиента политики (например, REINFORCE, который часто требует значительно большего числа итераций и демонстрирует высокую волатильность), PPO показал себя как более эффективный и стабильный алгоритм. Использование GAE и многократных обновлений на одном батче данных (PPO_EPOCHS) также способствует повышению эффективности использования собранных данных по сравнению с базовыми on-policy методами.

**5.2. Выводы**
В ходе данного проекта был успешно реализован и протестирован алгоритм Proximal Policy Optimization (PPO) для задачи навигации в кастомной сеточной среде.
1.  Разработана среда `GridEnvironment`, адекватно моделирующая поставленную задачу.
2.  Реализованы все ключевые компоненты PPO, включая нейросетевые модели актора и критика, вычисление GAE и механизм обновления PPO с отсечением.
3.  Эксперименты показали, что агент, обученный с помощью PPO, способен быстро и стабильно находить оптимальную или близкую к оптимальной стратегию поведения в среде.
4.  Анализ метрик обучения (награда, длина эпизода, потери, энтропия) подтвердил эффективность и стабильность алгоритма.
5.  Визуализация выученной политики продемонстрировала ее осмысленность и способность приводить агента к цели.

Таким образом, цель проекта достигнута: продемонстрировано понимание и практическое применение алгоритма PPO. PPO подтвердил свою репутацию как мощный, стабильный и относительно простой в настройке алгоритм обучения с подкреплением, подходящий для решения широкого спектра задач.

**5.3. Рекомендации по улучшению и дальнейшая работа**
1.  **Эксперименты с гиперпараметрами:** Провести более детальное исследование влияния ключевых гиперпараметров (например, `PPO_CLIP_EPSILON`, `ENTROPY_COEFF`, `GAE_LAMBDA_PPO`, скорости обучения) на производительность.
2.  **Масштабирование наград/потерь:** Проверить, как нормализация наград или использование других техник масштабирования может повлиять на стабильность и скорость обучения, особенно для более сложных сред.
3.  **Тестирование на более сложных средах:** Применить реализованный алгоритм к более сложным конфигурациям сеточной среды (например, с препятствиями, большим размером) или к стандартным средам из OpenAI Gym.
4.  **Сравнение с другими алгоритмами:** Реализовать и сравнить PPO с другими RL алгоритмами (например, DQN, A2C, SAC) на той же задаче.
5.  **Архитектура сетей:** Экспериментировать с различными архитектурами нейронных сетей для актора и критика (например, количество слоев, нейронов, использование общих слоев).