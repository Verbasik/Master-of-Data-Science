# Оглавление

I. **Введение в нейронные сети**
   *   Создание двухслойной нейросети.
   *   Параметры сети: веса и смещения.
   *   Математическая формализация вычислений в нейросети.
   *   Физический и геометрический смысл нейросетей.

II. **Упрощение создания нейросети с использованием Keras**
   *   Основные шаги при использовании Keras.
   *   Математическая формализация в Keras.
   *   Пример кода с Keras.
   *   Структура модели и параметры в Keras.
   *   Способы описания модели в Keras.

III. **Компиляция и обучение модели в Keras**
   *   Определение оптимизатора, функции потерь и метрик.
   *   Процесс обучения модели.

IV. **Визуализация результатов обучения нейросети**
   *   Построение графиков функции потерь и точности.
   *   Анализ графиков для выявления переобучения.

V. **Практическое применение: Датасет MNIST**
   *   Загрузка и подготовка датасета MNIST в Keras.
   *   Нормализация данных.
   *   Обучение однослойной нейросети на датасете MNIST.
   *   Компиляция и обучение с использованием one-hot encoding.
   *   Оценка производительности модели на MNIST.
   *   Использование матрицы ошибок и sparse categorical cross-entropy.
   *   Визуализация весов модели.

VI. **Важность нормализации данных**
   *   Влияние нормализации на обучение и точность вычислений.
   *   Нормализация данных и динамические графы вычислений.

VII. **Сравнение фреймворков для глубокого обучения**
   *   TensorFlow и Keras.
   *   PyTorch.
   *   JAX.
   *   Преимущества и недостатки каждого фреймворка.

# Введение

Данная лекция охватывает основные аспекты создания и обучения нейронных сетей, начиная с простых однослойных моделей и заканчивая более сложными двухслойными архитектурами. **Особое внимание уделяется параметрам сети, таким как матрицы весов ($w$) и векторы смещения ($b$), которые играют ключевую роль в преобразовании входных данных и определении выходных значений сети**. Кроме того, подчеркивается важность математической формализации вычислений, лежащих в основе работы нейросетей, включая уравнения для вычисления промежуточных и выходных значений, а также применение функций активации, таких как гиперболический тангенс и сигмоидная функция.

Далее, в лекции рассматриваются инструменты и методы для упрощения процесса разработки нейросетей, в частности, использование библиотеки Keras. **Keras позволяет абстрагироваться от низкоуровневых деталей реализации и сосредоточиться на архитектуре модели, что значительно ускоряет процесс прототипирования и обучения**. Будут рассмотрены основные шаги при использовании Keras, включая определение входного слоя, добавление полносвязных слоев и создание модели, а также способы описания структуры модели и визуализации ее параметров.

В заключительной части введения будет затронута **важность нормализации входных данных для улучшения обучения нейросетей, а также сравнение различных фреймворков для глубокого обучения, таких как TensorFlow, PyTorch и JAX**. Будут рассмотрены преимущества и недостатки каждого фреймворка, а также их особенности в контексте создания и обучения нейросетей. Особое внимание будет уделено функциям потерь, таким как categorical cross-entropy и sparse categorical cross-entropy, а также методам визуализации, включая матрицы ошибок и веса нейросетей, для анализа производительности моделей и внесения необходимых корректировок.

# Глоссарий терминов для лекции по искусственному интеллекту:

*   **Нейронная сеть** — это вычислительная модель, состоящая из взаимосвязанных узлов, называемых нейронами, организованных в слои, которые используются для машинного обучения и распознавания образов.
*   **Двухслойная нейросеть** — нейронная сеть, состоящая из входного слоя, одного скрытого слоя и выходного слоя, где каждый слой преобразует входные данные и передает их следующему слою.
*   **Веса ($w$)** — параметры нейронной сети, определяющие силу связи между нейронами в разных слоях. В двухслойной сети используются матрицы весов $w_1$ для первого слоя и $w_2$ для второго слоя.
*   **Смещение ($b$)** — параметры нейронной сети, добавляемые к взвешенной сумме входов нейрона. В двухслойной сети используются векторы смещения $b_1$ для первого слоя и $b_2$ для второго слоя.
*   **Матрица ошибок (confusion matrix)** — таблица, используемая для оценки производительности модели классификации, показывающая количество правильных и неправильных предсказаний для каждого класса.
*   **Функция активации** — функция, применяемая к выходу нейрона для введения нелинейности в модель. Примеры включают гиперболический тангенс ($\tanh$) и сигмоидную функцию ($\sigma$).
*   **Сигмоидная функция ($\sigma$)** — функция активации, которая сжимает значения в диапазон от 0 до 1, часто используется в задачах бинарной классификации.
*   **Гиперболический тангенс ($\tanh$)** — функция активации, которая сжимает значения в диапазон от -1 до 1.
*   **Keras** — это высокоуровневая библиотека для создания нейросетей, упрощающая процесс разработки моделей.
*   **Полносвязный слой (Dense layer)** — слой нейронной сети, в котором каждый нейрон связан со всеми нейронами предыдущего слоя.
*   **Размерность слоя** — количество нейронов в слое нейронной сети.
*   **Оптимизатор** — алгоритм, используемый для обновления весов модели на основе градиентного спуска. Примеры включают Adam и SGD (Stochastic Gradient Descent).
*   **Функция потерь** — функция, измеряющая, насколько хорошо модель предсказывает целевые значения.
*   **Метрика** — функция, используемая для оценки производительности модели во время обучения. Например, accuracy (точность).
*   **Binary crossentropy** — функция потерь, используемая для задач бинарной классификации, где на выходе используется сигмоидная функция.
*   **Categorical cross-entropy** — функция потерь, используемая для задач многоклассовой классификации, когда метки классов представлены в формате one-hot encoding.
*   **Sparse categorical cross-entropy** — функция потерь, используемая для задач многоклассовой классификации, когда метки классов представлены в виде целых чисел.
*   **One-hot encoding** — представление категориальных данных в виде векторов, где только один элемент равен 1, а остальные равны 0.
*   **Нормализация данных** — процесс приведения значений входных данных к одному диапазону (например, от 0 до 1), что помогает улучшить обучение нейросети.
*   **Переобучение** — ситуация, когда модель начинает запоминать данные, а не учиться обобщать, что приводит к ухудшению производительности на новых данных.
*   **Динамический граф вычислений** — подход, при котором граф вычислений создается на лету, что позволяет более гибко управлять вычислениями.
*    **Мантисса и порядок** - Представление чисел с плавающей точкой, где мантисса определяет значащие цифры числа, а порядок определяет его масштаб.
*   **Фреймворки для глубокого обучения** — программные инструменты, упрощающие создание и обучение нейросетей. Примеры включают TensorFlow, PyTorch и JAX.
*   **MNIST** — стандартный набор данных, состоящий из изображений рукописных цифр, используемый для тестирования алгоритмов машинного обучения.
*   **Softmax** — функция активации, используемая на выходном слое для преобразования выходных значений нейронов в вероятности для каждого класса.

---

# Summarization for Text

## Chunk 1

### **Название фрагмента: Создание двухслойной нейросети**

## **Двухслойная нейросеть**

Двухслойная нейросеть состоит из входного слоя, одного или нескольких скрытых слоев и выходного слоя. В данном случае мы добавляем один скрытый слой с несколькими нейронами. Основная идея заключается в том, что каждый слой преобразует входные данные, передавая их на следующий слой.

### Параметры сети

Для двухслойной нейросети нам понадобятся следующие параметры:
- $w_1$: матрица весов для первого слоя, которая преобразует входные данные в скрытые нейроны.
- $b_1$: вектор смещения для первого слоя.
- $w_2$: матрица весов для второго слоя, которая преобразует выходы скрытого слоя в выходные данные.
- $b_2$: вектор смещения для выходного слоя.

Размерности этих параметров будут следующими:
- $w_1$ будет иметь размерность $2 \times 5$, где 2 — количество входных параметров, а 5 — количество нейронов в скрытом слое.
- $b_1$ будет иметь размерность $5$, так как соответствует количеству нейронов в скрытом слое.
- $w_2$ будет иметь размерность $5 \times 1$, так как преобразует 5 выходов скрытого слоя в один выход.
- $b_2$ будет иметь размерность $1$.

### Математическая формализация

Процесс вычисления выходов нейросети можно описать следующими уравнениями:

1. Вычисление промежуточного значения $z_1$ для скрытого слоя:
$$
z_1 = x \cdot w_1 + b_1
$$
где:
- $x$ — входные данные,
- $w_1$ — матрица весов первого слоя,
- $b_1$ — вектор смещения первого слоя.

2. Применение активационной функции (например, гиперболического тангенса) к $z_1$ для получения выходов скрытого слоя:
$$
z_{h1} = \tanh(z_1)
$$

3. Вычисление выходного значения $z$ для выходного слоя:
$$
z = z_{h1} \cdot w_2 + b_2
$$

4. Применение сигмоидной функции к $z$ для получения окончательного выхода:
$$
y = \sigma(z)
$$
где $\sigma$ — сигмоидная функция.

### Пример кода

Теперь давайте рассмотрим пример кода, который реализует двухслойную нейросеть:

```python
from typing import Any
import tensorflow as tf

# Определяем параметры сети
input_size  = 2  # Количество входных параметров
hidden_size = 5  # Количество нейронов в скрытом слое
output_size = 1  # Количество выходных параметров

# Инициализация весов и смещений
w1 = tf.Variable(tf.random.normal([input_size, hidden_size]), name='weights1')  # Вес для первого слоя
b1 = tf.Variable(tf.zeros([hidden_size]), name='bias1')                         # Смещение для первого слоя
w2 = tf.Variable(tf.random.normal([hidden_size, output_size]), name='weights2') # Вес для второго слоя
b2 = tf.Variable(tf.zeros([output_size]), name='bias2')                         # Смещение для второго слоя

def neural_network(x: Any) -> Any:
    """
    Description:
        Определяет модель нейронной сети с двумя слоями.

    Args:
        x: Входные данные для нейронной сети.

    Returns:
        Выход нейронной сети после применения сигмоидной функции.

    Raises:
        None

    Examples:
        >>> x_input = tf.constant([[0.5, 1.0]], dtype=tf.float32)
        >>> output = neural_network(x_input)
        >>> print("Выход нейросети:", output.numpy())
        Выход нейросети: [[0.6224592]]
    """
    z1    = tf.matmul(x, w1) + b1     # Вычисляем z1
    z_h1  = tf.tanh(z1)               # Применяем активационную функцию
    z     = tf.matmul(z_h1, w2) + b2  # Вычисляем выход
    return tf.sigmoid(z)              # Применяем сигмоид для окончательного выхода

# Пример входных данных
x_input = tf.constant([[0.5, 1.0]], dtype=tf.float32)  # Входные данные
output  = neural_network(x_input)                      # Получаем выход
print("Выход нейросети:", output.numpy())
```

### Физический и геометрический смысл

Представьте, что вы пытаетесь предсказать, будет ли дождь на основе двух факторов: температуры и влажности. Входные данные (температура и влажность) представляют собой координаты в двумерном пространстве. Скрытый слой с 5 нейронами может быть представлен как 5 различных "гиперплоскостей", которые разделяют пространство на области, где дождь будет или не будет. Выходной слой затем принимает эти разделения и выдает вероятность дождя, используя сигмоидную функцию, которая сжимает значения в диапазон от 0 до 1.

Таким образом, двухслойная нейросеть позволяет более гибко моделировать сложные зависимости между входными данными и выходами, что делает ее мощным инструментом в задачах машинного обучения.

## Chunk 2

### **Название фрагмента: Использование Keras для создания нейросети**

**Предыдущий контекст:** В предыдущем фрагменте мы обсуждали, как создать двухслойную нейросеть с использованием TensorFlow, включая параметры весов и смещения. Теперь мы рассмотрим, как упростить процесс создания нейросети с помощью библиотеки Keras.

## **Упрощение создания нейросети с Keras**

Keras — это высокоуровневая библиотека для создания нейросетей, которая значительно упрощает процесс разработки моделей. Она позволяет быстро и удобно описывать архитектуру нейросети, не углубляясь в детали реализации каждого слоя.

### Основные шаги при использовании Keras

1. **Определение входного слоя:** Мы начинаем с определения входного слоя, указывая количество входных параметров. В нашем случае это 2.
   
   ```python
   input = tf.keras.Input(shape=(2,))
   ```

2. **Добавление полносвязного слоя:** Далее мы добавляем полносвязный слой (Dense layer), который будет иметь 5 нейронов и использовать гиперболический тангенс в качестве активационной функции. Keras автоматически определяет размерность весов на основе входного слоя.

   ```python
   z = tf.keras.layers.Dense(5, activation='tanh')(input)
   ```

3. **Добавление выходного слоя:** Затем мы добавляем выходной слой, который будет иметь 1 нейрон и использовать сигмоидную функцию активации.

   ```python
   p = tf.keras.layers.Dense(1, activation='sigmoid')(z)
   ```

4. **Создание модели:** После определения всех слоев мы создаем модель, связывая входные данные с выходными.

   ```python
   model = tf.keras.Model(inputs=input, outputs=p)
   ```

5. **Просмотр структуры модели:** Мы можем использовать метод `summary()`, чтобы увидеть структуру нашей модели и количество параметров.

   ```python
   model.summary()
   ```

### Пример кода с Keras

Вот пример кода, который реализует нейросеть с использованием Keras:

```python
import tensorflow as tf

# Определяем входной слой
input = tf.keras.Input(shape=(2,))  # Вход имеет 2 параметра

# Добавляем скрытый слой с 5 нейронами и активацией tanh
z = tf.keras.layers.Dense(5, activation='tanh')(input)

# Добавляем выходной слой с 1 нейроном и активацией sigmoid
p = tf.keras.layers.Dense(1, activation='sigmoid')(z)

# Создаем модель
model = tf.keras.Model(inputs=input, outputs=p)

# Выводим структуру модели
model.summary()
```

### Физический и геометрический смысл

Представьте, что вы хотите предсказать вероятность того, что определенное событие произойдет, основываясь на двух входных параметрах. Используя Keras, вы можете быстро создать модель, которая будет учитывать сложные зависимости между этими параметрами. Каждый слой нейросети может быть представлен как гиперплоскость, разделяющая пространство входных данных на области, соответствующие различным вероятностям выхода. Keras упрощает этот процесс, позволяя вам сосредоточиться на архитектуре модели, а не на деталях реализации. 

Таким образом, использование Keras позволяет значительно ускорить процесс разработки нейросетей, особенно когда речь идет о более сложных архитектурах, где требуется больше контроля над параметрами и функциями активации.

## Chunk 3

### **Название фрагмента: Описание структуры модели и параметров в Keras**

**Предыдущий контекст:** В предыдущем фрагменте мы рассмотрели, как использовать Keras для создания нейросети, включая определение входного слоя, скрытых слоев и выходного слоя. Теперь мы сосредоточимся на том, как описать структуру модели и параметры, а также на различных способах создания модели в Keras.

## **Структура модели и параметры в Keras**

При создании нейросети важно понимать, какова структура модели и сколько параметров она содержит. Это позволяет лучше контролировать обучение и оптимизацию модели. В Keras мы можем визуализировать структуру модели с помощью метода `summary()`, который показывает размеры слоев и количество параметров.

### Описание структуры модели

1. **Размерности слоев:** Каждый слой имеет определенные размерности, которые зависят от количества входных и выходных параметров. Например:
   - Входной слой имеет размерность $(\text{None}, 2)$, где "None" обозначает размер мини-батча, а "2" — количество входных параметров.
   - Первый скрытый слой имеет размерность $(\text{None}, 5)$, что означает, что он содержит 5 нейронов.
   - Выходной слой имеет размерность $(\text{None}, 1)$, так как он выдает одно значение.

2. **Количество параметров:** Количество параметров в каждом слое рассчитывается следующим образом:
   - Для первого слоя: 
     - Матрица весов $W_1$ имеет размерность $2 \times 5$, что дает $2 \times 5 = 10$ параметров.
     - Вектор смещения $b_1$ имеет размерность $5$, что добавляет еще $5$ параметров.
     - Итого: $10 + 5 = 15$ параметров.
   - Для выходного слоя:
     - Матрица весов $W_2$ имеет размерность $5 \times 1$, что дает $5 \times 1 = 5$ параметров.
     - Вектор смещения $b_2$ имеет размерность $1$, что добавляет еще $1$ параметр.
     - Итого: $5 + 1 = 6$ параметров.
   - Общее количество параметров в модели: $15 + 6 = 21$.

### Способы описания модели в Keras

В Keras есть несколько способов описания модели:

1. **Функциональное описание:** Мы можем описать модель, используя функциональный API, что позволяет более гибко управлять архитектурой. Например:

   ```python
   input = tf.keras.Input(shape=(2,))
   z = tf.keras.layers.Dense(5, activation='tanh')(input)
   p = tf.keras.layers.Dense(1, activation='sigmoid')(z)
   model = tf.keras.Model(inputs=input, outputs=p)
   ```

2. **Последовательное описание:** Мы можем использовать последовательный API, который позволяет добавлять слои один за другим:

   ```python
   model = tf.keras.models.Sequential()
   model.add(tf.keras.Input(shape=(2,)))                      # Входной слой
   model.add(tf.keras.layers.Dense(5, activation='tanh'))     # Скрытый слой
   model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # Выходной слой
   ```

3. **Список слоев:** Можно также передать список слоев в модель:

   ```python
   model = tf.keras.models.Sequential([
       tf.keras.layers.Dense(5, activation='tanh', input_shape=(2,)),  # Скрытый слой
       tf.keras.layers.Dense(1, activation='sigmoid')                  # Выходной слой
   ])
   ```

Таким образом, Keras предоставляет мощные инструменты для создания и визуализации нейросетей, что делает процесс разработки более интуитивным и эффективным.

## Chunk 4

### **Название фрагмента: Компиляция модели и обучение в Keras**

**Предыдущий контекст:** В предыдущем фрагменте мы обсудили, как описать структуру модели в Keras, включая различные способы создания модели и визуализации ее параметров. Теперь мы сосредоточимся на процессе компиляции модели и ее обучении.

## **Компиляция и обучение модели в Keras**

Компиляция модели в Keras — это важный этап, который подготавливает модель к обучению. На этом этапе мы определяем оптимизатор, функцию потерь и метрики, которые будут использоваться для оценки производительности модели.

### Шаги компиляции модели

1. **Определение оптимизатора:** Оптимизатор отвечает за обновление весов модели на основе градиентного спуска. В Keras можно использовать различные оптимизаторы, такие как Adam или SGD (Stochastic Gradient Descent). Например:

   ```python
   optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)
   ```

2. **Выбор функции потерь:** Функция потерь измеряет, насколько хорошо модель предсказывает целевые значения. Для задач бинарной классификации, где на выходе используется сигмоидная функция, обычно используется функция потерь "binary crossentropy":

   ```python
   loss_function = 'binary_crossentropy'
   ```

3. **Определение метрик:** Метрики используются для оценки производительности модели во время обучения. Для задач классификации часто используется метрика "accuracy":

   ```python
   metrics = ['accuracy']
   ```

4. **Компиляция модели:** После определения всех параметров мы компилируем модель с помощью метода `compile()`:

   ```python
   model.compile(optimizer=optimizer, loss=loss_function, metrics=metrics)
   ```

### Обучение модели

После компиляции модели мы можем приступить к ее обучению с помощью метода `fit()`. Этот метод принимает данные для обучения и валидации, а также параметры, такие как размер батча и количество эпох:

```python
history = model.fit(train_x, train_y, validation_data=(test_x, test_y), batch_size=16, epochs=5)
```

### Пример кода для компиляции и обучения модели

Вот пример кода, который демонстрирует процесс компиляции и обучения модели в Keras:

```python
import tensorflow as tf

# Пример данных для обучения и валидации
train_x = tf.random.normal([100, 2])                             # 100 примеров с 2 признаками
train_y = tf.random.uniform([100, 1], maxval=2, dtype=tf.int32)  # 100 меток (0 или 1)
test_x  = tf.random.normal([20, 2])                              # 20 примеров для валидации
test_y  = tf.random.uniform([20, 1], maxval=2, dtype=tf.int32)   # 20 меток (0 или 1)

# Создаем последовательную модель
model = tf.keras.models.Sequential()

# Добавляем входной слой
model.add(tf.keras.layers.Input(shape=(2,)))

# Добавляем скрытый слой с 1 нейроном и активацией sigmoid
model.add(tf.keras.layers.Dense(1, activation='sigmoid'))

# Компилируем модель
optimizer      = tf.keras.optimizers.SGD(learning_rate=0.1)   # Оптимизатор
loss_function  = 'binary_crossentropy'                        # Функция потерь
metrics        = ['accuracy']                                 # Метрики

model.compile(optimizer=optimizer, loss=loss_function, metrics=metrics)

# Обучаем модель
history = model.fit(train_x, train_y, validation_data=(test_x, test_y), batch_size=16, epochs=5)

# Выводим результаты обучения
print("Точность на валидации:", history.history['val_accuracy'][-1])
```

### Физический и геометрический смысл

Представьте, что вы обучаете модель для предсказания вероятности того, что определенное событие произойдет, основываясь на двух входных параметрах. Компиляция модели — это как подготовка инструмента для работы: вы выбираете, как именно будете настраивать его (оптимизатор), как будете оценивать его работу (функция потерь) и как будете отслеживать его успех (метрики).

Обучение модели — это процесс, в котором модель "учится" на основе предоставленных данных, корректируя свои параметры для улучшения предсказаний. В случае однослойной нейросети, разделяющая поверхность будет представлять собой прямую линию, которая разделяет пространство входных данных на две области, соответствующие различным классам. В более сложных моделях, таких как двухслойные нейросети, разделяющая поверхность становится более сложной и многомерной, что позволяет лучше моделировать сложные зависимости в данных.

Таким образом, процесс компиляции и обучения в Keras позволяет эффективно настраивать и оптимизировать нейросети для решения различных задач машинного обучения.

## Chunk 5

### **Название фрагмента: Визуализация результатов обучения нейросети**

**Предыдущий контекст:** В предыдущем фрагменте мы обсудили процесс компиляции и обучения модели в Keras, включая выбор оптимизатора, функции потерь и метрик. Теперь мы сосредоточимся на визуализации результатов обучения, чтобы лучше понять, как модель обучается и как изменяются ее параметры.

## **Визуализация результатов обучения нейросети**

Визуализация результатов обучения нейросети позволяет нам отслеживать, как изменяются функции потерь и точности на обучающей и валидационной выборках в процессе обучения. Это помогает выявить проблемы, такие как переобучение, когда модель начинает запоминать данные, а не учиться обобщать.

### Построение графиков

1. **История обучения:** После обучения модели мы можем получить историю обучения, которая содержит информацию о значениях функции потерь и точности на каждой эпохе. Эта информация хранится в объекте `history`.

2. **Построение графиков:** Мы можем использовать библиотеку Matplotlib для построения графиков. Например, чтобы построить график функции потерь, мы можем использовать следующий код:

```python
import matplotlib.pyplot as plt

# Построение графика функции потерь
plt.plot(history.history['loss'], label='train loss')           # Функция потерь на обучающей выборке
plt.plot(history.history['val_loss'], label='validation loss')  # Функция потерь на валидационной выборке
plt.title('Loss during training')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid()
plt.show()
```

3. **Анализ графиков:** На графике мы можем увидеть, как функция потерь изменяется в процессе обучения. Если функция потерь на валидационной выборке начинает расти, это может указывать на переобучение модели.

### Пример кода для визуализации

Вот пример кода, который демонстрирует, как визуализировать результаты обучения нейросети:

```python
import tensorflow as tf
import matplotlib.pyplot as plt

# Пример данных для обучения и валидации
train_x = tf.random.normal([100, 2])                             # 100 примеров с 2 признаками
train_y = tf.random.uniform([100, 1], maxval=2, dtype=tf.int32)  # 100 меток (0 или 1)
test_x  = tf.random.normal([20, 2])                              # 20 примеров для валидации
test_y  = tf.random.uniform([20, 1], maxval=2, dtype=tf.int32)   # 20 меток (0 или 1)

# Создаем последовательную модель
model = tf.keras.models.Sequential()

# Добавляем входной слой
model.add(tf.keras.layers.Input(shape=(2,)))

# Добавляем скрытый слой с 1 нейроном и активацией sigmoid
model.add(tf.keras.layers.Dense(1, activation='sigmoid'))

# Компилируем модель
optimizer      = tf.keras.optimizers.SGD(learning_rate=0.1)   # Оптимизатор
loss_function  = 'binary_crossentropy'                        # Функция потерь
metrics        = ['accuracy']                                 # Метрики

model.compile(optimizer=optimizer, loss=loss_function, metrics=metrics)

# Обучаем модель
history = model.fit(train_x, train_y, validation_data=(test_x, test_y), batch_size=16, epochs=5)

# Выводим результаты обучения
print("Точность на валидации:", history.history['val_accuracy'][-1])

def plot_training_history(history):
    """
    Description:
        Построение графиков функции потерь и точности во время обучения.

    Args:
        history: Объект истории обучения модели, содержащий метрики.

    Returns:
        None

    Raises:
        None

    Examples:
        >>> plot_training_history(history)
        (Графики отображаются в окне)
    """
    # Построение графика функции потерь
    plt.figure(figsize=(12, 5))

    # График потерь
    plt.subplot(1, 2, 1)
    plt.plot(history.history['loss'], label='train loss')
    plt.plot(history.history['val_loss'], label='validation loss')
    plt.title('Loss during training')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid()

    # График точности
    plt.subplot(1, 2, 2)
    plt.plot(history.history['accuracy'], label='train accuracy')
    plt.plot(history.history['val_accuracy'], label='validation accuracy')
    plt.title('Accuracy during training')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid()

    plt.show()

# Вызов функции для построения графиков
plot_training_history(history)
```

### Физический и геометрический смысл

Визуализация результатов обучения нейросети позволяет нам лучше понять, как модель адаптируется к данным. Например, если мы обучаем модель для распознавания изображений, графики потерь и точности показывают, насколько хорошо модель учится различать классы. 

Когда модель обучается, она настраивает свои параметры (веса и смещения), чтобы минимизировать функцию потерь. Если график функции потерь на валидационной выборке начинает расти, это может означать, что модель начинает запоминать данные, а не учиться обобщать, что является признаком переобучения.

Таким образом, визуализация результатов обучения является важным инструментом для анализа и улучшения производительности нейросетей, позволяя разработчикам принимать обоснованные решения о том, как улучшить модель и избежать проблем с переобучением.

### Переход к следующему примеру

Теперь, когда мы обсудили визуализацию результатов, мы можем перейти к следующему примеру, используя датасет MNIST. Этот датасет представляет собой набор изображений рукописных цифр и позволяет нам продемонстрировать, как параметры нейросети могут быть визуализированы в контексте многоклассовой классификации. MNIST уже встроен в Keras, что упрощает его использование для обучения и тестирования моделей.

## Chunk 6

### **Название фрагмента: Загрузка и подготовка датасета MNIST в Keras**

**Предыдущий контекст:** В предыдущем фрагменте мы обсудили визуализацию результатов обучения нейросети и важность отслеживания функции потерь и точности. Теперь мы перейдем к загрузке и подготовке датасета MNIST, который является стандартным набором данных для задач классификации изображений.

## **Загрузка и подготовка датасета MNIST**

Датасет MNIST состоит из изображений рукописных цифр и широко используется для тестирования алгоритмов машинного обучения. Keras предоставляет встроенные функции для загрузки этого датасета, что упрощает процесс подготовки данных для обучения моделей.

### Загрузка датасета

1. **Загрузка данных:** Мы можем загрузить датасет MNIST с помощью функции `tf.keras.datasets.mnist.load_data()`, которая возвращает обучающую и тестовую выборки. Обучающая выборка содержит 60,000 изображений, а тестовая — 10,000 изображений.

   ```python
   (train_x, train_y), (test_x, test_y) = tf.keras.datasets.mnist.load_data()
   ```

2. **Форма данных:** После загрузки мы можем проверить форму обучающей выборки. Изображения имеют размерность 28 на 28 пикселей, что означает, что каждое изображение представлено в виде двумерного массива.

   ```python
   print(train_x.shape)  # Вывод: (60000, 28, 28)
   ```

### Подготовка данных

1. **Нормализация:** Значения пикселей изображений находятся в диапазоне от 0 до 255. Для улучшения обучения модели мы можем нормализовать данные, разделив их на 255, чтобы они находились в диапазоне от 0 до 1.

   ```python
   train_x = train_x.astype('float32') / 255.0
   test_x = test_x.astype('float32') / 255.0
   ```

2. **Визуализация:** Мы можем визуализировать несколько изображений из обучающей выборки, чтобы убедиться, что данные загружены и подготовлены правильно. Для этого мы можем использовать библиотеку Matplotlib.

   ```python
   import matplotlib.pyplot as plt

   # Визуализация первых 10 изображений
   fig, axes = plt.subplots(1, 10, figsize=(10, 1))
   for ax, img in zip(axes, train_x[:10]):
       ax.imshow(img, cmap='gray')
       ax.axis('off')
   plt.show()
   ```

### Математическая формализация

При загрузке и подготовке данных мы не используем сложные математические формулы, но важно понимать, что нормализация данных помогает улучшить сходимость алгоритмов обучения. Нормализация может быть представлена как:

$$
x' = \frac{x}{255}
$$

где:
- $x$ — исходное значение пикселя (от 0 до 255);
- $x'$ — нормализованное значение пикселя (от 0 до 1).

### Пример кода для загрузки и подготовки MNIST

Вот пример кода, который демонстрирует загрузку и подготовку датасета MNIST:

```python
import tensorflow as tf
import matplotlib.pyplot as plt

# Загрузка датасета MNIST
(train_x, train_y), (test_x, test_y) = tf.keras.datasets.mnist.load_data()

# Нормализация данных
train_x = train_x.astype('float32') / 255.0
test_x = test_x.astype('float32') / 255.0

# Визуализация первых 10 изображений
fig, axes = plt.subplots(1, 10, figsize=(10, 1))
for ax, img in zip(axes, train_x[:10]):
    ax.imshow(img, cmap='gray')
    ax.axis('off')
plt.show()
```

### Физический и геометрический смысл

Загрузка и подготовка датасета MNIST позволяет нам работать с реальными данными, которые представляют собой изображения рукописных цифр. Каждое изображение можно рассматривать как точку в многомерном пространстве, где каждое измерение соответствует значению пикселя. Нормализация данных помогает улучшить обучение модели, позволяя ей быстрее находить оптимальные параметры.

Таким образом, подготовка данных является важным этапом в процессе обучения нейросетей, так как она влияет на качество и скорость обучения модели. Теперь, когда мы подготовили датасет MNIST, мы можем перейти к обучению модели на этих данных и исследованию разделяющих поверхностей, которые она создает для классификации изображений.

## Chunk 7

### **Название фрагмента: Обучение однослойной нейросети на датасете MNIST**

**Предыдущий контекст:** В предыдущем фрагменте мы загрузили и подготовили датасет MNIST, который состоит из изображений рукописных цифр. Теперь мы сосредоточимся на обучении однослойной нейросети для классификации этих изображений.

## **Обучение однослойной нейросети**

Однослойная нейросеть — это простая модель, которая может быть использована для решения задач классификации. В данном случае мы будем использовать ее для классификации изображений цифр из датасета MNIST.

### Архитектура модели

1. **Количество нейронов:** В однослойной нейросети количество нейронов на выходе соответствует количеству классов, которые мы хотим классифицировать. В случае MNIST у нас 10 классов (цифры от 0 до 9), поэтому мы будем использовать 10 нейронов на выходе.

2. **Размерность входных данных:** Каждое изображение имеет размерность 28 на 28 пикселей, что в сумме дает 784 пикселя. Таким образом, входная размерность для нашей модели будет 784.

3. **Функция активации:** На последнем слое мы будем использовать функцию активации softmax, которая преобразует выходные значения нейронов в вероятности для каждого класса. Это позволяет интерпретировать выходы модели как вероятности принадлежности к каждому из классов.

### Создание модели

Теперь мы можем создать модель с использованием Keras:

```python
import tensorflow as tf

# Создаем однослойную нейросеть
model = tf.keras.models.Sequential()

# Добавляем полносвязный слой с 10 нейронами и функцией активации softmax
model.add(tf.keras.layers.Dense(10, activation='softmax', input_shape=(784,)))
```

### Функция потерь

Для задачи классификации с использованием softmax на выходе мы будем использовать функцию потерь categorical crossentropy. Эта функция измеряет, насколько хорошо предсказанные вероятности соответствуют истинным меткам классов.

### Компиляция и обучение модели

После создания модели мы можем скомпилировать ее, указав оптимизатор, функцию потерь и метрики:

```python
# Компилируем модель
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Обучаем модель
history = model.fit(train_x.reshape(-1, 784), train_y, validation_data=(test_x.reshape(-1, 784), test_y), epochs=5, batch_size=32)
```

### Математическая формализация

Функция softmax преобразует вектор $z$ размерности 10 в вероятности следующим образом:

$$
\sigma(z_i) = \frac{e^{z_i}}{\sum_{j=0}^{9} e^{z_j}}
$$

где:
- $z_i$ — выход нейрона $i$,
- $\sigma(z_i)$ — вероятность, что входное изображение принадлежит классу $i$.

### Пример кода для обучения модели

Вот полный пример кода, который включает создание, компиляцию и обучение однослойной нейросети на датасете MNIST:

```python
import tensorflow as tf
import matplotlib.pyplot as plt

# Загрузка и подготовка датасета MNIST
(train_x, train_y), (test_x, test_y) = tf.keras.datasets.mnist.load_data()
train_x = train_x.astype('float32') / 255.0
test_x  = test_x.astype('float32') / 255.0

# Создаем однослойную нейросеть
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Dense(10, activation='softmax', input_shape=(784,)))

# Компилируем модель
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Обучаем модель
history = model.fit(train_x.reshape(-1, 784), train_y, validation_data=(test_x.reshape(-1, 784), test_y), epochs=5, batch_size=32)

def plot_training_history(history):
    """
    Description:
        Построение графика точности во время обучения.

    Args:
        history: Объект истории обучения модели, содержащий метрики.

    Returns:
        None

    Raises:
        None

    Examples:
        >>> plot_training_history(history)
        (График отображается в окне)
    """
    plt.plot(history.history['accuracy'], label='train accuracy')
    plt.plot(history.history['val_accuracy'], label='validation accuracy')
    plt.title('Accuracy during training')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid()
    plt.show()

def display_predictions(model, test_x, test_y):
    """
    Description:
        Отображение пары изображений цифр и выходного вектора вероятностей для каждой картинки.

    Args:
        model: Обученная модель нейронной сети.
        test_x: Тестовые данные для предсказания.
        test_y: Метки тестовых данных.

    Returns:
        None

    Raises:
        None

    Examples:
        >>> display_predictions(model, test_x, test_y)
        (Изображения и векторы вероятностей отображаются в окне)
    """
    # Выбираем две случайные картинки из тестового набора
    indices = [0, 1]
    images = test_x[indices]
    labels = test_y[indices]

    # Предсказываем вероятности для выбранных изображений
    predictions = model.predict(images.reshape(-1, 784))

    # Отображаем изображения и векторы вероятностей
    for i, (image, label, prediction) in enumerate(zip(images, labels, predictions)):
        plt.subplot(1, 2, i + 1)
        plt.imshow(image, cmap='gray')
        plt.title(f'True label: {label}\nPredicted: {prediction.argmax()}')
        plt.axis('off')

        # Выводим вектор вероятностей в удобочитаемом формате
        print(f'Probabilities for image {i + 1}:')
        for j, prob in enumerate(prediction):
            print(f'  Class {j}: {prob:.4f}')

    plt.show()

# Визуализация результатов обучения
plot_training_history(history)

# Отображение пары изображений и векторов вероятностей
display_predictions(model, test_x, test_y)
```

### Физический и геометрический смысл

Обучение однослойной нейросети на датасете MNIST позволяет нам визуализировать, как модель классифицирует изображения цифр. Каждый нейрон на выходе представляет собой вероятность принадлежности к определенному классу. Функция softmax обеспечивает, что сумма всех вероятностей равна 1, что делает результаты интерпретируемыми.

Таким образом, однослойная нейросеть может эффективно решать задачи классификации, хотя и с ограниченной способностью к обобщению по сравнению с более сложными архитектурами. Теперь, когда мы обучили модель, мы можем перейти к оценке ее производительности на тестовой выборке и анализу разделяющих поверхностей, которые она создает для классификации изображений.

## Chunk 8

### **Название фрагмента: Компиляция и обучение однослойной нейросети с использованием one-hot encoding**

**Предыдущий контекст:** В предыдущем фрагменте мы обсудили архитектуру однослойной нейросети для классификации изображений из датасета MNIST, включая количество нейронов, функции активации и функции потерь. Теперь мы сосредоточимся на компиляции модели и обучении с использованием one-hot encoding для меток классов.

## **Компиляция и обучение модели с one-hot encoding**

Для успешного обучения модели необходимо правильно настроить функции потерь и метрики, а также подготовить метки классов в формате, который может быть использован моделью.

### Компиляция модели

1. **Оптимизатор:** Мы можем использовать оптимизатор Adam, который хорошо подходит для большинства задач машинного обучения благодаря своей адаптивной скорости обучения.

2. **Функция потерь:** Поскольку мы имеем дело с многоклассовой классификацией, нам нужна функция потерь categorical cross-entropy. Эта функция измеряет, насколько хорошо предсказанные вероятности соответствуют истинным меткам классов.

3. **Метрики:** Для оценки производительности модели мы будем использовать метрику accuracy, которая показывает долю правильных предсказаний.

### Преобразование меток классов с использованием one-hot encoding

Для того чтобы нейросеть могла правильно обучаться, метки классов (например, цифры от 0 до 9) должны быть преобразованы в формат one-hot encoding. Это означает, что каждая метка будет представлена вектором, где на позиции, соответствующей классу, будет стоять 1, а на остальных позициях — 0.

Например, для цифры 5 вектор one-hot encoding будет выглядеть так: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0].

Для преобразования меток в формат one-hot encoding в Keras можно использовать функцию `tf.keras.utils.to_categorical()`.

### Пример кода для компиляции и обучения модели

Вот пример кода, который демонстрирует компиляцию модели и обучение с использованием one-hot encoding:

```python
import tensorflow as tf
import matplotlib.pyplot as plt

# Загрузка и подготовка датасета MNIST
(train_x, train_y), (test_x, test_y) = tf.keras.datasets.mnist.load_data()
train_x = train_x.astype('float32') / 255.0
test_x  = test_x.astype('float32') / 255.0

# Преобразование данных в одномерный формат
train_x = train_x.reshape(-1, 784)
test_x  = test_x.reshape(-1, 784)

# Преобразование меток в формат one-hot encoding
train_y_one_hot = tf.keras.utils.to_categorical(train_y, num_classes=10)
test_y_one_hot  = tf.keras.utils.to_categorical(test_y, num_classes=10)

# Создаем однослойную нейросеть
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Dense(10, activation='softmax', input_shape=(784,)))

# Компилируем модель
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Обучаем модель
history = model.fit(train_x, train_y_one_hot, validation_data=(test_x, test_y_one_hot), epochs=5, batch_size=16)

def plot_training_history(history):
    """
    Description:
        Построение графика точности во время обучения.

    Args:
        history: Объект истории обучения модели, содержащий метрики.

    Returns:
        None

    Raises:
        None

    Examples:
        >>> plot_training_history(history)
        (График отображается в окне)
    """
    plt.plot(history.history['accuracy'], label='train accuracy')
    plt.plot(history.history['val_accuracy'], label='validation accuracy')
    plt.title('Accuracy during training')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid()
    plt.show()

def display_predictions(model, test_x, test_y):
    """
    Description:
        Отображение пары изображений цифр и выходного вектора вероятностей для каждой картинки.

    Args:
        model: Обученная модель нейронной сети.
        test_x: Тестовые данные для предсказания.
        test_y: Метки тестовых данных.

    Returns:
        None

    Raises:
        None

    Examples:
        >>> display_predictions(model, test_x, test_y)
        (Изображения и векторы вероятностей отображаются в окне)
    """
    # Выбираем две случайные картинки из тестового набора
    indices = [0, 1]
    images = test_x[indices].reshape(-1, 28, 28)
    labels = test_y[indices]

    # Предсказываем вероятности для выбранных изображений
    predictions = model.predict(test_x[indices])

    # Отображаем изображения и векторы вероятностей
    for i, (image, label, prediction) in enumerate(zip(images, labels, predictions)):
        plt.subplot(1, 2, i + 1)
        plt.imshow(image, cmap='gray')
        plt.title(f'True label: {label}\nPredicted: {prediction.argmax()}')
        plt.axis('off')

        # Выводим вектор вероятностей в удобочитаемом формате
        print(f'Probabilities for image {i + 1}:')
        for j, prob in enumerate(prediction):
            print(f'  Class {j}: {prob:.4f}')

    plt.show()

# Визуализация результатов обучения
plot_training_history(history)

# Отображение пары изображений и векторов вероятностей
display_predictions(model, test_x, test_y)
```

### Математическая формализация

Функция потерь categorical cross-entropy для многоклассовой классификации может быть представлена следующим образом:

$$
L(y, \hat{y}) = -\sum_{i=1}^{C} y_i \log(\hat{y}_i)
$$

где:
- $L(y, \hat{y})$ — функция потерь,
- $C$ — количество классов,
- $y_i$ — истинная метка (one-hot encoded),
- $\hat{y}_i$ — предсказанная вероятность для класса $i$.

### Физический и геометрический смысл

Обучение однослойной нейросети с использованием one-hot encoding позволяет модели интерпретировать метки классов как вероятности принадлежности к каждому из классов. Это важно для задач классификации, так как модель должна уметь различать между несколькими классами, а не просто выдавать бинарные результаты.

Таким образом, использование one-hot encoding и правильной функции потерь позволяет нейросети эффективно обучаться и делать предсказания на основе входных данных. Теперь, когда мы обучили модель, мы можем перейти к оценке ее производительности и анализу результатов.

## Chunk 9

### **Название фрагмента: Обучение и оценка однослойной нейросети на MNIST**

**Предыдущий контекст:** В предыдущем фрагменте мы обсудили компиляцию модели и обучение однослойной нейросети с использованием one-hot encoding для меток классов. Теперь мы сосредоточимся на процессе обучения модели, включая преобразование входных данных и оценку производительности.

## **Обучение однослойной нейросети и оценка производительности**

Обучение нейросети включает в себя несколько ключевых этапов, таких как преобразование входных данных, настройка модели и оценка ее производительности на валидационных данных.

### Преобразование входных данных

1. **Решейпинг данных:** Поскольку входные данные имеют размерность (60000, 28, 28), нам необходимо преобразовать их в одномерный массив размерности (60000, 784), чтобы соответствовать ожиданиям полносвязного слоя. Для этого мы используем метод `reshape()`:

   ```python
   train_x_flat = train_x.reshape(-1, 784)  # Преобразуем обучающие данные
   test_x_flat = test_x.reshape(-1, 784)     # Преобразуем тестовые данные
   ```

   Здесь `-1` позволяет автоматически определить размер первой размерности, что делает код более гибким.

### Обучение модели

2. **Обучение модели:** После подготовки данных мы можем обучить модель с помощью метода `fit()`. Важно помнить, что если модель уже была обучена, необходимо заново инициализировать ее, чтобы избежать продолжения обучения с предыдущих весов.

   ```python
   history = model.fit(train_x_flat, train_y_one_hot, validation_data=(test_x_flat, test_y_one_hot), epochs=5, batch_size=16)
   ```

### Оценка производительности

3. **График точности:** После обучения модели мы можем построить график, чтобы визуализировать, как изменяется точность на обучающей и валидационной выборках. Это поможет выявить возможные проблемы, такие как переобучение.

   ```python
   plt.figure(figsize=(10, 3))
   plt.plot(history.history['accuracy'], label='train accuracy')
   plt.plot(history.history['val_accuracy'], label='validation accuracy')
   plt.title('Accuracy during training')
   plt.xlabel('Epochs')
   plt.ylabel('Accuracy')
   plt.legend()
   plt.grid()
   plt.show()
   ```

### Математическая формализация

При обучении модели мы используем функцию потерь categorical cross-entropy, которая измеряет, насколько хорошо предсказанные вероятности соответствуют истинным меткам классов. Формула для этой функции выглядит следующим образом:

$$
L(y, \hat{y}) = -\sum_{i=1}^{C} y_i \log(\hat{y}_i)
$$

где:
- $L(y, \hat{y})$ — функция потерь,
- $C$ — количество классов,
- $y_i$ — истинная метка (one-hot encoded),
- $\hat{y}_i$ — предсказанная вероятность для класса $i$.

### Пример кода для обучения и оценки модели

Вот полный пример кода, который включает в себя обучение модели и построение графика точности:

```python
import tensorflow as tf
import matplotlib.pyplot as plt

# Загрузка и подготовка датасета MNIST
(train_x, train_y), (test_x, test_y) = tf.keras.datasets.mnist.load_data()
train_x = train_x.astype('float32') / 255.0
test_x  = test_x.astype('float32') / 255.0

# Преобразование данных в одномерный формат
train_x_flat = train_x.reshape(-1, 784)
test_x_flat  = test_x.reshape(-1, 784)

# Преобразование меток в формат one-hot encoding
train_y_one_hot = tf.keras.utils.to_categorical(train_y, num_classes=10)
test_y_one_hot  = tf.keras.utils.to_categorical(test_y, num_classes=10)

# Создаем однослойную нейросеть
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Dense(10, activation='softmax', input_shape=(784,)))

# Компилируем модель
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Обучаем модель
history = model.fit(train_x_flat, train_y_one_hot, validation_data=(test_x_flat, test_y_one_hot), epochs=5, batch_size=16)

def plot_training_history(history):
    """
    Description:
        Построение графика точности во время обучения.

    Args:
        history: Объект истории обучения модели, содержащий метрики.

    Returns:
        None

    Raises:
        None

    Examples:
        >>> plot_training_history(history)
        (График отображается в окне)
    """
    plt.figure(figsize=(10, 3))
    plt.plot(history.history['accuracy'], label='train accuracy')
    plt.plot(history.history['val_accuracy'], label='validation accuracy')
    plt.title('Accuracy during training')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid()
    plt.show()

# Визуализация результатов обучения
plot_training_history(history)
```

### Физический и геометрический смысл

Обучение однослойной нейросети на датасете MNIST позволяет нам визуализировать, как модель классифицирует изображения цифр. График точности показывает, насколько хорошо модель обучается и обобщает данные. Если точность на валидационной выборке начинает падать, это может указывать на переобучение, когда модель начинает запоминать данные, а не учиться обобщать.

Таким образом, процесс обучения и оценки модели является важным этапом в разработке нейросетей, позволяя разработчикам анализировать производительность и вносить необходимые коррективы в архитектуру модели или процесс обучения. Теперь, когда мы обучили модель, мы можем перейти к оценке ее производительности на тестовой выборке и анализу результатов.

## Chunk 10

### **Название фрагмента: Использование матрицы ошибок и sparse categorical cross-entropy**

**Предыдущий контекст:** В предыдущем фрагменте мы обсудили процесс обучения однослойной нейросети на датасете MNIST, включая преобразование входных данных и оценку производительности. Теперь мы сосредоточимся на использовании матрицы ошибок для оценки качества модели и различиях между функциями потерь categorical cross-entropy и sparse categorical cross-entropy.

## **Матрица ошибок и функции потерь**

### Функции потерь

1. **Categorical Cross-Entropy:** Эта функция потерь используется, когда метки классов представлены в формате one-hot encoding. Она принимает два вектора: предсказанные вероятности и истинные метки в формате one-hot. Формула для categorical cross-entropy выглядит следующим образом:

   $$
   L(y, \hat{y}) = -\sum_{i=1}^{C} y_i \log(\hat{y}_i)
   $$

   где:
   - $L(y, \hat{y})$ — функция потерь,
   - $C$ — количество классов,
   - $y_i$ — истинная метка (one-hot encoded),
   - $\hat{y}_i$ — предсказанная вероятность для класса $i$.

2. **Sparse Categorical Cross-Entropy:** Эта функция потерь используется, когда метки классов представлены в виде целых чисел (например, 0, 1, 2,..., 9). Вместо передачи one-hot encoded векторов, мы передаем просто номера классов. Это экономит память и упрощает код. Формула остается такой же, но вместо вектора one-hot используется индекс класса:

   $$
   L(y, \hat{y}) = -\log(\hat{y}_{y})
   $$

   где:
   - $y$ — истинный класс (номер),
   - $\hat{y}_{y}$ — предсказанная вероятность для истинного класса.

### Матрица ошибок

Матрица ошибок (confusion matrix) позволяет визуализировать, как модель классифицирует данные, показывая количество правильных и неправильных предсказаний для каждого класса. Для ее построения нам нужны предсказанные классы и истинные метки классов.

1. **Получение предсказаний:** Мы можем получить предсказания модели с помощью метода `predict()`, который возвращает вероятности для каждого класса. Чтобы получить номер предсказанного класса, мы можем использовать функцию `argmax()`:

   ```python
   predictions = model.predict(test_x_flat)  # Получаем вероятности
   predicted_classes = tf.argmax(predictions, axis=1)  # Получаем номера классов
   ```

2. **Построение матрицы ошибок:** Для построения матрицы ошибок мы можем использовать функцию `confusion_matrix` из библиотеки `sklearn`. Она принимает истинные метки и предсказанные классы:

   ```python
   from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

   # Получаем истинные метки
   true_classes = test_y

   # Строим матрицу ошибок
   cm = confusion_matrix(true_classes, predicted_classes)
   disp = ConfusionMatrixDisplay(confusion_matrix=cm)
   disp.plot()
   plt.show()
   ```

### Пример кода для оценки модели

Вот пример кода, который демонстрирует использование sparse categorical cross-entropy и построение матрицы ошибок:

```python
import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Загрузка и подготовка датасета MNIST
(train_x, train_y), (test_x, test_y) = tf.keras.datasets.mnist.load_data()
train_x = train_x.astype('float32') / 255.0
test_x  = test_x.astype('float32') / 255.0

# Преобразование данных в одномерный формат
train_x_flat = train_x.reshape(-1, 784)
test_x_flat  = test_x.reshape(-1, 784)

# Создаем однослойную нейросеть
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Dense(10, activation='softmax', input_shape=(784,)))

# Компилируем модель с использованием sparse categorical cross-entropy
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Обучаем модель
history = model.fit(train_x_flat, train_y, validation_data=(test_x_flat, test_y), epochs=5, batch_size=16)

def plot_training_history(history):
    """
    Description:
        Построение графика точности во время обучения.

    Args:
        history: Объект истории обучения модели, содержащий метрики.

    Returns:
        None

    Raises:
        None

    Examples:
        >>> plot_training_history(history)
        (График отображается в окне)
    """
    plt.figure(figsize=(10, 3))
    plt.plot(history.history['accuracy'], label='train accuracy')
    plt.plot(history.history['val_accuracy'], label='validation accuracy')
    plt.title('Accuracy during training')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid()
    plt.show()

def plot_confusion_matrix(test_y, predicted_classes):
    """
    Description:
        Построение матрицы ошибок для оценки качества модели.

    Args:
        test_y: Истинные метки тестового набора данных.
        predicted_classes: Предсказанные метки классов.

    Returns:
        None

    Raises:
        None

    Examples:
        >>> plot_confusion_matrix(test_y, predicted_classes)
        (Матрица ошибок отображается в окне)
    """
    cm = confusion_matrix(test_y, predicted_classes)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)
    disp.plot()
    plt.show()

# Получаем предсказания
predictions = model.predict(test_x_flat)
predicted_classes = tf.argmax(predictions, axis=1)

# Визуализация результатов обучения
plot_training_history(history)

# Построение матрицы ошибок
plot_confusion_matrix(test_y, predicted_classes)
```

### Физический и геометрический смысл

Использование матрицы ошибок позволяет нам визуально оценить, как хорошо модель классифицирует данные. Каждый элемент матрицы показывает количество предсказаний для каждой пары истинного и предсказанного классов. Это помогает выявить, какие классы модель путает, и вносить коррективы в архитектуру модели или процесс обучения.

Таким образом, понимание различий между функциями потерь и использование матрицы ошибок являются важными аспектами в разработке и оценке нейросетей. Теперь, когда мы оценили модель, мы можем перейти к анализу ее производительности и возможным улучшениям.

## Chunk 11

### **Название фрагмента: Оценка производительности модели и визуализация весов**

**Предыдущий контекст:** В предыдущем фрагменте мы обсудили использование функции потерь sparse categorical cross-entropy и построение матрицы ошибок для оценки качества модели. Теперь мы сосредоточимся на получении предсказаний, построении матрицы ошибок и визуализации весов модели.

## **Оценка производительности и визуализация весов**

### Получение предсказаний

1. **Предсказания модели:** После обучения модели мы можем получить предсказания для тестовых данных с помощью метода `predict()`. Этот метод возвращает вероятности для каждого класса, и чтобы получить номер предсказанного класса, мы используем функцию `argmax()`:

   ```python
   predictions = model.predict(test_x_flat)            # Получаем вероятности
   predicted_classes = tf.argmax(predictions, axis=1)  # Получаем номера классов
   ```

   Здесь `axis=1` указывает, что мы хотим получить максимальные индексы по второй оси (по строкам), что соответствует предсказанным классам для каждого изображения.

### Построение матрицы ошибок

2. **Матрица ошибок:** Мы можем построить матрицу ошибок, используя функцию `ConfusionMatrixDisplay` из библиотеки `sklearn`. Она позволяет визуализировать, как модель классифицирует данные, показывая количество правильных и неправильных предсказаний для каждого класса.

   ```python
   from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

   # Строим матрицу ошибок
   cm = confusion_matrix(test_y, predicted_classes)
   disp = ConfusionMatrixDisplay(confusion_matrix=cm)
   disp.plot()
   plt.show()
   ```

   На матрице ошибок по диагонали будут находиться правильные предсказания, а остальные элементы покажут, какие классы были перепутаны.

### Визуализация весов

3. **Визуализация весов:** Веса модели можно визуализировать, чтобы понять, как нейросеть принимает решения. Мы можем получить веса первого слоя и отобразить их в виде изображений. Каждый нейрон в выходном слое соответствует одному классу, и веса можно интерпретировать как фильтры, которые нейросеть использует для распознавания цифр.

   ```python
   weights = model.layers[0].get_weights()[0]                  # Получаем веса первого слоя
   plt.figure(figsize=(10, 5))
   for i in range(10):
       plt.subplot(2, 5, i + 1)
       plt.imshow(weights[:, i].reshape(28, 28), cmap='gray')  # Визуализируем веса
       plt.title(f'Class {i}')
       plt.axis('off')
   plt.show()
   ```

### Математическая формализация

При использовании функции потерь sparse categorical cross-entropy, мы можем выразить ее следующим образом:

$$
L(y, \hat{y}) = -\log(\hat{y}_{y})
$$

где:
- $y$ — истинный класс (номер),
- $\hat{y}_{y}$ — предсказанная вероятность для истинного класса.

### Пример кода для оценки и визуализации

Вот полный пример кода, который включает в себя получение предсказаний, построение матрицы ошибок и визуализацию весов:

```python
import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Загрузка и подготовка датасета MNIST
(train_x, train_y), (test_x, test_y) = tf.keras.datasets.mnist.load_data()
train_x = train_x.astype('float32') / 255.0
test_x = test_x.astype('float32') / 255.0

# Преобразование данных в одномерный формат
train_x_flat = train_x.reshape(-1, 784)
test_x_flat = test_x.reshape(-1, 784)

# Создаем однослойную нейросеть
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Dense(10, activation='softmax', input_shape=(784,)))

# Компилируем модель с использованием sparse categorical cross-entropy
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Обучаем модель
model.fit(train_x_flat, train_y, validation_data=(test_x_flat, test_y), epochs=5, batch_size=16)

# Получаем предсказания
predictions = model.predict(test_x_flat)
predicted_classes = tf.argmax(predictions, axis=1)

# Строим матрицу ошибок
cm = confusion_matrix(test_y, predicted_classes)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot()
plt.show()

# Визуализация весов
weights = model.layers[0].get_weights()[0]
plt.figure(figsize=(10, 5))
for i in range(10):
    plt.subplot(2, 5, i + 1)
    plt.imshow(weights[:, i].reshape(28, 28), cmap='gray')
    plt.title(f'Class {i}')
    plt.axis('off')
plt.show()
```

### Физический и геометрический смысл

Оценка производительности модели с помощью матрицы ошибок и визуализация весов помогают понять, как нейросеть принимает решения. Матрица ошибок показывает, какие классы модель путает, а визуализация весов позволяет увидеть, какие характеристики изображения наиболее важны для классификации. Это важно для дальнейшего улучшения модели и понимания ее работы.

Таким образом, использование матрицы ошибок и визуализация весов являются важными инструментами для анализа и улучшения производительности нейросетей. Теперь, когда мы оценили модель, мы можем перейти к анализу ее производительности и возможным улучшениям.

## Chunk 12

### **Название фрагмента: Влияние нормализации на обучение нейросети и точность вычислений**

**Предыдущий контекст:** В предыдущем фрагменте мы обсудили процесс обучения однослойной нейросети на датасете MNIST, включая использование функции потерь sparse categorical cross-entropy и построение матрицы ошибок. Теперь мы сосредоточимся на важности нормализации входных данных и влиянии масштабирования на точность вычислений.

## **Влияние нормализации на обучение нейросети**

### Нормализация данных

1. **Зачем нужна нормализация:** Нормализация входных данных — это процесс приведения значений к одному диапазону, что помогает улучшить обучение нейросети. Если входные данные имеют разные масштабы, это может привести к проблемам с оптимизацией, так как градиенты могут быть слишком малыми или слишком большими, что затрудняет процесс обучения.

2. **Диапазоны нормализации:** Обычно данные нормализуют в диапазоне от 0 до 1 или от -1 до 1. Это соглашение позволяет нейросети лучше и быстрее обучаться, так как функции активации, такие как sigmoid или tanh, работают более эффективно в этих диапазонах.

### Проблемы с точностью вычислений

3. **Проблемы с мантисой и порядком:** При работе с числами с плавающей точкой важно учитывать, что они представляются мантисой и порядком. Если у нас есть большое число, например, $1e10$, и мы прибавляем к нему 1, результат будет точным. Однако, если мы прибавим 1 к числу $1e20$, результат не изменится, так как 1 незначителен по сравнению с $1e20$. Это приводит к потере точности.

4. **Сложение чисел с разными масштабами:** Если числа находятся в разных диапазонах, и мы их складываем, это может привести к катастрофической потере точности. Например, если одно число очень большое, а другое очень маленькое, то при сложении результат будет определяться только большим числом.

### Пример кода для нормализации

Вот пример кода, который демонстрирует нормализацию данных перед обучением модели:

```python
import tensorflow as tf

# Загрузка и подготовка датасета MNIST
(train_x, train_y), (test_x, test_y) = tf.keras.datasets.mnist.load_data()

# Нормализация данных
train_x = train_x.astype('float32') / 255.0  # Приведение к диапазону [0, 1]
test_x = test_x.astype('float32') / 255.0    # Приведение к диапазону [0, 1]

# Преобразование данных в одномерный формат
train_x_flat = train_x.reshape(-1, 784)
test_x_flat = test_x.reshape(-1, 784)
```

### Физический и геометрический смысл

Нормализация данных позволяет нейросети более эффективно обучаться, так как она помогает избежать проблем с масштабами входных данных. Это важно для дальнейшего улучшения модели и понимания ее работы. Понимание влияния нормализации на точность вычислений также критично, так как это может повлиять на качество предсказаний модели.

Таким образом, нормализация входных данных и понимание точности вычислений являются важными аспектами в процессе обучения нейросетей. Теперь, когда мы обсудили эти концепции, мы можем перейти к анализу производительности модели и возможным улучшениям.

## Chunk 13

### **Название фрагмента: Нормализация данных и динамические графы вычислений в нейросетях**

**Предыдущий контекст:** В предыдущем фрагменте мы обсудили процесс обучения однослойной нейросети на датасете MNIST, включая использование функции потерь sparse categorical cross-entropy и построение матрицы ошибок. Теперь мы сосредоточимся на важности нормализации входных данных и различиях между фреймворками TensorFlow и PyTorch.

## **Нормализация данных и динамические графы вычислений**

### Нормализация данных

1. **Зачем нужна нормализация:** Нормализация входных данных — это процесс приведения значений к одному диапазону, что помогает улучшить обучение нейросети. Если входные данные имеют разные масштабы, это может привести к проблемам с оптимизацией, так как поверхность ошибок становится плоской и нерегулярной. Это затрудняет процесс обучения, так как градиенты могут быть слишком малыми или слишком большими.

2. **Преимущества нормализации:** Нормализация помогает обеспечить, чтобы веса нейросети были инициализированы с учетом того, что входные сигналы находятся в диапазоне от -1 до 1 или от 0 до 1. Это позволяет избежать проблем с потерей точности при вычислениях, особенно когда числа имеют разные масштабы.

### Динамические графы вычислений

3. **Что такое динамический граф:** Динамический граф вычислений — это подход, при котором граф вычислений создается на лету, что позволяет более гибко управлять вычислениями. В отличие от статического графа, где граф создается заранее и затем выполняется, динамический граф позволяет изменять структуру графа во время выполнения.

4. **Преимущества PyTorch:** PyTorch, разработанный компанией Meta, стал популярным благодаря своей простоте и удобству использования динамических графов. Это позволяет разработчикам легко изменять архитектуру модели и выполнять операции с тензорами без необходимости предварительного определения графа.

### Физический и геометрический смысл

Нормализация данных позволяет нейросети более эффективно обучаться, так как она помогает избежать проблем с масштабами входных данных. Динамические графы вычислений в PyTorch обеспечивают большую гибкость и удобство при разработке моделей, позволяя разработчикам легко изменять архитектуру и выполнять операции с тензорами.

Таким образом, понимание нормализации данных и динамических графов вычислений является важным аспектом в разработке и обучении нейросетей. Теперь, когда мы обсудили эти концепции, мы можем перейти к анализу производительности модели и возможным улучшениям.

## Chunk 14

### **Название фрагмента: Сравнение фреймворков TensorFlow, PyTorch и JAX**

**Предыдущий контекст:** В предыдущем фрагменте мы обсудили важность нормализации данных и динамических графов вычислений в нейросетях, а также их влияние на обучение и производительность моделей. Теперь мы сосредоточимся на сравнении различных фреймворков для глубокого обучения, таких как TensorFlow, PyTorch и JAX.

## **Сравнение фреймворков для глубокого обучения**

### TensorFlow и Keras

1. **TensorFlow:** Это мощный фреймворк, разработанный Google, который предоставляет множество инструментов для создания и обучения нейросетей. TensorFlow изначально использовал статические графы вычислений, что требовало предварительного определения структуры модели перед ее выполнением.

2. **Keras:** Это высокоуровневая библиотека, которая работает поверх TensorFlow и упрощает создание нейросетей. Keras позволяет быстро и удобно строить модели, используя простые и интуитивно понятные команды. Это делает его популярным выбором для обучения и прототипирования.

### PyTorch

3. **PyTorch:** Разработанный Meta, PyTorch стал популярным благодаря своей простоте и удобству использования динамических графов вычислений. Это позволяет разработчикам легко изменять архитектуру модели и выполнять операции с тензорами без необходимости предварительного определения графа. PyTorch также имеет аналог Keras, называемый PyTorch Lightning, который упрощает создание и обучение моделей, но менее интегрирован в сам фреймворк.

### JAX

4. **JAX:** Это более новый фреймворк от Google, который ориентирован на символическое дифференцирование и автоматизацию вычислений. JAX позволяет выполнять операции на GPU и TPU, что делает его мощным инструментом для научных вычислений. Однако JAX не имеет такой же удобной высокоуровневой библиотеки, как Keras, что может усложнить процесс разработки.

### Преимущества и недостатки

- **TensorFlow с Keras:** Отлично подходит для быстрого прототипирования и создания сложных моделей. Хорошо интегрирован и имеет множество инструментов для визуализации и отладки.
- **PyTorch:** Предлагает большую гибкость и простоту в использовании, особенно для исследователей и разработчиков, которые хотят быстро тестировать идеи. Однако может потребоваться больше кода для реализации сложных моделей.
- **JAX:** Идеален для научных вычислений и символического дифференцирования, но требует больше усилий для создания моделей, так как не имеет высокоуровневых абстракций, как Keras.

### Заключение

Каждый из этих фреймворков имеет свои сильные и слабые стороны, и выбор между ними зависит от конкретных задач и предпочтений разработчика. TensorFlow и Keras отлично подходят для быстрого создания и развертывания моделей, в то время как PyTorch предлагает большую гибкость для исследовательских проектов. JAX, в свою очередь, является мощным инструментом для научных вычислений, но требует более глубокого понимания для эффективного использования.

Теперь, когда мы обсудили различные фреймворки для глубокого обучения, мы можем перейти к практическим примерам и лабораторным работам, которые помогут закрепить полученные знания.

## Final Summary
### **Сводка текста: Создание и обучение нейросетей с использованием различных фреймворков**

В данной статье рассматривается процесс создания и обучения нейросетей, начиная с однослойной модели и переходя к более сложным архитектурам, таким как двухслойные нейросети. Обсуждаются параметры сети, включая матрицы весов и смещения, а также важность нормализации входных данных для улучшения обучения. 

Основное внимание уделяется различиям между фреймворками TensorFlow, Keras, PyTorch и JAX. TensorFlow, в сочетании с Keras, предлагает мощные инструменты для быстрого прототипирования и создания моделей, в то время как PyTorch обеспечивает большую гибкость благодаря динамическим графам вычислений. JAX, ориентированный на научные вычисления, предоставляет возможности для автоматического дифференцирования, но требует больше усилий для разработки.

В статье также рассматриваются функции потерь, такие как categorical cross-entropy и sparse categorical cross-entropy, а также методы визуализации, включая матрицы ошибок и веса нейросетей. Эти инструменты помогают анализировать производительность моделей и вносить необходимые коррективы в их архитектуру.

В заключение, выбор фреймворка зависит от конкретных задач и предпочтений разработчика, и каждый из них имеет свои преимущества и недостатки.
