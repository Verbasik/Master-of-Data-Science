# Оглавление

**I. Сложность задач и интуиция в решении**
*   Факторы, определяющие сложность задач
*   Роль интуиции в решении задач
*   Математическая формализация интуиции через префиксные суммы

**II. Идея задачи и интуиция в решении**
*   Выделение сути задачи и ключевых моментов
*   Интуитивное понимание и креативный подход
*   Использование словарей для упрощения решения задач

**III. Структура курса и дедлайны**
*   Индивидуальный подход к объему и сложности заданий
*   Важность соблюдения дедлайнов
*   Математическая формализация планирования задач

**IV. Бинарный поиск и его применение**
*   Принцип работы бинарного поиска на отсортированных массивах
*   Задача поиска минимального элемента больше или равного заданному значению
*   Оптимизация поиска с использованием бинарного поиска
*   Алгоритм бинарного поиска для нахождения минимального элемента
*   Формулировка успеха в бинарном поиске
*   Реализация бинарного поиска для нахождения подходящего элемента
*   Поиск максимального индекса элемента не больше заданного
*   Поиск минимального индекса элемента не меньше заданного
*   Поиск элемента в отсортированном массиве с использованием бинарного поиска
*   Реализация бинарного поиска для проверки наличия элемента
*   Поиск количества чисел в заданном диапазоне
*   Поиск индексов в заданном диапазоне с использованием бинарного поиска
*   Оптимизация поиска с использованием сортировки и бинарного поиска

**V. Метрики оценки качества предсказаний**
*   Введение в математическое ожидание и дисперсию
*   Центральная предельная теорема и её применение
*   Вычисление математического ожидания и дисперсии для нормального распределения
*   Вычисление математического ожидания и дисперсии модуля случайной величины
*   Метрики MAPE и S-MAPE в оценке качества предсказаний
*   Проблемы с MAPE и преимущества S-MAPE
*   Симметричность S-MAPE и её ограничения
*   Сравнение метрик MAE и MSE
*   Понимание R-квадрат и его применение
*   Понимание относительных метрик и их применение
*   Гибридные метрики: Huber и Log-Cosh
*   Квантильная функция потери и её применение
*   Сравнение MSE и MAE в контексте выбросов
*   Влияние выбросов на MSE и MAE
*   Влияние выбросов на MSE и MAE, и применение Huber
*   Квантильные интервалы и метод Bootstrap
*   Метод Bootstrap для оценки качества моделей
*   Доверительные интервалы и центральная предельная теорема
*   Доверительные интервалы для MAE и MSE с использованием Bootstrap и ЦПТ
*   Понимание распределений и их влияние на метрики

**VI. Заключение и рекомендации по курсу**
*   Основные выводы по метрикам и их применению
*   Рекомендации по практике и дальнейшему обучению


# Введение

В данной лекции будут рассмотрены следующте аспекты, связанные со сложностью задач, интуицией в их решении и эффективными методами поиска в отсортированных данных. Также будет затронута тема оценки качества предсказательных моделей, включая различные метрики и статистические методы.

Первая часть лекции посвящена **пониманию сложности задач и развитию интуиции**. Будут рассмотрены факторы, влияющие на восприятие сложности, такие как недостаток теоретической базы и отсутствие опыта. Особое внимание будет уделено роли интуиции и креативного подхода в процессе решения задач, а также математическим инструментам, которые могут помочь в формализации интуитивных решений, например, префиксные суммы и словари.

Далее будет рассмотрен **бинарный поиск как эффективный метод для работы с отсортированными данными**. Этот алгоритм является основой для многих других алгоритмов и методов, и его понимание критически важно для решения широкого круга задач. Будут изучены различные вариации бинарного поиска, включая поиск минимального или максимального индекса элемента, а также проверка наличия элемента в массиве.

В заключительной части лекции будут представлены **метрики для оценки качества предсказаний моделей**, такие как MSE, MAE, MAPE, S-MAPE, Huber и Log-Cosh. Будет обсуждаться, как **выбросы в данных могут влиять на эти метрики** и как использовать метод Bootstrap для построения доверительных интервалов. Также будет рассмотрена центральная предельная теорема и её роль в построении доверительных интервалов.


# Глассарий терминов

*   **Префиксные суммы** – метод, используемый для эффективного решения задач, связанных с суммированием подмножеств данных. Для массива $A$ длиной $n$, префиксная сумма $P$ определяется как $P[i] = A + A + ... + A[i]$, что позволяет быстро вычислять суммы подотрезков массива.

*   **Бинарный поиск** – эффективный алгоритм для поиска элемента в отсортированном массиве, который работает, деля массив пополам на каждом шаге и сравнивая искомый элемент с элементом в середине массива. Используется для нахождения минимального элемента, большего или равного заданному значению, а также для поиска индексов в заданном диапазоне.

*   **Математическое ожидание (мат. ожидание)** – среднее значение случайной величины $X$, обозначается как $E(X)$ и вычисляется по формуле $ E(X) = \int_{-\infty}^{+\infty} x \cdot f(x) , dx $, где $f(x)$ – функция плотности вероятности случайной величины $X$.

*   **Дисперсия** – мера степени разброса значений случайной величины относительно её математического ожидания, обозначается как $D(X)$ и вычисляется по формуле $ D(X) = E((X - E(X))^2) = E(X^2) - (E(X))^2 $, где $E(X^2)$ – математическое ожидание квадрата случайной величины.

*   **Центральная предельная теорема (ЦПТ)** – утверждает, что при достаточно большом количестве независимых и одинаково распределенных случайных величин, их сумма (или среднее) будет стремиться к нормальному распределению, независимо от формы исходного распределения.

*   **Средняя абсолютная ошибка (MAE)** – среднее значение абсолютных ошибок между истинными и предсказанными значениями, вычисляется по формуле $ \text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i| $.

*   **Средняя квадратичная ошибка (MSE)** – среднее значение квадратов ошибок, вычисляется по формуле $ \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 $. MSE более чувствительна к выбросам.

*   **R-квадрат (коэффициент детерминации)** – метрика, измеряющая, насколько хорошо предсказанные значения модели соответствуют истинным значениям, позволяющая понять, какую долю вариации в данных объясняет модель.

*   **MAPE (Mean Absolute Percentage Error)** – относительная метрика, показывающая среднее абсолютное процентное отклонение предсказанных значений от истинных значений, вычисляется по формуле $ \text{MAPE} = \frac{1}{n} \sum_{i=1}^{n} \left| \frac{y_i - \hat{y}_i}{y_i} \right| \times 100% $.

*   **S-MAPE (Symmetric Mean Absolute Percentage Error)** – относительная метрика, учитывающая как истинные, так и предсказанные значения в знаменателе, что делает её более устойчивой к малым значениям, вычисляется по формуле $ \text{S-MAPE} = \frac{1}{n} \sum_{i=1}^{n} \frac{|y_i - \hat{y}_i|}{|y_i| + |\hat{y}_i|} \times 100% $.

*   **Метрика Huber** – гибридная метрика, объединяющая преимущества MAE и MSE, определяемая как:
    $ L_{\delta}(y, \hat{y}) = \begin{cases} \frac{1}{2}(y - \hat{y})^2 & \text{если } |y - \hat{y}| \leq \delta \ \delta \cdot (|y - \hat{y}| - \frac{1}{2}\delta) & \text{иначе} \end{cases} $, где $\delta$ – порог, определяющий, когда использовать MSE или MAE.

*   **Метрика Log-Cosh** – гибридная метрика, определяемая как $ L(y, \hat{y}) = \log(\cosh(y - \hat{y})) $, где $\cosh(x)$ – гиперболический косинус, вычисляемый как $ \cosh(x) = \frac{e^x + e^{-x}}{2} $. Ведет себя как MSE для небольших ошибок и как MAE для больших ошибок, что делает её устойчивой к выбросам.

*   **Квантильная функция потери** – метрика, позволяющая настраивать модель в зависимости от того, насколько критично недопрогнозировать или перепрогнозировать значения, определяемая как: $ L(y, \hat{y}) = \begin{cases} \theta (y - \hat{y}) & \text{если } y - \hat{y} < 0 \ (1 - \theta)(y - \hat{y}) & \text{иначе} \end{cases} $, где $\theta$ – гиперпараметр.

*   **Метод Bootstrap** – статистический метод, позволяющий оценить распределение статистики на основе повторных выборок из имеющихся данных, используемый для построения доверительных интервалов.

*   **Доверительные интервалы** – диапазон значений, в котором с определенной вероятностью находится истинное значение метрики, показывающий надежность результатов модели.

---

# Summarization for Text

## Chunk 1

### **Название фрагмента [Обсуждение сложности задач и интуиции в решении]:**

**Предыдущий контекст:** В предыдущем фрагменте обсуждалась сложность выполнения домашних заданий, где студенты выражали свои чувства по поводу объема задач и недостатка теории. 

## **Сложность задач и интуиция в решении**

В этом фрагменте основное внимание уделяется сложности задач, которые студенты должны решать, и тому, как интуиция играет важную роль в процессе решения. Студенты отмечают, что некоторые задачи кажутся им сложными из-за отсутствия опыта с подобными задачами, в то время как другие могут находить их очевидными. Это подчеркивает важность индивидуального подхода к обучению и пониманию.

Сложность задач может быть обусловлена несколькими факторами:
1. **Недостаток теории:** Если студенты не имеют достаточной теоретической базы, они могут испытывать трудности при решении задач.
2. **Новый опыт:** Для студентов, которые впервые сталкиваются с определенными типами задач, они могут показаться сложными, даже если для других они являются простыми.
3. **Разные уровни подготовки:** Уровень подготовки студентов может варьироваться, что также влияет на восприятие сложности.

Интуиция в решении задач — это способность предчувствовать, как подойти к решению, основываясь на предыдущем опыте и знаниях. Это может включать в себя понимание паттернов и подводных камней, которые могут возникнуть в процессе решения.

Математическая формализация интуиции может быть представлена через концепцию префиксных сумм, которая позволяет эффективно решать задачи, связанные с суммированием подмножеств данных. Например, если у нас есть массив $A$ длиной $n$, префиксная сумма $P$ может быть определена как:

$$
P[i] = A[0] + A[1] + ... + A[i]
$$

где $P[i]$ — это сумма элементов массива от индекса 0 до $i$. Это позволяет быстро вычислять суммы подотрезков массива, что значительно упрощает решение задач.

Пример кода для вычисления префиксной суммы:

```python
def prefix_sum(arr):
    """
    Описание:
        Функция для вычисления префиксной суммы массива.

    Аргументы:
        arr: Список чисел.

    Возвращает:
        Список префиксных сумм.

    Примеры:
        >>> prefix_sum([1, 2, 3, 4])
        [1, 3, 6, 10]
    """
    n = len(arr)  # Получаем длину массива
    prefix_sums = [0] * n  # Инициализируем массив префиксных сумм
    prefix_sums[0] = arr[0]  # Первая префиксная сумма равна первому элементу

    # Вычисляем префиксные суммы
    for i in range(1, n):
        prefix_sums[i] = prefix_sums[i - 1] + arr[i]  # Суммируем текущий элемент с предыдущей префиксной суммой

    return prefix_sums  # Возвращаем массив префиксных сумм
```

Этот код создает массив префиксных сумм, который позволяет быстро находить сумму элементов в любом подотрезке массива. Например, чтобы найти сумму элементов с индексами от $l$ до $r$, можно использовать:

$$
\text{sum}(A[l:r]) = P[r] - P[l-1]
$$

где $P$ — массив префиксных сумм.

Физический смысл префиксной суммы можно проиллюстрировать на примере задачи о нахождении общего расстояния, пройденного автомобилем за определенное время. Если у нас есть массив, где каждый элемент представляет собой расстояние, пройденное за час, префиксная сумма позволит быстро вычислить общее расстояние за любые два часа.

Таким образом, понимание сложности задач и развитие интуиции в решении являются ключевыми аспектами успешного обучения и выполнения заданий.

## Chunk 3
### **Название фрагмента [Идея задачи и интуиция в решении]:**

**Предыдущий контекст:** В предыдущем фрагменте обсуждались сложности выполнения задач и важность интуиции в их решении. Студенты выражали свои чувства по поводу объема задач и недостатка теории, что подчеркивало необходимость индивидуального подхода к обучению.

## **Идея задачи и интуиция в решении**

В этом фрагменте акцентируется внимание на том, как важно понимать основную идею задачи и интуитивно подходить к ее решению. Студенты должны уметь кратко формулировать суть задачи, выделяя ключевые моменты, которые помогут в ее решении. Это может быть как техническая задача, так и творческая, требующая нестандартного подхода.

Ключевые аспекты, которые следует учитывать при решении задач:
1. **Суть задачи:** Необходимо уметь выделить основную идею задачи и сформулировать ее в нескольких предложениях. Это помогает сосредоточиться на главном и не отвлекаться на второстепенные детали.
2. **Интуиция:** Интуитивное понимание задачи может помочь в нахождении решения. Это включает в себя использование известных паттернов, формул и методов, которые могут быть применены к данной задаче.
3. **Креативный подход:** Иногда для решения задачи требуется нестандартный подход, например, использование шифров или других математических инструментов, таких как префиксные суммы или словари.

Важно отметить, что на вопрос о том, как именно решать задачу, нет единственно правильного ответа. Каждый студент может подойти к решению по-своему, и это нормально. Главное — это честность и оригинальность в подходе к решению.

Математическая формализация интуитивного подхода может быть представлена через использование различных алгоритмов и структур данных. Например, использование словарей для хранения значений может значительно упростить решение задач, связанных с поиском и подсчетом.

Пример кода, который демонстрирует использование словаря для подсчета частоты элементов в списке:

```python
def count_frequencies(arr):
    """
    Описание:
        Функция для подсчета частоты элементов в списке.

    Аргументы:
        arr: Список элементов.

    Возвращает:
        Словарь с элементами и их частотой.

    Примеры:
        >>> count_frequencies([1, 2, 2, 3, 1])
        {1: 2, 2: 2, 3: 1}
    """
    frequency_dict = {}  # Инициализируем пустой словарь для хранения частот

    # Проходим по каждому элементу в списке
    for item in arr:
        if item in frequency_dict:
            frequency_dict[item] += 1  # Увеличиваем счетчик, если элемент уже есть в словаре
        else:
            frequency_dict[item] = 1  # Инициализируем счетчик для нового элемента

    return frequency_dict  # Возвращаем словарь с частотами
```

Этот код позволяет быстро подсчитать, сколько раз каждый элемент встречается в списке. Это может быть полезно в задачах, где необходимо анализировать данные и находить наиболее часто встречающиеся элементы.

Физический смысл интуитивного подхода можно проиллюстрировать на примере задачи о нахождении максимального значения в наборе данных. Если у нас есть массив значений, интуитивно мы можем предположить, что максимальное значение будет находиться на краю массива или в его центре. Используя алгоритм, мы можем пройти по всем элементам и найти максимальное значение, что является простым и эффективным методом.

Таким образом, понимание идеи задачи и развитие интуиции в решении являются важными аспектами успешного обучения и выполнения заданий.

## Chunk 4
### **Название фрагмента [Структура курса и дедлайны]:**

**Предыдущий контекст:** В предыдущем фрагменте обсуждались идеи задачи и интуиция в решении, а также важность понимания сути задачи для успешного выполнения заданий.

## **Структура курса и дедлайны**

В этом фрагменте акцентируется внимание на структуре курса и организации дедлайнов для выполнения домашних заданий. Преподаватель подчеркивает, что подход к организации курса может варьироваться в зависимости от его требований и содержания. Это означает, что не всегда необходимо выделять одинаковое количество времени на каждый предмет, и что объем заданий может быть адаптирован в зависимости от сложности и целей курса.

Ключевые моменты, которые следует учитывать:
1. **Индивидуальный подход:** Каждый курс может требовать разного объема времени и усилий, и преподаватель стремится сделать задания доступными и понятными для студентов.
2. **Дедлайны:** Преподаватель не рассматривает возможность отмены дедлайнов, так как они являются важным элементом учебного процесса. Дедлайны помогают студентам организовать свое время и поддерживать регулярный темп обучения.
3. **Обратная связь:** Преподаватель открыт к обсуждению объемов заданий и готов адаптировать их в зависимости от потребностей студентов, но при этом подчеркивает важность соблюдения сроков.

Математическая формализация, связанная с дедлайнами и объемами заданий, может быть представлена через концепцию планирования задач. Например, если у нас есть $N$ задач, каждая из которых требует $t_i$ времени на выполнение, то общее время $T$ на выполнение всех задач можно выразить как:

$$
T = \sum_{i=1}^{N} t_i
$$

где $t_i$ — время, необходимое для выполнения $i$-й задачи.

Пример кода для планирования задач с учетом дедлайнов:

```python
def schedule_tasks(tasks):
    """
    Описание:
        Функция для планирования задач с учетом дедлайнов.

    Аргументы:
        tasks: Список кортежей, где каждый кортеж содержит (название задачи, время на выполнение, дедлайн).

    Возвращает:
        Список задач, которые можно выполнить в срок.

    Примеры:
        >>> schedule_tasks([("Задача 1", 2, 5), ("Задача 2", 3, 6), ("Задача 3", 1, 4)])
        ['Задача 1', 'Задача 3']
    """
    current_time = 0  # Инициализируем текущее время
    completed_tasks = []  # Список для завершенных задач

    # Сортируем задачи по дедлайну
    tasks.sort(key=lambda x: x[2])  # Сортируем по третьему элементу (дедлайн)

    for task in tasks:
        name, duration, deadline = task  # Распаковываем кортеж
        if current_time + duration <= deadline:  # Проверяем, укладываемся ли в дедлайн
            completed_tasks.append(name)  # Добавляем задачу в список завершенных
            current_time += duration  # Увеличиваем текущее время

    return completed_tasks  # Возвращаем список завершенных задач
```

Этот код позволяет планировать выполнение задач с учетом их дедлайнов. Он сортирует задачи по времени выполнения и проверяет, укладываются ли они в заданные сроки.

Физический смысл планирования задач можно проиллюстрировать на примере управления проектом. Если у нас есть несколько задач, каждая из которых требует определенного времени на выполнение, важно организовать их так, чтобы все задачи были завершены в срок. Это требует учета времени и дедлайнов, что позволяет эффективно распределять ресурсы и время.

Таким образом, структура курса и организация дедлайнов играют важную роль в процессе обучения, помогая студентам организовать свое время и успешно справляться с заданиями.

## Chunk 5
### **Название фрагмента [Бинарный поиск и его применение]:**

**Предыдущий контекст:** В предыдущем фрагменте обсуждались структура курса и дедлайны, а также важность организации времени для успешного выполнения заданий. Преподаватель подчеркивал необходимость понимания сути задач и подхода к их решению.

## **Бинарный поиск и его применение**

В этом фрагменте акцентируется внимание на методе бинарного поиска, который является эффективным способом нахождения элемента в отсортированном массиве. Преподаватель объясняет, что бинарный поиск часто используется в более сложных задачах и является основой для понимания других алгоритмов, таких как метод двух указателей.

Ключевые аспекты бинарного поиска:
1. **Принцип работы:** Бинарный поиск работает на отсортированных массивах, деля массив пополам на каждом шаге и сравнивая искомый элемент с элементом в середине массива. Это позволяет значительно сократить количество проверок по сравнению с линейным поиском.
2. **Задача поиска:** В данном случае задача заключается в нахождении минимального элемента, который больше или равен заданному значению $x$. Это может быть полезно в различных приложениях, таких как поиск в базах данных или обработка запросов.
3. **Индексы и границы:** Важно правильно управлять индексами и границами при реализации бинарного поиска, чтобы избежать ошибок и исключений.

Математическая формализация бинарного поиска может быть представлена следующим образом. Пусть $A$ — отсортированный массив, а $x$ — искомое значение. Алгоритм бинарного поиска можно описать следующим образом:

1. Установите границы: $left = 0$, $right = n - 1$, где $n$ — длина массива.
2. Пока $left \leq right$:
   - Вычислите середину: $mid = \left\lfloor \frac{left + right}{2} \right\rfloor$.
   - Если $A[mid] < x$, установите $left = mid + 1$.
   - Если $A[mid] \geq x$, установите $right = mid - 1$.

В конце алгоритма, если $left < n$, то $A[left]$ будет минимальным элементом, который больше или равен $x$.

Пример кода для реализации бинарного поиска:

```python
def binary_search(arr, x):
    """
    Описание:
        Функция для выполнения бинарного поиска в отсортированном массиве.

    Аргументы:
        arr: Отсортированный список чисел.
        x: Значение, для которого нужно найти минимальный элемент, больше или равный x.

    Возвращает:
        Минимальный элемент, который больше или равен x, или None, если такого элемента нет.

    Примеры:
        >>> binary_search([1, 2, 4, 7, 12, 15, 17], 13)
        15
    """
    left, right = 0, len(arr) - 1  # Устанавливаем границы поиска

    while left <= right:  # Пока границы не пересеклись
        mid = (left + right) // 2  # Находим середину

        if arr[mid] < x:  # Если элемент в середине меньше x
            left = mid + 1  # Ищем в правой половине
        else:  # Если элемент в середине больше или равен x
            right = mid - 1  # Ищем в левой половине

    # Если left находится в пределах массива, возвращаем элемент
    if left < len(arr):
        return arr[left]
    return None  # Если элемента нет, возвращаем None
```

Этот код реализует бинарный поиск, позволяя находить минимальный элемент, который больше или равен заданному значению $x$. Он эффективно использует свойства отсортированного массива, сокращая количество проверок.

Физический смысл бинарного поиска можно проиллюстрировать на примере поиска в библиотеке. Если у вас есть отсортированный каталог книг, бинарный поиск позволяет быстро находить нужную книгу, проверяя только половину каталога на каждом шаге, что значительно ускоряет процесс поиска.

Таким образом, бинарный поиск является мощным инструментом для работы с отсортированными данными и служит основой для понимания более сложных алгоритмов и методов.

## Chunk 6
### **Название фрагмента [Оптимизация поиска с использованием бинарного поиска]:**

**Предыдущий контекст:** В предыдущем фрагменте обсуждался бинарный поиск и его применение для нахождения минимального элемента, который больше или равен заданному значению. Преподаватель объяснял, как бинарный поиск работает на отсортированных массивах и как правильно управлять индексами.

## **Оптимизация поиска с использованием бинарного поиска**

В этом фрагменте акцентируется внимание на том, как можно оптимизировать поиск элемента в отсортированном массиве, используя бинарный поиск вместо линейного. Преподаватель подчеркивает, что использование бинарного поиска позволяет значительно сократить время выполнения алгоритма, особенно когда количество запросов велико.

Ключевые аспекты оптимизации:
1. **Сложность линейного поиска:** При использовании линейного поиска, когда мы проходим по всему массиву для нахождения элемента, сложность алгоритма составляет $O(n)$ для одного запроса. Если у нас есть $m$ запросов, общая сложность будет $O(m \cdot n)$.
2. **Преимущества бинарного поиска:** Бинарный поиск позволяет сократить время поиска до $O(\log n)$ для каждого запроса, что значительно улучшает общую производительность, особенно при большом количестве запросов.
3. **Условия для применения:** Бинарный поиск может быть применен только к отсортированным массивам, что делает его особенно эффективным в ситуациях, когда данные уже отсортированы или могут быть отсортированы заранее.

Математическая формализация бинарного поиска может быть представлена следующим образом. Пусть $A$ — отсортированный массив, а $x$ — искомое значение. Алгоритм бинарного поиска можно описать следующим образом:

1. Установите границы: $left = 0$, $right = n - 1$, где $n$ — длина массива.
2. Пока $left \leq right$:
   - Вычислите середину: $mid = \left\lfloor \frac{left + right}{2} \right\rfloor$.
   - Если $A[mid] < x$, установите $left = mid + 1$.
   - Если $A[mid] \geq x$, установите $right = mid - 1$.

В конце алгоритма, если $left < n$, то $A[left]$ будет минимальным элементом, который больше или равен $x$.

Пример кода для реализации бинарного поиска с учетом минимального элемента:

```python
def binary_search_min_greater_or_equal(arr, x):
    """
    Описание:
        Функция для выполнения бинарного поиска, чтобы найти минимальный элемент,
        который больше или равен заданному значению x.

    Аргументы:
        arr: Отсортированный список чисел.
        x: Значение, для которого нужно найти минимальный элемент, больше или равный x.

    Возвращает:
        Минимальный элемент, который больше или равен x, или None, если такого элемента нет.

    Примеры:
        >>> binary_search_min_greater_or_equal([1, 2, 4, 7, 12, 15, 17], 13)
        15
    """
    left, right = 0, len(arr) - 1  # Устанавливаем границы поиска

    while left <= right:  # Пока границы не пересеклись
        mid = (left + right) // 2  # Находим середину

        if arr[mid] < x:  # Если элемент в середине меньше x
            left = mid + 1  # Ищем в правой половине
        else:  # Если элемент в середине больше или равен x
            right = mid - 1  # Ищем в левой половине

    # Если left находится в пределах массива, возвращаем элемент
    if left < len(arr):
        return arr[left]
    return None  # Если элемента нет, возвращаем None
```

Этот код реализует бинарный поиск, позволяя находить минимальный элемент, который больше или равен заданному значению $x$. Он эффективно использует свойства отсортированного массива, сокращая количество проверок.

Физический смысл бинарного поиска можно проиллюстрировать на примере поиска в библиотеке. Если у вас есть отсортированный каталог книг, бинарный поиск позволяет быстро находить нужную книгу, проверяя только половину каталога на каждом шаге, что значительно ускоряет процесс поиска.

Таким образом, оптимизация поиска с использованием бинарного поиска является важным инструментом для работы с отсортированными данными и позволяет значительно улучшить производительность алгоритмов.

## Chunk 7
### **Название фрагмента [Алгоритм бинарного поиска для нахождения минимального элемента]:**

**Предыдущий контекст:** В предыдущем фрагменте обсуждалась оптимизация поиска с использованием бинарного поиска, который позволяет находить минимальный элемент, превышающий заданное значение, с помощью эффективного алгоритма.

## **Алгоритм бинарного поиска для нахождения минимального элемента**

В этом фрагменте акцентируется внимание на том, как работает алгоритм бинарного поиска для нахождения минимального элемента, который больше или равен заданному значению $x$. Преподаватель объясняет, как использовать свойства отсортированного массива для оптимизации поиска.

Ключевые аспекты алгоритма:
1. **Определение середины:** Алгоритм начинается с нахождения середины массива, что позволяет делить массив на две части. Если элемент в середине больше или равен $x$, это означает, что все элементы справа от него также будут больше или равны $x$.
2. **Сужение диапазона:** Если элемент в середине меньше $x$, то все элементы слева от него также будут меньше $x$, и поиск продолжается только в правой части массива. Если элемент в середине больше или равен $x$, поиск продолжается в левой части массива.
3. **Итеративный процесс:** Процесс продолжается до тех пор, пока не будет найден минимальный элемент, который больше или равен $x$. Это позволяет значительно сократить количество проверок по сравнению с линейным поиском.

Математическая формализация алгоритма бинарного поиска может быть представлена следующим образом. Пусть $A$ — отсортированный массив, а $x$ — искомое значение. Алгоритм можно описать так:

1. Установите границы: $left = 0$, $right = n - 1$, где $n$ — длина массива.
2. Пока $left \leq right$:
   - Вычислите середину: $mid = \left\lfloor \frac{left + right}{2} \right\rfloor$.
   - Если $A[mid] < x$, установите $left = mid + 1$.
   - Если $A[mid] \geq x$, установите $right = mid - 1$.

В конце алгоритма, если $left < n$, то $A[left]$ будет минимальным элементом, который больше или равен $x$.

Пример кода для реализации бинарного поиска с нахождением минимального элемента:

```python
def binary_search_min_greater_or_equal(arr, x):
    """
    Описание:
        Функция для выполнения бинарного поиска, чтобы найти минимальный элемент,
        который больше или равен заданному значению x.

    Аргументы:
        arr: Отсортированный список чисел.
        x: Значение, для которого нужно найти минимальный элемент, больше или равный x.

    Возвращает:
        Минимальный элемент, который больше или равен x, или None, если такого элемента нет.

    Примеры:
        >>> binary_search_min_greater_or_equal([1, 2, 4, 7, 12, 15, 17], 10)
        12
    """
    left, right = 0, len(arr) - 1  # Устанавливаем границы поиска

    while left <= right:  # Пока границы не пересеклись
        mid = (left + right) // 2  # Находим середину

        if arr[mid] < x:  # Если элемент в середине меньше x
            left = mid + 1  # Ищем в правой половине
        else:  # Если элемент в середине больше или равен x
            right = mid - 1  # Ищем в левой половине

    # Если left находится в пределах массива, возвращаем элемент
    if left < len(arr):
        return arr[left]
    return None  # Если элемента нет, возвращаем None
```

Этот код реализует бинарный поиск, позволяя находить минимальный элемент, который больше или равен заданному значению $x$. Он эффективно использует свойства отсортированного массива, сокращая количество проверок.

Физический смысл бинарного поиска можно проиллюстрировать на примере поиска в магазине. Если у вас есть отсортированный каталог товаров по цене, бинарный поиск позволяет быстро находить товар, цена которого минимально превышает заданную сумму, что значительно ускоряет процесс выбора.

Таким образом, алгоритм бинарного поиска является мощным инструментом для работы с отсортированными данными и позволяет эффективно находить минимальные элементы, соответствующие заданным условиям.

## Chunk 8
### **Название фрагмента [Формулировка успеха в бинарном поиске]:**

**Предыдущий контекст:** В предыдущем фрагменте обсуждался алгоритм бинарного поиска для нахождения минимального элемента, который больше или равен заданному значению. Преподаватель объяснял, как использовать свойства отсортированного массива для оптимизации поиска.

## **Формулировка успеха в бинарном поиске**

В этом фрагменте акцентируется внимание на том, как правильно формулировать задачу поиска в контексте бинарного поиска. Преподаватель подчеркивает, что успех в данной задаче заключается в нахождении минимального элемента, который больше или равен заданному значению $x$. Это требует понимания монотонности и правильного подхода к формулировке условий поиска.

Ключевые аспекты формулировки успеха:
1. **Определение успеха:** Успех в данной задаче заключается в нахождении такого значения $y$, чтобы $y \geq x$ и $y$ было минимальным. Это означает, что мы ищем первый элемент, который удовлетворяет этому условию.
2. **Монотонность:** Важно понимать, что элементы массива могут быть упорядочены, и если мы находим элемент, который больше или равен $x$, то все элементы справа от него также будут больше или равны $x$. Это позволяет сужать диапазон поиска.
3. **Поиск первого подходящего элемента:** Алгоритм должен быть настроен на нахождение первого элемента, который удовлетворяет условию, а не просто любого элемента, который больше или равен $x$.

Математическая формализация поиска может быть представлена следующим образом. Пусть $A$ — отсортированный массив, а $x$ — искомое значение. Алгоритм бинарного поиска можно описать так:

1. Установите границы: $left = 0$, $right = n - 1$, где $n$ — длина массива.
2. Пока $left \leq right$:
   - Вычислите середину: $mid = \left\lfloor \frac{left + right}{2} \right\rfloor$.
   - Если $A[mid] < x$, установите $left = mid + 1$.
   - Если $A[mid] \geq x$, установите $right = mid - 1$.

В конце алгоритма, если $left < n$, то $A[left]$ будет минимальным элементом, который больше или равен $x$.

Пример кода для реализации бинарного поиска с нахождением минимального элемента:

```python
def binary_search_first_greater_or_equal(arr, x):
    """
    Описание:
        Функция для выполнения бинарного поиска, чтобы найти первый элемент,
        который больше или равен заданному значению x.

    Аргументы:
        arr: Отсортированный список чисел.
        x: Значение, для которого нужно найти первый элемент, больше или равный x.

    Возвращает:
        Первый элемент, который больше или равен x, или None, если такого элемента нет.

    Примеры:
        >>> binary_search_first_greater_or_equal([1, 2, 4, 7, 12, 15, 17], 10)
        12
    """
    left, right = 0, len(arr) - 1  # Устанавливаем границы поиска

    while left <= right:  # Пока границы не пересеклись
        mid = (left + right) // 2  # Находим середину

        if arr[mid] < x:  # Если элемент в середине меньше x
            left = mid + 1  # Ищем в правой половине
        else:  # Если элемент в середине больше или равен x
            right = mid - 1  # Ищем в левой половине

    # Если left находится в пределах массива, возвращаем элемент
    if left < len(arr):
        return arr[left]
    return None  # Если элемента нет, возвращаем None
```

Этот код реализует бинарный поиск, позволяя находить первый элемент, который больше или равен заданному значению $x$. Он эффективно использует свойства отсортированного массива, сокращая количество проверок.

Физический смысл бинарного поиска можно проиллюстрировать на примере поиска в магазине. Если у вас есть отсортированный каталог товаров по цене, бинарный поиск позволяет быстро находить товар, цена которого минимально превышает заданную сумму, что значительно ускоряет процесс выбора.

Таким образом, правильная формулировка успеха в бинарном поиске и понимание монотонности являются ключевыми аспектами для эффективного поиска в отсортированных данных.

## Chunk 9
### **Название фрагмента [Реализация бинарного поиска для нахождения подходящего элемента]:**

**Предыдущий контекст:** В предыдущем фрагменте обсуждалась формулировка успеха в бинарном поиске, а также важность монотонности и правильного подхода к формулировке условий поиска. Преподаватель подчеркивал, что успех заключается в нахождении минимального элемента, который больше или равен заданному значению.

## **Реализация бинарного поиска для нахождения подходящего элемента**

В этом фрагменте акцентируется внимание на практической реализации алгоритма бинарного поиска для нахождения минимального элемента, который больше или равен заданному значению $x$. Преподаватель объясняет, как правильно настроить границы поиска и как использовать условия для нахождения нужного элемента.

Ключевые аспекты реализации:
1. **Инициализация границ:** Начальные границы поиска устанавливаются как $L = 0$ (начало массива) и $R = \text{len}(Ray) - 1$ (конец массива).
2. **Цикл поиска:** Используется цикл `while`, который продолжается до тех пор, пока $L \leq R$. Внутри цикла вычисляется середина массива $mid$.
3. **Условия для сужения диапазона:** Если элемент в середине массива удовлетворяет условию (то есть больше или равен $x$), то необходимо проверить, можно ли найти более подходящий элемент, сдвигая правую границу. В противном случае сдвигается левая граница.

Математическая формализация алгоритма может быть представлена следующим образом:

1. Установите границы: $L = 0$, $R = n - 1$, где $n$ — длина массива.
2. Пока $L \leq R$:
   - Вычислите середину: $mid = \left\lfloor \frac{L + R}{2} \right\rfloor$.
   - Если $A[mid] < x$, установите $L = mid + 1$.
   - Если $A[mid] \geq x$, установите $R = mid - 1$.

В конце алгоритма, если $L < n$, то $A[L]$ будет минимальным элементом, который больше или равен $x$.

Пример кода для реализации бинарного поиска:

```python
def binary_search_min_greater_or_equal(arr, x):
    """
    Описание:
        Функция для выполнения бинарного поиска, чтобы найти минимальный элемент,
        который больше или равен заданному значению x.

    Аргументы:
        arr: Отсортированный список чисел.
        x: Значение, для которого нужно найти минимальный элемент, больше или равный x.

    Возвращает:
        Минимальный элемент, который больше или равен x, или None, если такого элемента нет.

    Примеры:
        >>> binary_search_min_greater_or_equal([1, 2, 4, 7, 12, 15, 17], 13)
        15
    """
    L, R = 0, len(arr) - 1  # Устанавливаем границы поиска

    while L <= R:  # Пока границы не пересеклись
        mid = (L + R) // 2  # Находим середину

        if arr[mid] < x:  # Если элемент в середине меньше x
            L = mid + 1  # Ищем в правой половине
        else:  # Если элемент в середине больше или равен x
            R = mid - 1  # Ищем в левой половине

    # Если L находится в пределах массива, возвращаем элемент
    if L < len(arr):
        return arr[L]
    return None  # Если элемента нет, возвращаем None
```

Этот код реализует бинарный поиск, позволяя находить минимальный элемент, который больше или равен заданному значению $x$. Он эффективно использует свойства отсортированного массива, сокращая количество проверок.

Физический смысл бинарного поиска можно проиллюстрировать на примере поиска в библиотеке. Если у вас есть отсортированный каталог книг по цене, бинарный поиск позволяет быстро находить книгу, цена которой минимально превышает заданную сумму, что значительно ускоряет процесс выбора.

Таким образом, реализация бинарного поиска является важным инструментом для работы с отсортированными данными и позволяет эффективно находить минимальные элементы, соответствующие заданным условиям.

## Chunk 10
### **Название фрагмента [Формулировка алгоритма бинарного поиска]:**

**Предыдущий контекст:** В предыдущем фрагменте обсуждалась реализация бинарного поиска для нахождения подходящего элемента, а также важность правильной настройки границ поиска и условий для нахождения нужного элемента.

## **Формулировка алгоритма бинарного поиска**

В этом фрагменте акцентируется внимание на формулировке алгоритма бинарного поиска, который позволяет находить минимальный элемент, удовлетворяющий заданному условию. Преподаватель объясняет, как правильно настроить границы поиска и как использовать условия для нахождения нужного элемента.

Ключевые аспекты алгоритма:
1. **Определение границ:** Начальные границы поиска устанавливаются как $L = 0$ (начало массива) и $R = \text{len}(Ray) - 1$ (конец массива). Это позволяет определить диапазон, в котором будет происходить поиск.
2. **Цикл поиска:** Используется цикл `while`, который продолжается до тех пор, пока $L \leq R$. Внутри цикла вычисляется середина массива $mid$.
3. **Условия для сужения диапазона:** Если элемент в середине массива удовлетворяет условию (то есть больше или равен $x$), то необходимо проверить, можно ли найти более подходящий элемент, сдвигая правую границу. В противном случае сдвигается левая граница.

Математическая формализация алгоритма может быть представлена следующим образом:

1. Установите границы: $L = 0$, $R = n - 1$, где $n$ — длина массива.
2. Пока $L \leq R$:
   - Вычислите середину: $mid = \left\lfloor \frac{L + R}{2} \right\rfloor$.
   - Если $A[mid] < x$, установите $L = mid + 1$.
   - Если $A[mid] \geq x$, установите $R = mid - 1$.

В конце алгоритма, если $L < n$, то $A[L]$ будет минимальным элементом, который больше или равен $x$.

Пример кода для реализации бинарного поиска:

```python
def binary_search_min_greater_or_equal(arr, x):
    """
    Описание:
        Функция для выполнения бинарного поиска, чтобы найти минимальный элемент,
        который больше или равен заданному значению x.

    Аргументы:
        arr: Отсортированный список чисел.
        x: Значение, для которого нужно найти минимальный элемент, больше или равный x.

    Возвращает:
        Минимальный элемент, который больше или равен x, или None, если такого элемента нет.

    Примеры:
        >>> binary_search_min_greater_or_equal([1, 2, 4, 7, 12, 15, 17], 13)
        15
    """
    L, R = 0, len(arr) - 1  # Устанавливаем границы поиска

    while L <= R:  # Пока границы не пересеклись
        mid = (L + R) // 2  # Находим середину

        if arr[mid] < x:  # Если элемент в середине меньше x
            L = mid + 1  # Ищем в правой половине
        else:  # Если элемент в середине больше или равен x
            R = mid - 1  # Ищем в левой половине

    # Если L находится в пределах массива, возвращаем элемент
    if L < len(arr):
        return arr[L]
    return None  # Если элемента нет, возвращаем None
```

Этот код реализует бинарный поиск, позволяя находить минимальный элемент, который больше или равен заданному значению $x$. Он эффективно использует свойства отсортированного массива, сокращая количество проверок.

Физический смысл бинарного поиска можно проиллюстрировать на примере поиска в магазине. Если у вас есть отсортированный каталог товаров по цене, бинарный поиск позволяет быстро находить товар, цена которого минимально превышает заданную сумму, что значительно ускоряет процесс выбора.

Таким образом, формулировка алгоритма бинарного поиска и правильная настройка границ поиска являются ключевыми аспектами для эффективного нахождения минимальных элементов, соответствующих заданным условиям.

## Chunk 11
### **Название фрагмента [Поиск максимального индекса элемента в отсортированном массиве]:**

**Предыдущий контекст:** В предыдущем фрагменте обсуждалась реализация бинарного поиска для нахождения минимального элемента, который больше или равен заданному значению. Преподаватель объяснял, как правильно настроить границы поиска и условия для нахождения нужного элемента.

## **Поиск максимального индекса элемента в отсортированном массиве**

В этом фрагменте акцентируется внимание на задаче поиска максимального индекса элемента в отсортированном массиве, который не превышает заданное значение. Преподаватель объясняет, как использовать бинарный поиск для решения этой задачи, учитывая, что массив отсортирован по неубыванию.

Ключевые аспекты задачи:
1. **Определение задачи:** Необходимо найти максимальный индекс элемента в массиве, который не больше заданного значения. Это означает, что мы ищем последний элемент, который удовлетворяет условию $A[i] \leq x$.
2. **Использование бинарного поиска:** Бинарный поиск позволяет эффективно находить нужный индекс, сокращая количество проверок по сравнению с линейным поиском.
3. **Формулировка успеха:** Успех в данной задаче заключается в нахождении индекса, который соответствует максимальному элементу, не превышающему заданное значение.

Математическая формализация задачи может быть представлена следующим образом. Пусть $A$ — отсортированный массив, а $x$ — искомое значение. Алгоритм бинарного поиска можно описать так:

1. Установите границы: $L = 0$, $R = n - 1$, где $n$ — длина массива.
2. Пока $L \leq R$:
   - Вычислите середину: $mid = \left\lfloor \frac{L + R}{2} \right\rfloor$.
   - Если $A[mid] \leq x$, установите $answer = mid$ и $L = mid + 1$ (ищем дальше вправо).
   - Если $A[mid] > x$, установите $R = mid - 1$ (ищем в левой половине).

В конце алгоритма, если $answer$ был обновлен, то он будет максимальным индексом элемента, который не превышает $x$.

Пример кода для реализации бинарного поиска для нахождения максимального индекса:

```python
def binary_search_max_index(arr, x):
    """
    Описание:
        Функция для выполнения бинарного поиска, чтобы найти максимальный индекс элемента,
        который не больше заданного значения x.

    Аргументы:
        arr: Отсортированный список чисел.
        x: Значение, для которого нужно найти максимальный индекс элемента, не превышающего x.

    Возвращает:
        Максимальный индекс элемента, который не больше x, или -1, если такого элемента нет.

    Примеры:
        >>> binary_search_max_index([1, 2, 4, 7, 12, 15, 17], 10)
        3
    """
    L, R = 0, len(arr) - 1  # Устанавливаем границы поиска
    answer = -1  # Изначально ответ не найден

    while L <= R:  # Пока границы не пересеклись
        mid = (L + R) // 2  # Находим середину

        if arr[mid] <= x:  # Если элемент в середине меньше или равен x
            answer = mid  # Обновляем ответ
            L = mid + 1  # Ищем в правой половине
        else:  # Если элемент в середине больше x
            R = mid - 1  # Ищем в левой половине

    return answer  # Возвращаем максимальный индекс или -1, если не найден
```

Этот код реализует бинарный поиск, позволяя находить максимальный индекс элемента, который не превышает заданное значение $x$. Он эффективно использует свойства отсортированного массива, сокращая количество проверок.

Физический смысл данной задачи можно проиллюстрировать на примере поиска в магазине. Если у вас есть отсортированный каталог товаров по цене, задача заключается в нахождении последнего товара, цена которого не превышает заданную сумму. Это позволяет быстро определять, какие товары доступны для покупки в рамках бюджета.

Таким образом, поиск максимального индекса элемента в отсортированном массиве с использованием бинарного поиска является важным инструментом для работы с отсортированными данными и позволяет эффективно находить элементы, соответствующие заданным условиям.

## Chunk 12
### **Название фрагмента [Поиск максимального индекса элемента не больше заданного]:**

**Предыдущий контекст:** В предыдущем фрагменте обсуждалась реализация бинарного поиска для нахождения подходящего элемента, а также формулировка успеха в контексте поиска минимального элемента, который больше или равен заданному значению.

## **Поиск максимального индекса элемента не больше заданного**

В этом фрагменте акцентируется внимание на задаче поиска максимального индекса элемента в отсортированном массиве, который не превышает заданное значение. Преподаватель объясняет, как использовать бинарный поиск для решения этой задачи, учитывая, что массив отсортирован по неубыванию.

Ключевые аспекты задачи:
1. **Определение задачи:** Необходимо найти максимальный индекс элемента в массиве, который не больше заданного значения $x$. Это означает, что мы ищем последний элемент, который удовлетворяет условию $A[i] \leq x$.
2. **Использование бинарного поиска:** Бинарный поиск позволяет эффективно находить нужный индекс, сокращая количество проверок по сравнению с линейным поиском.
3. **Формулировка успеха:** Успех в данной задаче заключается в нахождении индекса, который соответствует максимальному элементу, не превышающему заданное значение.

Математическая формализация задачи может быть представлена следующим образом. Пусть $A$ — отсортированный массив, а $x$ — искомое значение. Алгоритм бинарного поиска можно описать так:

1. Установите границы: $L = 0$, $R = n - 1$, где $n$ — длина массива.
2. Пока $L \leq R$:
   - Вычислите середину: $mid = \left\lfloor \frac{L + R}{2} \right\rfloor$.
   - Если $A[mid] \leq x$, установите $answer = mid$ и $L = mid + 1$ (ищем дальше вправо).
   - Если $A[mid] > x$, установите $R = mid - 1$ (ищем в левой половине).

В конце алгоритма, если $answer$ был обновлен, то он будет максимальным индексом элемента, который не превышает $x$.

Пример кода для реализации бинарного поиска для нахождения максимального индекса:

```python
def binary_search_max_index(arr, x):
    """
    Описание:
        Функция для выполнения бинарного поиска, чтобы найти максимальный индекс элемента,
        который не больше заданного значения x.

    Аргументы:
        arr: Отсортированный список чисел.
        x: Значение, для которого нужно найти максимальный индекс элемента, не превышающего x.

    Возвращает:
        Максимальный индекс элемента, который не больше x, или -1, если такого элемента нет.

    Примеры:
        >>> binary_search_max_index([1, 2, 4, 7, 12, 15, 17], 10)
        3
    """
    L, R = 0, len(arr) - 1  # Устанавливаем границы поиска
    answer = -1  # Изначально ответ не найден

    while L <= R:  # Пока границы не пересеклись
        mid = (L + R) // 2  # Находим середину

        if arr[mid] <= x:  # Если элемент в середине меньше или равен x
            answer = mid  # Обновляем ответ
            L = mid + 1  # Ищем в правой половине
        else:  # Если элемент в середине больше x
            R = mid - 1  # Ищем в левой половине

    return answer  # Возвращаем максимальный индекс или -1, если не найден
```

Этот код реализует бинарный поиск, позволяя находить максимальный индекс элемента, который не превышает заданное значение $x$. Он эффективно использует свойства отсортированного массива, сокращая количество проверок.

Физический смысл данной задачи можно проиллюстрировать на примере поиска в магазине. Если у вас есть отсортированный каталог товаров по цене, задача заключается в нахождении последнего товара, цена которого не превышает заданную сумму. Это позволяет быстро определять, какие товары доступны для покупки в рамках бюджета.

Таким образом, поиск максимального индекса элемента в отсортированном массиве с использованием бинарного поиска является важным инструментом для работы с отсортированными данными и позволяет эффективно находить элементы, соответствующие заданным условиям.

## Chunk 13
### **Название фрагмента [Использование бинарного поиска для нахождения индекса]:**

**Предыдущий контекст:** В предыдущем фрагменте обсуждалась задача поиска максимального индекса элемента в отсортированном массиве, который не превышает заданное значение. Преподаватель объяснял, как использовать бинарный поиск для решения этой задачи.

## **Использование бинарного поиска для нахождения индекса**

В этом фрагменте акцентируется внимание на реализации бинарного поиска для нахождения максимального индекса элемента в отсортированном массиве, который не превышает заданное значение. Преподаватель объясняет, как правильно настроить алгоритм и какие условия необходимо учитывать для корректного выполнения задачи.

Ключевые аспекты реализации:
1. **Формулировка успеха:** Успех в данной задаче заключается в нахождении индекса, который соответствует максимальному элементу, не превышающему заданное значение $x$. Это означает, что мы ищем последний элемент, который удовлетворяет условию $A[i] \leq x$.
2. **Настройка границ:** Начальные границы поиска устанавливаются как $L = 0$ (начало массива) и $R = n - 1$ (конец массива). Это позволяет определить диапазон, в котором будет происходить поиск.
3. **Условия для сужения диапазона:** Если элемент в середине массива удовлетворяет условию (то есть меньше или равен $x$), то необходимо проверить, можно ли найти более подходящий элемент, сдвигая правую границу. В противном случае сдвигается левая граница.

Математическая формализация алгоритма может быть представлена следующим образом:

1. Установите границы: $L = 0$, $R = n - 1$, где $n$ — длина массива.
2. Пока $L \leq R$:
   - Вычислите середину: $mid = \left\lfloor \frac{L + R}{2} \right\rfloor$.
   - Если $A[mid] \leq x$, установите $answer = mid$ и $L = mid + 1$ (ищем дальше вправо).
   - Если $A[mid] > x$, установите $R = mid - 1$ (ищем в левой половине).

В конце алгоритма, если $answer$ был обновлен, то он будет максимальным индексом элемента, который не превышает $x$.

Пример кода для реализации бинарного поиска для нахождения максимального индекса:

```python
def binary_search_max_index(arr, x):
    """
    Описание:
        Функция для выполнения бинарного поиска, чтобы найти максимальный индекс элемента,
        который не больше заданного значения x.

    Аргументы:
        arr: Отсортированный список чисел.
        x: Значение, для которого нужно найти максимальный индекс элемента, не превышающего x.

    Возвращает:
        Максимальный индекс элемента, который не больше x, или -1, если такого элемента нет.

    Примеры:
        >>> binary_search_max_index([1, 2, 4, 7, 12, 15, 17], 10)
        3
    """
    L, R = 0, len(arr) - 1  # Устанавливаем границы поиска
    answer = -1  # Изначально ответ не найден

    while L <= R:  # Пока границы не пересеклись
        mid = (L + R) // 2  # Находим середину

        if arr[mid] <= x:  # Если элемент в середине меньше или равен x
            answer = mid  # Обновляем ответ
            L = mid + 1  # Ищем в правой половине
        else:  # Если элемент в середине больше x
            R = mid - 1  # Ищем в левой половине

    return answer  # Возвращаем максимальный индекс или -1, если не найден
```

Этот код реализует бинарный поиск, позволяя находить максимальный индекс элемента, который не превышает заданное значение $x$. Он эффективно использует свойства отсортированного массива, сокращая количество проверок.

Физический смысл данной задачи можно проиллюстрировать на примере поиска в магазине. Если у вас есть отсортированный каталог товаров по цене, задача заключается в нахождении последнего товара, цена которого не превышает заданную сумму. Это позволяет быстро определять, какие товары доступны для покупки в рамках бюджета.

Таким образом, использование бинарного поиска для нахождения максимального индекса элемента в отсортированном массиве является важным инструментом для работы с отсортированными данными и позволяет эффективно находить элементы, соответствующие заданным условиям.

## Chunk 14
### **Название фрагмента [Поиск минимального индекса элемента не меньше заданного]:**

**Предыдущий контекст:** В предыдущем фрагменте обсуждалась реализация бинарного поиска для нахождения максимального индекса элемента, который не превышает заданное значение. Преподаватель объяснял, как правильно настроить алгоритм и какие условия необходимо учитывать для корректного выполнения задачи.

## **Поиск минимального индекса элемента не меньше заданного**

В этом фрагменте акцентируется внимание на задаче поиска минимального индекса элемента в отсортированном массиве, который не меньше заданного значения. Преподаватель объясняет, как использовать бинарный поиск для решения этой задачи, учитывая, что массив отсортирован по неубыванию.

Ключевые аспекты задачи:
1. **Определение задачи:** Необходимо найти минимальный индекс элемента в массиве, который не меньше заданного значения $x$. Это означает, что мы ищем первый элемент, который удовлетворяет условию $A[i] \geq x$.
2. **Использование бинарного поиска:** Бинарный поиск позволяет эффективно находить нужный индекс, сокращая количество проверок по сравнению с линейным поиском.
3. **Формулировка успеха:** Успех в данной задаче заключается в нахождении индекса, который соответствует минимальному элементу, не меньшему заданному значению.

Математическая формализация задачи может быть представлена следующим образом. Пусть $A$ — отсортированный массив, а $x$ — искомое значение. Алгоритм бинарного поиска можно описать так:

1. Установите границы: $L = 0$, $R = n - 1$, где $n$ — длина массива.
2. Пока $L \leq R$:
   - Вычислите середину: $mid = \left\lfloor \frac{L + R}{2} \right\rfloor$.
   - Если $A[mid] < x$, установите $L = mid + 1$ (ищем в правой половине).
   - Если $A[mid] \geq x$, установите $answer = mid$ и $R = mid - 1$ (ищем в левой половине).

В конце алгоритма, если $answer$ был обновлен, то он будет минимальным индексом элемента, который не меньше $x$.

Пример кода для реализации бинарного поиска для нахождения минимального индекса:

```python
def binary_search_min_index(arr, x):
    """
    Описание:
        Функция для выполнения бинарного поиска, чтобы найти минимальный индекс элемента,
        который не меньше заданного значения x.

    Аргументы:
        arr: Отсортированный список чисел.
        x: Значение, для которого нужно найти минимальный индекс элемента, не меньше x.

    Возвращает:
        Минимальный индекс элемента, который не меньше x, или -1, если такого элемента нет.

    Примеры:
        >>> binary_search_min_index([1, 2, 4, 7, 12, 15, 17], 10)
        4
    """
    L, R = 0, len(arr) - 1  # Устанавливаем границы поиска
    answer = -1  # Изначально ответ не найден

    while L <= R:  # Пока границы не пересеклись
        mid = (L + R) // 2  # Находим середину

        if arr[mid] < x:  # Если элемент в середине меньше x
            L = mid + 1  # Ищем в правой половине
        else:  # Если элемент в середине больше или равен x
            answer = mid  # Обновляем ответ
            R = mid - 1  # Ищем в левой половине

    return answer  # Возвращаем минимальный индекс или -1, если не найден
```

Этот код реализует бинарный поиск, позволяя находить минимальный индекс элемента, который не меньше заданного значения $x$. Он эффективно использует свойства отсортированного массива, сокращая количество проверок.

Физический смысл данной задачи можно проиллюстрировать на примере поиска в магазине. Если у вас есть отсортированный каталог товаров по цене, задача заключается в нахождении первого товара, цена которого не меньше заданной суммы. Это позволяет быстро определять, какие товары доступны для покупки в рамках бюджета.

Таким образом, поиск минимального индекса элемента в отсортированном массиве с использованием бинарного поиска является важным инструментом для работы с отсортированными данными и позволяет эффективно находить элементы, соответствующие заданным условиям.

## Chunk 15
### **Название фрагмента [Поиск элемента в отсортированном массиве с использованием бинарного поиска]:**

**Предыдущий контекст:** В предыдущем фрагменте обсуждалась задача поиска минимального индекса элемента в отсортированном массиве, который не меньше заданного значения. Преподаватель объяснял, как использовать бинарный поиск для решения этой задачи.

## **Поиск элемента в отсортированном массиве с использованием бинарного поиска**

В этом фрагменте акцентируется внимание на задаче поиска конкретного элемента в отсортированном массиве с использованием бинарного поиска. Преподаватель объясняет, как правильно настроить алгоритм для нахождения элемента и какие условия необходимо учитывать для корректного выполнения задачи.

Ключевые аспекты задачи:
1. **Определение задачи:** Необходимо определить, присутствует ли заданный элемент $x$ в массиве. Если элемент найден, нужно вывести "Yes", если нет — "No".
2. **Использование бинарного поиска:** Бинарный поиск позволяет эффективно находить элемент, сокращая количество проверок по сравнению с линейным поиском.
3. **Формулировка успеха:** Успех в данной задаче заключается в нахождении элемента, который равен заданному значению $x$.

Математическая формализация задачи может быть представлена следующим образом. Пусть $A$ — отсортированный массив, а $x$ — искомое значение. Алгоритм бинарного поиска можно описать так:

1. Установите границы: $L = 0$, $R = n - 1$, где $n$ — длина массива.
2. Пока $L \leq R$:
   - Вычислите середину: $mid = \left\lfloor \frac{L + R}{2} \right\rfloor$.
   - Если $A[mid] < x$, установите $L = mid + 1$ (ищем в правой половине).
   - Если $A[mid] > x$, установите $R = mid - 1$ (ищем в левой половине).
   - Если $A[mid] = x$, то элемент найден, и мы можем вернуть "Yes".

Если после завершения поиска элемент не найден, возвращаем "No".

Пример кода для реализации бинарного поиска для нахождения элемента:

```python
def binary_search(arr, x):
    """
    Описание:
        Функция для выполнения бинарного поиска, чтобы определить, присутствует ли элемент x в массиве.

    Аргументы:
        arr: Отсортированный список чисел.
        x: Значение, которое нужно найти в массиве.

    Возвращает:
        'Yes', если элемент найден, и 'No', если элемента нет.

    Примеры:
        >>> binary_search([1, 2, 4, 7, 12, 15, 17], 10)
        'No'
        >>> binary_search([1, 2, 4, 7, 12, 15, 17], 7)
        'Yes'
    """
    L, R = 0, len(arr) - 1  # Устанавливаем границы поиска

    while L <= R:  # Пока границы не пересеклись
        mid = (L + R) // 2  # Находим середину

        if arr[mid] < x:  # Если элемент в середине меньше x
            L = mid + 1  # Ищем в правой половине
        elif arr[mid] > x:  # Если элемент в середине больше x
            R = mid - 1  # Ищем в левой половине
        else:  # Если элемент в середине равен x
            return 'Yes'  # Элемент найден

    return 'No'  # Если элемент не найден
```

Этот код реализует бинарный поиск, позволяя определять, присутствует ли заданный элемент $x$ в отсортированном массиве. Он эффективно использует свойства отсортированного массива, сокращая количество проверок.

Физический смысл данной задачи можно проиллюстрировать на примере поиска в магазине. Если у вас есть отсортированный каталог товаров по цене, задача заключается в нахождении конкретного товара, цена которого равна заданной сумме. Это позволяет быстро определять, доступен ли товар для покупки.

Таким образом, поиск элемента в отсортированном массиве с использованием бинарного поиска является важным инструментом для работы с отсортированными данными и позволяет эффективно находить элементы, соответствующие заданным условиям.

## Chunk 16
### **Название фрагмента [Реализация бинарного поиска для проверки наличия элемента]:**

**Предыдущий контекст:** В предыдущем фрагменте обсуждалась задача поиска минимального индекса элемента в отсортированном массиве, который не меньше заданного значения. Преподаватель объяснял, как использовать бинарный поиск для решения этой задачи.

## **Реализация бинарного поиска для проверки наличия элемента**

В этом фрагменте акцентируется внимание на реализации бинарного поиска для проверки наличия конкретного элемента в отсортированном массиве. Преподаватель объясняет, как правильно настроить алгоритм для нахождения элемента и какие условия необходимо учитывать для корректного выполнения задачи.

Ключевые аспекты реализации:
1. **Определение задачи:** Необходимо определить, присутствует ли заданный элемент $x$ в массиве. Если элемент найден, нужно вывести "Yes", если нет — "No".
2. **Использование бинарного поиска:** Бинарный поиск позволяет эффективно находить элемент, сокращая количество проверок по сравнению с линейным поиском.
3. **Формулировка успеха:** Успех в данной задаче заключается в нахождении элемента, который равен заданному значению $x$.

Математическая формализация задачи может быть представлена следующим образом. Пусть $A$ — отсортированный массив, а $x$ — искомое значение. Алгоритм бинарного поиска можно описать так:

1. Установите границы: $L = 0$, $R = n - 1$, где $n$ — длина массива.
2. Пока $L \leq R$:
   - Вычислите середину: $mid = \left\lfloor \frac{L + R}{2} \right\rfloor$.
   - Если $A[mid] < x$, установите $L = mid + 1$ (ищем в правой половине).
   - Если $A[mid] > x$, установите $R = mid - 1$ (ищем в левой половине).
   - Если $A[mid] = x$, то элемент найден, и мы можем вернуть "Yes".

Если после завершения поиска элемент не найден, возвращаем "No".

Пример кода для реализации бинарного поиска для проверки наличия элемента:

```python
def binary_search(arr, x):
    """
    Описание:
        Функция для выполнения бинарного поиска, чтобы определить, присутствует ли элемент x в массиве.

    Аргументы:
        arr: Отсортированный список чисел.
        x: Значение, которое нужно найти в массиве.

    Возвращает:
        'Yes', если элемент найден, и 'No', если элемента нет.

    Примеры:
        >>> binary_search([1, 2, 4, 7, 12, 15, 17], 10)
        'No'
        >>> binary_search([1, 2, 4, 7, 12, 15, 17], 7)
        'Yes'
    """
    L, R = 0, len(arr) - 1  # Устанавливаем границы поиска

    while L <= R:  # Пока границы не пересеклись
        mid = (L + R) // 2  # Находим середину

        if arr[mid] < x:  # Если элемент в середине меньше x
            L = mid + 1  # Ищем в правой половине
        elif arr[mid] > x:  # Если элемент в середине больше x
            R = mid - 1  # Ищем в левой половине
        else:  # Если элемент в середине равен x
            return 'Yes'  # Элемент найден

    return 'No'  # Если элемент не найден
```

Этот код реализует бинарный поиск, позволяя определять, присутствует ли заданный элемент $x$ в отсортированном массиве. Он эффективно использует свойства отсортированного массива, сокращая количество проверок.

Физический смысл данной задачи можно проиллюстрировать на примере поиска в магазине. Если у вас есть отсортированный каталог товаров по цене, задача заключается в нахождении конкретного товара, цена которого равна заданной сумме. Это позволяет быстро определять, доступен ли товар для покупки.

Таким образом, реализация бинарного поиска для проверки наличия элемента в отсортированном массиве является важным инструментом для работы с отсортированными данными и позволяет эффективно находить элементы, соответствующие заданным условиям.

## Chunk 17
### **Название фрагмента [Поиск количества чисел в заданном диапазоне]:**

**Предыдущий контекст:** В предыдущем фрагменте обсуждалась реализация бинарного поиска для проверки наличия элемента в отсортированном массиве. Преподаватель объяснял, как правильно настроить алгоритм и какие условия необходимо учитывать для корректного выполнения задачи.

## **Поиск количества чисел в заданном диапазоне**

В этом фрагменте акцентируется внимание на задаче поиска количества чисел в массиве, которые находятся в заданном диапазоне от $l$ до $r$. Преподаватель объясняет, как использовать бинарный поиск для эффективного ответа на запросы о количестве элементов в этом диапазоне.

Ключевые аспекты задачи:
1. **Определение задачи:** Необходимо определить, сколько чисел в массиве находятся в диапазоне от $l$ до $r$. Это означает, что мы ищем количество элементов, удовлетворяющих условию $l \leq A[i] \leq r$.
2. **Использование бинарного поиска:** Бинарный поиск позволяет эффективно находить границы диапазона, что сокращает количество проверок по сравнению с линейным поиском.
3. **Формулировка успеха:** Успех в данной задаче заключается в нахождении индексов, которые соответствуют элементам, находящимся в заданном диапазоне.

Математическая формализация задачи может быть представлена следующим образом. Пусть $A$ — отсортированный массив, а $l$ и $r$ — границы диапазона. Алгоритм бинарного поиска можно описать так:

1. Установите границы: $L = 0$, $R = n - 1$, где $n$ — длина массива.
2. Найдите индекс первого элемента, который не меньше $l$:
   - Установите $L$ и $R$ как в предыдущих примерах.
   - Пока $L \leq R$:
     - Вычислите середину: $mid = \left\lfloor \frac{L + R}{2} \right\rfloor$.
     - Если $A[mid] < l$, установите $L = mid + 1$.
     - Если $A[mid] \geq l$, установите $R = mid - 1$.
3. Найдите индекс первого элемента, который больше чем $r$:
   - Установите $L$ и $R$ как в предыдущих примерах.
   - Пока $L \leq R$:
     - Вычислите середину: $mid = \left\lfloor \frac{L + R}{2} \right\rfloor$.
     - Если $A[mid] \leq r$, установите $answer = mid$ и $L = mid + 1$.
     - Если $A[mid] > r$, установите $R = mid - 1$.

Количество элементов в диапазоне можно вычислить как $answer - first\_index + 1$, где $first\_index$ — индекс первого элемента, который не меньше $l$.

Пример кода для реализации поиска количества чисел в заданном диапазоне:

```python
def count_in_range(arr, l, r):
    """
    Описание:
        Функция для подсчета количества чисел в массиве, которые находятся в диапазоне [l, r].

    Аргументы:
        arr: Отсортированный список чисел.
        l: Нижняя граница диапазона.
        r: Верхняя граница диапазона.

    Возвращает:
        Количество элементов в диапазоне [l, r].

    Примеры:
        >>> count_in_range([1, 2, 4, 7, 12, 15, 17], 5, 15)
        4
    """
    def binary_search_first(arr, x):
        L, R = 0, len(arr) - 1
        while L <= R:
            mid = (L + R) // 2
            if arr[mid] < x:
                L = mid + 1
            else:
                R = mid - 1
        return L  # Возвращаем индекс первого элемента >= x

    def binary_search_last(arr, x):
        L, R = 0, len(arr) - 1
        answer = -1
        while L <= R:
            mid = (L + R) // 2
            if arr[mid] <= x:
                answer = mid  # Обновляем ответ
                L = mid + 1
            else:
                R = mid - 1
        return answer  # Возвращаем индекс последнего элемента <= x

    first_index = binary_search_first(arr, l)  # Индекс первого элемента >= l
    last_index = binary_search_last(arr, r)  # Индекс последнего элемента <= r

    if first_index <= last_index and first_index < len(arr) and last_index >= 0:
        return last_index - first_index + 1  # Количество элементов в диапазоне
    return 0  # Если диапазон пуст
```

Этот код реализует поиск количества чисел в заданном диапазоне от $l$ до $r$ в отсортированном массиве. Он эффективно использует бинарный поиск для нахождения границ диапазона.

Физический смысл данной задачи можно проиллюстрировать на примере анализа данных о температуре. Если у вас есть отсортированный массив температур за месяц, задача заключается в нахождении количества дней, когда температура находилась в заданном диапазоне. Это позволяет быстро определять, сколько дней соответствовали заданным условиям.

Таким образом, поиск количества чисел в заданном диапазоне с использованием бинарного поиска является важным инструментом для работы с отсортированными данными и позволяет эффективно находить элементы, соответствующие заданным условиям.

## Chunk 18
### **Название фрагмента [Поиск индексов в заданном диапазоне с использованием бинарного поиска]:**

**Предыдущий контекст:** В предыдущем фрагменте обсуждалась задача поиска минимального индекса элемента в отсортированном массиве, который не меньше заданного значения. Преподаватель объяснял, как использовать бинарный поиск для решения этой задачи.

## **Поиск индексов в заданном диапазоне с использованием бинарного поиска**

В этом фрагменте акцентируется внимание на реализации алгоритма бинарного поиска для нахождения индексов элементов в отсортированном массиве, которые находятся в заданном диапазоне от $L$ до $R$. Преподаватель объясняет, как правильно настроить алгоритм для нахождения минимального и максимального индексов, соответствующих элементам в этом диапазоне.

Ключевые аспекты реализации:
1. **Определение задачи:** Необходимо найти минимальный индекс элемента, который не меньше $L$, и максимальный индекс элемента, который не больше $R$. Это означает, что мы ищем элементы, удовлетворяющие условиям $A[i] \geq L$ и $A[i] \leq R$.
2. **Использование бинарного поиска:** Бинарный поиск позволяет эффективно находить нужные индексы, сокращая количество проверок по сравнению с линейным поиском.
3. **Формулировка успеха:** Успех в данной задаче заключается в нахождении индексов, которые соответствуют элементам, находящимся в заданном диапазоне.

Математическая формализация задачи может быть представлена следующим образом. Пусть $A$ — отсортированный массив, а $L$ и $R$ — границы диапазона. Алгоритм бинарного поиска можно описать так:

1. Установите границы: $L = 0$, $R = n - 1$, где $n$ — длина массива.
2. Найдите индекс первого элемента, который не меньше $L$:
   - Установите $L$ и $R$ как в предыдущих примерах.
   - Пока $L \leq R$:
     - Вычислите середину: $mid = \left\lfloor \frac{L + R}{2} \right\rfloor$.
     - Если $A[mid] < L$, установите $L = mid + 1$.
     - Если $A[mid] \geq L$, установите $R = mid - 1$.
3. Найдите индекс последнего элемента, который не больше $R$:
   - Установите $L$ и $R$ как в предыдущих примерах.
   - Пока $L \leq R$:
     - Вычислите середину: $mid = \left\lfloor \frac{L + R}{2} \right\rfloor$.
     - Если $A[mid] \leq R$, установите $answer = mid$ и $L = mid + 1$.
     - Если $A[mid] > R$, установите $R = mid - 1$.

Количество элементов в диапазоне можно вычислить как $answer - first\_index + 1$, где $first\_index$ — индекс первого элемента, который не меньше $L$.

Пример кода для реализации поиска индексов в заданном диапазоне:

```python
def count_in_range(arr, L, R):
    """
    Описание:
        Функция для подсчета количества чисел в массиве, которые находятся в диапазоне [L, R].

    Аргументы:
        arr: Отсортированный список чисел.
        L: Нижняя граница диапазона.
        R: Верхняя граница диапазона.

    Возвращает:
        Количество элементов в диапазоне [L, R].

    Примеры:
        >>> count_in_range([1, 2, 4, 7, 12, 15, 17], 5, 15)
        4
    """
    def binary_search_first(arr, x):
        L, R = 0, len(arr) - 1
        while L <= R:
            mid = (L + R) // 2
            if arr[mid] < x:
                L = mid + 1
            else:
                R = mid - 1
        return L  # Возвращаем индекс первого элемента >= x

    def binary_search_last(arr, x):
        L, R = 0, len(arr) - 1
        answer = -1
        while L <= R:
            mid = (L + R) // 2
            if arr[mid] <= x:
                answer = mid  # Обновляем ответ
                L = mid + 1
            else:
                R = mid - 1
        return answer  # Возвращаем индекс последнего элемента <= x

    first_index = binary_search_first(arr, L)  # Индекс первого элемента >= L
    last_index = binary_search_last(arr, R)  # Индекс последнего элемента <= R

    if first_index <= last_index and first_index < len(arr) and last_index >= 0:
        return last_index - first_index + 1  # Количество элементов в диапазоне
    return 0  # Если диапазон пуст
```

Этот код реализует поиск количества чисел в заданном диапазоне от $L$ до $R$ в отсортированном массиве. Он эффективно использует бинарный поиск для нахождения границ диапазона.

Физический смысл данной задачи можно проиллюстрировать на примере анализа данных о температуре. Если у вас есть отсортированный массив температур за месяц, задача заключается в нахождении количества дней, когда температура находилась в заданном диапазоне. Это позволяет быстро определять, сколько дней соответствовали заданным условиям.

Таким образом, поиск индексов в заданном диапазоне с использованием бинарного поиска является важным инструментом для работы с отсортированными данными и позволяет эффективно находить элементы, соответствующие заданным условиям.

## Chunk 19
### **Название фрагмента [Поиск количества элементов в заданном диапазоне]:**

**Предыдущий контекст:** В предыдущем фрагменте обсуждалась реализация бинарного поиска для проверки наличия элемента в отсортированном массиве. Преподаватель объяснял, как правильно настроить алгоритм и какие условия необходимо учитывать для корректного выполнения задачи.

## **Поиск количества элементов в заданном диапазоне**

В этом фрагменте акцентируется внимание на реализации алгоритма бинарного поиска для нахождения количества элементов в отсортированном массиве, которые находятся в заданном диапазоне от $L$ до $R$. Преподаватель объясняет, как правильно настроить алгоритм для нахождения минимального и максимального индексов, соответствующих элементам в этом диапазоне.

Ключевые аспекты реализации:
1. **Определение задачи:** Необходимо определить, сколько чисел в массиве находятся в диапазоне от $L$ до $R$. Это означает, что мы ищем количество элементов, удовлетворяющих условию $L \leq A[i] \leq R$.
2. **Использование бинарного поиска:** Бинарный поиск позволяет эффективно находить границы диапазона, что сокращает количество проверок по сравнению с линейным поиском.
3. **Формулировка успеха:** Успех в данной задаче заключается в нахождении индексов, которые соответствуют элементам, находящимся в заданном диапазоне.

Математическая формализация задачи может быть представлена следующим образом. Пусть $A$ — отсортированный массив, а $L$ и $R$ — границы диапазона. Алгоритм бинарного поиска можно описать так:

1. Установите границы: $L = 0$, $R = n - 1$, где $n$ — длина массива.
2. Найдите индекс первого элемента, который не меньше $L$:
   - Установите $L$ и $R$ как в предыдущих примерах.
   - Пока $L \leq R$:
     - Вычислите середину: $mid = \left\lfloor \frac{L + R}{2} \right\rfloor$.
     - Если $A[mid] < L$, установите $L = mid + 1$.
     - Если $A[mid] \geq L$, установите $R = mid - 1$.
3. Найдите индекс последнего элемента, который не больше $R$:
   - Установите $L$ и $R$ как в предыдущих примерах.
   - Пока $L \leq R$:
     - Вычислите середину: $mid = \left\lfloor \frac{L + R}{2} \right\rfloor$.
     - Если $A[mid] \leq R$, установите $answer = mid$ и $L = mid + 1$.
     - Если $A[mid] > R$, установите $R = mid - 1$.

Количество элементов в диапазоне можно вычислить как $answer - first\_index + 1$, где $first\_index$ — индекс первого элемента, который не меньше $L$.

Пример кода для реализации поиска количества элементов в заданном диапазоне:

```python
def count_in_range(arr, L, R):
    """
    Описание:
        Функция для подсчета количества чисел в массиве, которые находятся в диапазоне [L, R].

    Аргументы:
        arr: Отсортированный список чисел.
        L: Нижняя граница диапазона.
        R: Верхняя граница диапазона.

    Возвращает:
        Количество элементов в диапазоне [L, R].

    Примеры:
        >>> count_in_range([1, 2, 4, 7, 12, 15, 17], 5, 15)
        4
    """
    def binary_search_first(arr, x):
        L, R = 0, len(arr) - 1
        while L <= R:
            mid = (L + R) // 2
            if arr[mid] < x:
                L = mid + 1
            else:
                R = mid - 1
        return L  # Возвращаем индекс первого элемента >= x

    def binary_search_last(arr, x):
        L, R = 0, len(arr) - 1
        answer = -1
        while L <= R:
            mid = (L + R) // 2
            if arr[mid] <= x:
                answer = mid  # Обновляем ответ
                L = mid + 1
            else:
                R = mid - 1
        return answer  # Возвращаем индекс последнего элемента <= x

    first_index = binary_search_first(arr, L)  # Индекс первого элемента >= L
    last_index = binary_search_last(arr, R)  # Индекс последнего элемента <= R

    if first_index <= last_index and first_index < len(arr) and last_index >= 0:
        return last_index - first_index + 1  # Количество элементов в диапазоне
    return 0  # Если диапазон пуст
```

Этот код реализует поиск количества чисел в заданном диапазоне от $L$ до $R$ в отсортированном массиве. Он эффективно использует бинарный поиск для нахождения границ диапазона.

Физический смысл данной задачи можно проиллюстрировать на примере анализа данных о температуре. Если у вас есть отсортированный массив температур за месяц, задача заключается в нахождении количества дней, когда температура находилась в заданном диапазоне. Это позволяет быстро определять, сколько дней соответствовали заданным условиям.

Таким образом, поиск количества чисел в заданном диапазоне с использованием бинарного поиска является важным инструментом для работы с отсортированными данными и позволяет эффективно находить элементы, соответствующие заданным условиям.

## Chunk 20
### **Название фрагмента [Оптимизация поиска с использованием сортировки и бинарного поиска]:**

**Предыдущий контекст:** В предыдущем фрагменте обсуждалась задача поиска минимального индекса элемента в отсортированном массиве, который не меньше заданного значения. Преподаватель объяснял, как использовать бинарный поиск для решения этой задачи.

## **Оптимизация поиска с использованием сортировки и бинарного поиска**

В этом фрагменте акцентируется внимание на оптимизации поиска элементов в массиве с использованием предварительной сортировки и бинарного поиска. Преподаватель объясняет, как правильно организовать алгоритм для эффективного выполнения запросов на наличие элементов в заданном диапазоне.

Ключевые аспекты оптимизации:
1. **Сортировка массива:** Для эффективного поиска необходимо, чтобы массив был отсортирован. Это позволяет использовать бинарный поиск, который значительно сокращает время поиска по сравнению с линейным методом.
2. **Формулировка успеха:** Успех в данной задаче заключается в нахождении индекса, который соответствует элементам, находящимся в заданном диапазоне. Если элемент найден, необходимо вернуть его индекс, если нет — вернуть значение, указывающее на отсутствие элемента.
3. **Сложность алгоритма:** Сложность алгоритма включает в себя время на сортировку массива ($O(n \log n)$) и время на обработку запросов ($O(m \log n)$), где $m$ — количество запросов.

Математическая формализация задачи может быть представлена следующим образом. Пусть $A$ — массив, который нужно отсортировать, а $Q$ — массив запросов. Алгоритм можно описать так:

1. Отсортируйте массив $A$.
2. Для каждого запроса $q$ в массиве $Q$:
   - Используйте бинарный поиск для нахождения индекса элемента, который соответствует условию.

Пример кода для реализации поиска с использованием сортировки и бинарного поиска:

```python
def binary_search(arr, x):
    """
    Описание:
        Функция для выполнения бинарного поиска, чтобы определить, присутствует ли элемент x в массиве.

    Аргументы:
        arr: Отсортированный список чисел.
        x: Значение, которое нужно найти в массиве.

    Возвращает:
        'Yes', если элемент найден, и 'No', если элемента нет.

    Примеры:
        >>> binary_search([1, 2, 4, 7, 12, 15, 17], 10)
        'No'
        >>> binary_search([1, 2, 4, 7, 12, 15, 17], 7)
        'Yes'
    """
    L, R = 0, len(arr) - 1  # Устанавливаем границы поиска

    while L <= R:  # Пока границы не пересеклись
        mid = (L + R) // 2  # Находим середину

        if arr[mid] < x:  # Если элемент в середине меньше x
            L = mid + 1  # Ищем в правой половине
        elif arr[mid] > x:  # Если элемент в середине больше x
            R = mid - 1  # Ищем в левой половине
        else:  # Если элемент в середине равен x
            return 'Yes'  # Элемент найден

    return 'No'  # Если элемент не найден

def process_queries(arr, queries):
    """
    Описание:
        Функция для обработки запросов на наличие элементов в отсортированном массиве.

    Аргументы:
        arr: Отсортированный список чисел.
        queries: Список запросов.

    Возвращает:
        Список ответов на запросы.

    Примеры:
        >>> process_queries([1, 2, 4, 7, 12, 15, 17], [10, 7])
        ['No', 'Yes']
    """
    results = []
    for q in queries:
        results.append(binary_search(arr, q))  # Обрабатываем каждый запрос
    return results

# Пример использования
n = 7
arr = [1, 2, 4, 7, 12, 15, 17]  # Отсортированный массив
queries = [10, 7]  # Запросы
print(process_queries(arr, queries))  # Вывод: ['No', 'Yes']
```

Этот код реализует бинарный поиск для проверки наличия элементов в отсортированном массиве и обрабатывает запросы, возвращая соответствующие ответы.

Физический смысл данной задачи можно проиллюстрировать на примере поиска в магазине. Если у вас есть отсортированный каталог товаров по цене, задача заключается в нахождении конкретного товара, цена которого равна заданной сумме. Это позволяет быстро определять, доступен ли товар для покупки.

Таким образом, оптимизация поиска с использованием сортировки и бинарного поиска является важным инструментом для работы с отсортированными данными и позволяет эффективно находить элементы, соответствующие заданным условиям.

## Final Summary
### **Сводка текста:**

В данной статье рассматриваются ключевые аспекты алгоритма бинарного поиска, который используется для нахождения элементов в отсортированных массивах. Обсуждаются задачи, связанные с поиском минимального индекса элемента, который не меньше заданного значения, и максимального индекса элемента, который не превышает заданное значение. Преподаватель акцентирует внимание на важности правильной формулировки условий успеха, а также на необходимости предварительной сортировки массива для эффективного выполнения бинарного поиска.

Статья включает математическую формализацию алгоритма, а также примеры кода, демонстрирующие реализацию бинарного поиска. Приводятся примеры использования алгоритма в практических задачах, таких как поиск товаров в магазине по цене. В заключение подчеркивается, что бинарный поиск является мощным инструментом для работы с отсортированными данными, позволяющим эффективно находить элементы, соответствующие заданным условиям.

## Chunk 2
### **Название фрагмента: Введение в мат ожидание и дисперсию**

**Предыдущий контекст:** В предыдущем фрагменте обсуждались основные темы семинара, включая задачи по машинному обучению и метрики регрессии. Теперь мы переходим к основам теории вероятности, а именно к математическому ожиданию и дисперсии.

## **Математическое ожидание и дисперсия**

Математическое ожидание (мат ожидание) и дисперсия — это ключевые концепции в теории вероятностей, которые помогают описать случайные величины. 

### Математическое ожидание

Математическое ожидание случайной величины $X$ обозначается как $E(X)$ и представляет собой среднее значение, которое мы ожидаем получить, если будем многократно проводить эксперимент. Оно вычисляется по формуле:

$$
E(X) = \int_{-\infty}^{+\infty} x \cdot f(x) \, dx
$$

где:
- $f(x)$ — функция плотности вероятности случайной величины $X$.

### Дисперсия

Дисперсия случайной величины $X$, обозначаемая как $D(X)$, измеряет степень разброса значений случайной величины относительно её математического ожидания. Она вычисляется по формуле:

$$
D(X) = E((X - E(X))^2) = E(X^2) - (E(X))^2
$$

где:
- $E(X^2)$ — математическое ожидание квадрата случайной величины.

### Интуитивное понимание

Представьте себе нормальное распределение, которое выглядит как колокол. Если дисперсия велика, колокол будет широкий и плоский, что означает, что значения случайной величины сильно разбросаны. Если дисперсия мала, колокол будет узким и высоким, что указывает на то, что значения сосредоточены вокруг математического ожидания.

### Пример кода

Давайте рассмотрим пример кода на Python, который вычисляет математическое ожидание и дисперсию для заданного набора данных:

```python
import numpy as np

# Функция для вычисления математического ожидания и дисперсии
def calculate_expectation_and_variance(data):
    """
    Описание:
        Вычисляет математическое ожидание и дисперсию для заданного набора данных.

    Аргументы:
        data: Список чисел, представляющих выборку.

    Возвращает:
        tuple: Математическое ожидание и дисперсия.
    """
    # Преобразуем данные в массив NumPy для удобства
    data_array = np.array(data)
    
    # Вычисляем математическое ожидание
    expectation = np.mean(data_array)
    
    # Вычисляем дисперсию
    variance = np.var(data_array)
    
    return expectation, variance

# Пример использования функции
data = [1, 2, 3, 4, 5]
expectation, variance = calculate_expectation_and_variance(data)
print(f"Математическое ожидание: {expectation}, Дисперсия: {variance}")
```

В этом коде:
- Мы используем библиотеку NumPy для удобного вычисления математического ожидания и дисперсии.
- Функция `calculate_expectation_and_variance` принимает список чисел и возвращает математическое ожидание и дисперсию.
- Мы выводим результаты на экран.

### Физический и геометрический смысл

В физике дисперсия может быть использована для описания неопределенности в измерениях. Например, если мы измеряем длину объекта несколько раз, дисперсия значений измерений покажет, насколько сильно они разбросаны. Если дисперсия мала, это означает, что наши измерения точны и близки к истинному значению.

## Chunk 3
### **Название фрагмента: Центральная предельная теорема и её применение**

**Предыдущий контекст:** В предыдущем фрагменте мы обсудили математическое ожидание и дисперсию, а также их интуитивное понимание через нормальное распределение. Теперь мы перейдем к центральной предельной теореме, которая является основополагающей в теории вероятностей и статистики.

## **Центральная предельная теорема (ЦПТ)**

Центральная предельная теорема (ЦПТ) утверждает, что при достаточно большом количестве независимых и одинаково распределенных случайных величин, их сумма (или среднее) будет стремиться к нормальному распределению, независимо от формы исходного распределения. Это свойство делает ЦПТ важным инструментом в статистике и вероятностной теории.

### Формулировка ЦПТ

Если $X_1, X_2, \ldots, X_n$ — независимые случайные величины с одинаковым распределением, имеющими конечное математическое ожидание $E(X)$ и конечную дисперсию $D(X)$, то при увеличении $n$ распределение среднего:

$$
\bar{X} = \frac{1}{n} \sum_{i=1}^{n} X_i
$$

будет стремиться к нормальному распределению с математическим ожиданием $E(X)$ и дисперсией $\frac{D(X)}{n}$.

### Математическая формализация

ЦПТ можно записать в следующем виде:

$$
Z_n = \frac{\bar{X} - E(X)}{\sigma / \sqrt{n}} \xrightarrow{d} N(0, 1)
$$

где:
- $Z_n$ — стандартизированное значение среднего;
- $\sigma$ — стандартное отклонение случайной величины $X$;
- $N(0, 1)$ — стандартное нормальное распределение.

### Пример кода

Рассмотрим пример кода на Python, который демонстрирует применение ЦПТ на случайных величинах:

```python
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Функция для генерации выборки и построения гистограммы
def central_limit_theorem(sample_size, num_samples):
    """
    Описание:
        Генерирует выборки случайных величин и показывает, как распределение их среднего стремится к нормальному.

    Аргументы:
        sample_size: Размер каждой выборки.
        num_samples: Количество выборок.

    Возвращает:
        None
    """
    # Генерируем случайные величины из равномерного распределения
    samples = [np.random.uniform(0, 1, sample_size) for _ in range(num_samples)]
    
    # Вычисляем средние значения выборок
    means = [np.mean(sample) for sample in samples]
    
    # Строим гистограмму средних значений
    sns.histplot(means, kde=True)
    plt.title('Распределение средних значений выборок')
    plt.xlabel('Средние значения')
    plt.ylabel('Частота')
    plt.show()

# Пример использования функции
central_limit_theorem(sample_size=30, num_samples=1000)
```

В этом коде:
- Мы генерируем выборки случайных величин из равномерного распределения.
- Для каждой выборки вычисляем среднее значение.
- Строим гистограмму средних значений, чтобы визуализировать, как они стремятся к нормальному распределению.

### Физический и геометрический смысл

Центральная предельная теорема имеет важное значение в физике, особенно в экспериментах, где мы измеряем одно и то же явление многократно. Например, если мы измеряем длину стержня, то каждое измерение будет случайной величиной с некоторой дисперсией. При увеличении числа измерений среднее значение будет стремиться к истинной длине стержня, и распределение этих средних значений будет приближаться к нормальному, что позволяет нам делать статистические выводы о длине стержня.

## Chunk 4
### **Название фрагмента: Вычисление математического ожидания и дисперсии для нормального распределения**

**Предыдущий контекст:** В предыдущем фрагменте мы обсудили центральную предельную теорему и её значение в статистике. Теперь мы перейдем к вычислению математического ожидания и дисперсии для нормального распределения, используя функцию плотности.

## **Вычисление математического ожидания и дисперсии**

В этом разделе мы рассмотрим, как вычислить математическое ожидание и дисперсию для случайной величины, распределенной нормально. Мы будем использовать функцию плотности нормального распределения и интегралы для нахождения этих значений.

### Математическое ожидание

Для нормального распределения с математическим ожиданием $0$ и дисперсией $\sigma^2$, функция плотности вероятности $f(x)$ имеет вид:

$$
f(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{x^2}{2\sigma^2}}
$$

Чтобы найти математическое ожидание от модуля случайной величины $E(|X|)$, мы можем использовать интеграл:

$$
E(|X|) = 2 \int_{0}^{+\infty} x f(x) \, dx
$$

Подставляя функцию плотности, получаем:

$$
E(|X|) = 2 \int_{0}^{+\infty} x \cdot \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{x^2}{2\sigma^2}} \, dx
$$

### Замена переменных

Для упрощения интеграла мы можем сделать замену переменных. Пусть $u = \frac{x}{\sigma}$, тогда $du = \frac{1}{\sigma} dx$, и интеграл преобразуется:

$$
E(|X|) = 2 \cdot \sigma \cdot \frac{1}{\sqrt{2\pi}} \int_{0}^{+\infty} u e^{-\frac{u^2}{2}} \, du
$$

### Интеграл от $u e^{-\frac{u^2}{2}}$

Этот интеграл можно решить, используя известное свойство производной экспоненты. Мы знаем, что:

$$
\frac{d}{du} \left( -e^{-\frac{u^2}{2}} \right) = u e^{-\frac{u^2}{2}}
$$

Таким образом, интеграл можно выразить через пределы:

$$
\int_{0}^{+\infty} u e^{-\frac{u^2}{2}} \, du = \left[ -e^{-\frac{u^2}{2}} \right]_{0}^{+\infty} = 0 - (-1) = 1
$$

### Итоговое значение математического ожидания

Подставляя это значение обратно, получаем:

$$
E(|X|) = \sigma \cdot \sqrt{\frac{2}{\pi}}
$$

### Дисперсия

Теперь перейдем к вычислению дисперсии. Дисперсия случайной величины $X$ определяется как:

$$
D(X) = E(X^2) - (E(X))^2
$$

Для нормального распределения с математическим ожиданием $0$, дисперсия равна:

$$
D(X) = E(X^2)
$$

Для нормального распределения $E(X^2)$ можно найти следующим образом:

$$
E(X^2) = \int_{-\infty}^{+\infty} x^2 f(x) \, dx
$$

Подставляя функцию плотности, получаем:

$$
E(X^2) = \int_{-\infty}^{+\infty} x^2 \cdot \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{x^2}{2\sigma^2}} \, dx
$$

### Пример кода

Давайте рассмотрим пример кода на Python, который вычисляет математическое ожидание и дисперсию для нормального распределения:

```python
import numpy as np
import scipy.stats as stats

# Функция для вычисления математического ожидания и дисперсии
def normal_distribution_params(mu, sigma):
    """
    Описание:
        Вычисляет математическое ожидание и дисперсию для нормального распределения.

    Аргументы:
        mu: Математическое ожидание.
        sigma: Стандартное отклонение.

    Возвращает:
        tuple: Математическое ожидание и дисперсия.
    """
    # Математическое ожидание
    expectation = mu
    
    # Дисперсия
    variance = sigma ** 2
    
    return expectation, variance

# Пример использования функции
mu = 0  # Математическое ожидание
sigma = 1  # Стандартное отклонение
expectation, variance = normal_distribution_params(mu, sigma)
print(f"Математическое ожидание: {expectation}, Дисперсия: {variance}")
```

В этом коде:
- Мы определяем функцию `normal_distribution_params`, которая принимает математическое ожидание и стандартное отклонение.
- Функция возвращает математическое ожидание и дисперсию для нормального распределения.
- Мы выводим результаты на экран.

### Физический и геометрический смысл

В физике дисперсия может быть использована для описания неопределенности в измерениях. Например, если мы измеряем температуру в комнате несколько раз, дисперсия значений измерений покажет, насколько сильно они разбросаны. Если дисперсия мала, это означает, что наши измерения точны и близки к истинному значению.

## Chunk 5
### **Название фрагмента: Вычисление математического ожидания и дисперсии модуля случайной величины**

**Предыдущий контекст:** В предыдущем фрагменте мы обсудили, как вычислить математическое ожидание и дисперсию для нормального распределения, используя функцию плотности. Теперь мы перейдем к вычислению математического ожидания и дисперсии модуля случайной величины.

## **Математическое ожидание и дисперсия модуля случайной величины**

В этом разделе мы рассмотрим, как вычислить математическое ожидание и дисперсию модуля случайной величины $X$, распределенной нормально. Мы будем использовать интегралы и свойства функции плотности для нахождения этих значений.

### Математическое ожидание модуля случайной величины

Чтобы найти математическое ожидание модуля случайной величины $E(|X|)$, мы можем использовать интеграл:

$$
E(|X|) = \int_{-\infty}^{+\infty} |x| f(x) \, dx
$$

где $f(x)$ — функция плотности нормального распределения:

$$
f(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{x^2}{2\sigma^2}}
$$

Поскольку функция плотности симметрична относительно нуля, мы можем упростить интеграл:

$$
E(|X|) = 2 \int_{0}^{+\infty} x f(x) \, dx
$$

Подставляя функцию плотности, получаем:

$$
E(|X|) = 2 \int_{0}^{+\infty} x \cdot \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{x^2}{2\sigma^2}} \, dx
$$

### Математическое ожидание квадрата случайной величины

Теперь мы можем найти математическое ожидание квадрата случайной величины $E(X^2)$:

$$
E(X^2) = \int_{-\infty}^{+\infty} x^2 f(x) \, dx
$$

Подставляя функцию плотности, получаем:

$$
E(X^2) = \int_{-\infty}^{+\infty} x^2 \cdot \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{x^2}{2\sigma^2}} \, dx
$$

### Дисперсия модуля случайной величины

Дисперсия модуля случайной величины $D(|X|)$ определяется как:

$$
D(|X|) = E(|X|^2) - (E(|X|))^2
$$

Где $E(|X|^2)$ можно найти как:

$$
E(|X|^2) = E(X^2)
$$

Таким образом, дисперсия модуля случайной величины будет равна:

$$
D(|X|) = E(X^2) - (E(|X|))^2
$$

### Пример кода

Давайте рассмотрим пример кода на Python, который вычисляет математическое ожидание и дисперсию модуля случайной величины:

```python
import numpy as np
import scipy.stats as stats

# Функция для вычисления математического ожидания и дисперсии модуля
def expectation_and_variance_of_absolute(mu, sigma):
    """
    Описание:
        Вычисляет математическое ожидание и дисперсию модуля случайной величины.

    Аргументы:
        mu: Математическое ожидание.
        sigma: Стандартное отклонение.

    Возвращает:
        tuple: Математическое ожидание и дисперсия модуля.
    """
    # Математическое ожидание модуля
    expectation_abs = sigma * np.sqrt(2 / np.pi)
    
    # Математическое ожидание квадрата
    expectation_square = sigma ** 2
    
    # Дисперсия модуля
    variance_abs = expectation_square - (expectation_abs ** 2)
    
    return expectation_abs, variance_abs

# Пример использования функции
mu = 0  # Математическое ожидание
sigma = 1  # Стандартное отклонение
expectation_abs, variance_abs = expectation_and_variance_of_absolute(mu, sigma)
print(f"Математическое ожидание модуля: {expectation_abs}, Дисперсия модуля: {variance_abs}")
```

В этом коде:
- Мы определяем функцию `expectation_and_variance_of_absolute`, которая принимает математическое ожидание и стандартное отклонение.
- Функция возвращает математическое ожидание и дисперсию модуля случайной величины.
- Мы выводим результаты на экран.

### Физический и геометрический смысл

В физике математическое ожидание модуля случайной величины может быть использовано для описания средних значений измерений, например, в экспериментах с колебаниями. Если мы измеряем амплитуду колебаний, математическое ожидание модуля даст нам представление о средней величине колебаний, а дисперсия покажет, насколько сильно эти колебания варьируются.

## Chunk 6
### **Название фрагмента: Метрики MAPE и S-MAPE в оценке качества предсказаний**

**Предыдущий контекст:** В предыдущем фрагменте мы обсудили математическое ожидание и дисперсию модуля случайной величины, а также их применение в статистике. Теперь мы перейдем к метрикам MAPE (Mean Absolute Percentage Error) и S-MAPE (Symmetric Mean Absolute Percentage Error), которые используются для оценки качества предсказаний моделей.

## **Метрики MAPE и S-MAPE**

MAPE и S-MAPE — это относительные метрики, которые помогают оценить точность предсказаний модели, сравнивая предсказанные значения с истинными значениями. Эти метрики особенно полезны в задачах регрессии, где важно понимать, насколько хорошо модель предсказывает результаты.

### Определение MAPE

MAPE определяется как среднее абсолютное процентное отклонение предсказанных значений от истинных значений:

$$
\text{MAPE} = \frac{1}{n} \sum_{i=1}^{n} \left| \frac{y_i - \hat{y}_i}{y_i} \right| \times 100\%
$$

где:
- $y_i$ — истинное значение,
- $\hat{y}_i$ — предсказанное значение,
- $n$ — количество наблюдений.

### Определение S-MAPE

S-MAPE, в свою очередь, является симметричной версией MAPE и учитывает как истинные, так и предсказанные значения в знаменателе:

$$
\text{S-MAPE} = \frac{1}{n} \sum_{i=1}^{n} \frac{|y_i - \hat{y}_i|}{|y_i| + |\hat{y}_i|} \times 100\%
$$

### Применение метрик

1. **Ситуация 1:** Если все истинные значения $y_i$ значительно больше нуля (например, порядка $10^5$), то MAPE и S-MAPE будут вести себя аналогично. Если модель предсказывает значения правильно, то разность между истинными и предсказанными значениями будет мала, и обе метрики будут близки к нулю, что указывает на хорошую работу модели.

2. **Ситуация 2:** Если некоторые истинные значения равны нулю или близки к нулю, то MAPE становится неопределенной, так как деление на ноль невозможно. В этом случае S-MAPE может дать более стабильные результаты, так как учитывает как истинные, так и предсказанные значения в знаменателе.

### Пример кода

Давайте рассмотрим пример кода на Python, который вычисляет MAPE и S-MAPE для заданных истинных и предсказанных значений:

```python
import numpy as np

def calculate_mape(y_true, y_pred):
    """
    Описание:
        Вычисляет MAPE для заданных истинных и предсказанных значений.

    Аргументы:
        y_true: Массив истинных значений.
        y_pred: Массив предсказанных значений.

    Возвращает:
        MAPE: Среднее абсолютное процентное отклонение.
    """
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100

def calculate_smap(y_true, y_pred):
    """
    Описание:
        Вычисляет S-MAPE для заданных истинных и предсказанных значений.

    Аргументы:
        y_true: Массив истинных значений.
        y_pred: Массив предсказанных значений.

    Возвращает:
        S-MAPE: Симметричное среднее абсолютное процентное отклонение.
    """
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    return np.mean(np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred))) * 100

# Пример использования функций
y_true = [100, 200, 300]
y_pred = [110, 190, 290]

mape = calculate_mape(y_true, y_pred)
smap = calculate_smap(y_true, y_pred)

print(f"MAPE: {mape:.2f}%, S-MAPE: {smap:.2f}%")
```

В этом коде:
- Мы определяем функции `calculate_mape` и `calculate_smap`, которые принимают массивы истинных и предсказанных значений.
- Каждая функция возвращает соответствующую метрику.
- Мы выводим результаты на экран.

### Физический и геометрический смысл

В физике метрики MAPE и S-MAPE могут быть использованы для оценки точности измерений. Например, если мы измеряем скорость объекта в разных точках времени, MAPE поможет нам понять, насколько близки наши предсказания к истинным значениям скорости. Если значения близки к нулю, это указывает на высокую точность модели, в то время как большие значения указывают на значительные ошибки в предсказаниях.

## Chunk 7
### **Название фрагмента: Проблемы с MAPE и преимущества S-MAPE**

**Предыдущий контекст:** В предыдущем фрагменте мы обсудили метрики MAPE и S-MAPE, их определения и применение в оценке качества предсказаний. Теперь мы сосредоточимся на проблемах, связанных с использованием MAPE, особенно когда истинные значения близки к нулю, и преимуществах S-MAPE в таких ситуациях.

## **Проблемы с MAPE и преимущества S-MAPE**

MAPE (Mean Absolute Percentage Error) может давать некорректные результаты, когда истинные значения $y_t$ близки к нулю. В таких случаях ошибка может "взрываться", что приводит к очень высоким значениям метрики.

### Проблема с MAPE

Когда истинное значение $y_t$ близко к нулю, например, $0.1$, а предсказанное значение $\hat{y}$ значительно больше, например, $90$, то MAPE будет вычисляться следующим образом:

$$
\text{MAPE} = \left| \frac{y_t - \hat{y}}{y_t} \right| \times 100\%
$$

Подставляя значения, получаем:

$$
\text{MAPE} = \left| \frac{0.1 - 90}{0.1} \right| \times 100\% = \left| \frac{-89.9}{0.1} \right| \times 100\% = 89900\%
$$

Таким образом, MAPE "улетает в космос", что делает её неадекватной метрикой в таких ситуациях. Это происходит потому, что деление на очень малое значение в знаменателе приводит к огромным значениям ошибки.

### Преимущества S-MAPE

S-MAPE (Symmetric Mean Absolute Percentage Error) решает эту проблему, так как учитывает как истинные, так и предсказанные значения в знаменателе:

$$
\text{S-MAPE} = \frac{1}{n} \sum_{i=1}^{n} \frac{|y_t - \hat{y}|}{|y_t| + |\hat{y}|} \times 100\%
$$

Таким образом, S-MAPE не будет "взрываться", даже если истинные значения близки к нулю, так как в знаменателе будет сумма абсолютных значений истинного и предсказанного значений, что делает её более устойчивой к таким ситуациям.

### Пример кода

Давайте рассмотрим пример кода на Python, который демонстрирует, как MAPE и S-MAPE ведут себя при различных истинных и предсказанных значениях:

```python
import numpy as np

def calculate_mape(y_true, y_pred):
    """
    Описание:
        Вычисляет MAPE для заданных истинных и предсказанных значений.

    Аргументы:
        y_true: Массив истинных значений.
        y_pred: Массив предсказанных значений.

    Возвращает:
        MAPE: Среднее абсолютное процентное отклонение.
    """
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100

def calculate_smap(y_true, y_pred):
    """
    Описание:
        Вычисляет S-MAPE для заданных истинных и предсказанных значений.

    Аргументы:
        y_true: Массив истинных значений.
        y_pred: Массив предсказанных значений.

    Возвращает:
        S-MAPE: Симметричное среднее абсолютное процентное отклонение.
    """
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    return np.mean(np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred))) * 100

# Пример использования функций
y_true = [0.1, 0.1, 0.1]
y_pred = [90, 90, 90]

mape = calculate_mape(y_true, y_pred)
smap = calculate_smap(y_true, y_pred)

print(f"MAPE: {mape:.2f}%, S-MAPE: {smap:.2f}%")
```

В этом коде:
- Мы определяем функции `calculate_mape` и `calculate_smap`, которые принимают массивы истинных и предсказанных значений.
- Каждая функция возвращает соответствующую метрику.
- Мы выводим результаты на экран.

### Физический и геометрический смысл

В физике, когда мы измеряем какие-либо величины, такие как скорость или давление, важно, чтобы метрики ошибки были устойчивыми к малым значениям. Например, если мы измеряем давление в системе, и истинное значение близко к нулю, использование MAPE может привести к неверным выводам о точности измерений. S-MAPE, учитывая как истинные, так и предсказанные значения, позволяет более адекватно оценивать качество предсказаний в таких ситуациях.

## Chunk 8
### **Название фрагмента: Симметричность S-MAPE и её ограничения**

**Предыдущий контекст:** В предыдущем фрагменте мы обсудили проблемы, связанные с использованием MAPE, особенно когда истинные значения близки к нулю, и преимущества S-MAPE в таких ситуациях. Теперь мы сосредоточимся на симметричности S-MAPE и её ограничениях.

## **Симметричность S-MAPE и её ограничения**

S-MAPE (Symmetric Mean Absolute Percentage Error) считается симметричной метрикой, так как она учитывает как истинные, так и предсказанные значения в знаменателе, что позволяет избежать проблем, связанных с делением на ноль, и делает её более устойчивой к малым значениям.

### Почему S-MAPE считается симметричной?

1. **Ограниченность значений:** S-MAPE ограничена диапазоном от $0$ до $200\%$. Это означает, что независимо от того, насколько сильно предсказание отклоняется от истинного значения, метрика не может превышать $200\%$. Это делает её более понятной для интерпретации, так как пользователи могут сразу увидеть, насколько хорошо или плохо работает модель.

2. **Симметрия относительно ошибок:** S-MAPE симметрична, потому что она учитывает как положительные, так и отрицательные отклонения. Это означает, что если предсказание завышено или занижено, метрика будет одинаково реагировать на оба случая. Например, если истинное значение $y_t$ близко к нулю, а предсказанное значение $\hat{y}$ значительно больше, то ошибка будет большой, но если $y_t$ также близко к нулю и $\hat{y}$ тоже, то ошибка будет мала.

### Примеры крайних случаев

1. **Ситуация 1:** Если истинное значение $y_t$ равно нулю, а предсказанное значение $\hat{y}$ значительно больше нуля, то S-MAPE будет вычисляться следующим образом:

$$
\text{S-MAPE} = \frac{|y_t - \hat{y}|}{|y_t| + |\hat{y}|} \times 100\%
$$

В этом случае, если $y_t = 0.1$ и $\hat{y} = 90$, то:

$$
\text{S-MAPE} = \frac{|0.1 - 90|}{|0.1| + |90|} \times 100\% \approx 99.89\%
$$

2. **Ситуация 2:** Если оба значения близки к нулю, например, $y_t = 0.1$ и $\hat{y} = 0.1$, то S-MAPE будет:

$$
\text{S-MAPE} = \frac{|0.1 - 0.1|}{|0.1| + |0.1|} \times 100\% = 0\%
$$

### Пример кода

Давайте рассмотрим пример кода на Python, который демонстрирует, как S-MAPE ведет себя при различных истинных и предсказанных значениях:

```python
import numpy as np

def calculate_smap(y_true, y_pred):
    """
    Описание:
        Вычисляет S-MAPE для заданных истинных и предсказанных значений.

    Аргументы:
        y_true: Массив истинных значений.
        y_pred: Массив предсказанных значений.

    Возвращает:
        S-MAPE: Симметричное среднее абсолютное процентное отклонение.
    """
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    return np.mean(np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred))) * 100

# Пример использования функции
y_true = [0.1, 0.1, 0.1]
y_pred = [90, 90, 90]

smap = calculate_smap(y_true, y_pred)
print(f"S-MAPE: {smap:.2f}%")
```

В этом коде:
- Мы определяем функцию `calculate_smap`, которая принимает массивы истинных и предсказанных значений.
- Функция возвращает значение S-MAPE.
- Мы выводим результат на экран.

### Физический и геометрический смысл

В физике, когда мы измеряем величины, такие как температура или давление, важно, чтобы метрики ошибки были устойчивыми к малым значениям. Например, если мы измеряем давление в системе, и истинное значение близко к нулю, использование S-MAPE позволяет более адекватно оценивать качество предсказаний, так как она учитывает как истинные, так и предсказанные значения, что делает её более надежной в ситуациях, когда истинные значения могут быть малы.

## Chunk 9
### **Название фрагмента: Сравнение метрик MAE и MSE**

**Предыдущий контекст:** В предыдущем фрагменте мы обсудили проблемы с MAPE, особенно когда истинные значения близки к нулю, и преимущества S-MAPE в таких ситуациях. Теперь мы перейдем к сравнению двух других метрик: средней абсолютной ошибки (MAE) и средней квадратичной ошибки (MSE).

## **Средняя абсолютная ошибка (MAE) и средняя квадратичная ошибка (MSE)**

MAE и MSE — это две распространенные метрики, используемые для оценки качества предсказаний моделей. Они помогают понять, насколько близки предсказанные значения к истинным.

### Средняя абсолютная ошибка (MAE)

MAE определяется как среднее значение абсолютных ошибок между истинными и предсказанными значениями:

$$
\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
$$

где:
- $y_i$ — истинное значение,
- $\hat{y}_i$ — предсказанное значение,
- $n$ — количество наблюдений.

MAE показывает среднюю величину ошибок в тех же единицах, что и сами данные, что делает её легко интерпретируемой.

### Пример вычисления MAE

Для набора данных:
- Истинные значения: $[3, -0.5, 2, 7]$
- Предсказанные значения: $[2.5, 0, 2, 8]$

Вычислим MAE:

1. Вычисляем абсолютные ошибки:
   - $|3 - 2.5| = 0.5$
   - $|-0.5 - 0| = 0.5$
   - $|2 - 2| = 0$
   - $|7 - 8| = 1$

2. Суммируем абсолютные ошибки:
   - $0.5 + 0.5 + 0 + 1 = 2$

3. Делим на количество наблюдений:
   - $\text{MAE} = \frac{2}{4} = 0.5$

### Средняя квадратичная ошибка (MSE)

MSE определяется как среднее значение квадратов ошибок:

$$
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

MSE более чувствительна к выбросам, так как ошибки возводятся в квадрат, что приводит к увеличению влияния больших ошибок на итоговое значение.

### Пример вычисления MSE

Для тех же данных:

1. Вычисляем квадраты ошибок:
   - $(3 - 2.5)^2 = 0.25$
   - $(-0.5 - 0)^2 = 0.25$
   - $(2 - 2)^2 = 0$
   - $(7 - 8)^2 = 1$

2. Суммируем квадраты ошибок:
   - $0.25 + 0.25 + 0 + 1 = 1.5$

3. Делим на количество наблюдений:
   - $\text{MSE} = \frac{1.5}{4} = 0.375$

### Сравнение MAE и MSE

- **MAE**: Легко интерпретируется, показывает среднюю ошибку в тех же единицах, что и данные. Не чувствительна к выбросам.
- **MSE**: Чувствительна к выбросам, так как ошибки возводятся в квадрат. Это может быть полезно, если важно минимизировать большие ошибки.

### Пример кода

Давайте рассмотрим пример кода на Python, который вычисляет MAE и MSE для заданных истинных и предсказанных значений:

```python
import numpy as np

def calculate_mae(y_true, y_pred):
    """
    Описание:
        Вычисляет среднюю абсолютную ошибку (MAE) для заданных истинных и предсказанных значений.

    Аргументы:
        y_true: Массив истинных значений.
        y_pred: Массив предсказанных значений.

    Возвращает:
        MAE: Средняя абсолютная ошибка.
    """
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    return np.mean(np.abs(y_true - y_pred))

def calculate_mse(y_true, y_pred):
    """
    Описание:
        Вычисляет среднюю квадратичную ошибку (MSE) для заданных истинных и предсказанных значений.

    Аргументы:
        y_true: Массив истинных значений.
        y_pred: Массив предсказанных значений.

    Возвращает:
        MSE: Средняя квадратичная ошибка.
    """
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    return np.mean((y_true - y_pred) ** 2)

# Пример использования функций
y_true = [3, -0.5, 2, 7]
y_pred = [2.5, 0, 2, 8]

mae = calculate_mae(y_true, y_pred)
mse = calculate_mse(y_true, y_pred)

print(f"MAE: {mae:.2f}, MSE: {mse:.2f}")
```

В этом коде:
- Мы определяем функции `calculate_mae` и `calculate_mse`, которые принимают массивы истинных и предсказанных значений.
- Каждая функция возвращает соответствующую метрику.
- Мы выводим результаты на экран.

### Физический и геометрический смысл

В физике MAE и MSE могут быть использованы для оценки точности измерений. Например, если мы измеряем длину объекта, MAE даст нам представление о средней ошибке в измерениях, а MSE покажет, насколько сильно отклоняются измерения от истинного значения, особенно если есть выбросы. Это позволяет исследователям и инженерам лучше понимать, насколько точны их измерения и предсказания.

## Chunk 10
### **Название фрагмента: Понимание R-квадрат и его применение**

**Предыдущий контекст:** В предыдущем фрагменте мы обсудили среднюю абсолютную ошибку (MAE) и среднюю квадратичную ошибку (MSE), а также их особенности и применение. Теперь мы перейдем к метрике R-квадрат, которая помогает оценить качество предсказаний модели в более понятных терминах.

## **Метрика R-квадрат**

R-квадрат (или коэффициент детерминации) — это метрика, которая измеряет, насколько хорошо предсказанные значения модели соответствуют истинным значениям. Она позволяет понять, какую долю вариации в данных объясняет модель.

### Формула R-квадрат

R-квадрат определяется следующим образом:

$$
R^2 = 1 - \frac{\text{MSE}_{\text{модели}}}{\text{MSE}_{\text{константной}}}
$$

где:
- $\text{MSE}_{\text{модели}}$ — средняя квадратичная ошибка модели, которую мы хотим оценить,
- $\text{MSE}_{\text{константной}}$ — средняя квадратичная ошибка константной модели, которая всегда предсказывает среднее значение истинных значений.

### Понимание R-квадрат

1. **Значение от 0 до 1:** R-квадрат принимает значения от $0$ до $1$. Если $R^2 = 1$, это означает, что модель идеально предсказывает все значения. Если $R^2 = 0$, это означает, что модель не лучше, чем простая константная модель, которая всегда предсказывает среднее значение.

2. **Интерпретация:** Если $R^2 = 0.5$, это означает, что модель объясняет 50% вариации в данных. Если $R^2 < 0.5$, это указывает на то, что модель работает хуже, чем константная модель.

### Пример вычисления R-квадрат

Для набора данных:
- Истинные значения: $[3, -0.5, 2, 7]$
- Предсказанные значения: $[2.5, 0, 2, 8]$

1. **Вычисляем MSE модели:**
   - Мы уже вычислили MSE и получили значение $0.375$.

2. **Вычисляем MSE константной модели:**
   - Сначала находим среднее значение истинных значений:
   $$
   \bar{y} = \frac{3 + (-0.5) + 2 + 7}{4} = 2.625
   $$
   - Затем вычисляем MSE для константной модели:
   $$
   \text{MSE}_{\text{константной}} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \bar{y})^2
   $$
   - Подставляем значения:
   $$
   \text{MSE}_{\text{константной}} = \frac{1}{4} \left( (3 - 2.625)^2 + (-0.5 - 2.625)^2 + (2 - 2.625)^2 + (7 - 2.625)^2 \right)
   $$
   - Вычисляем:
   $$
   = \frac{1}{4} \left( 0.140625 + 9.765625 + 0.390625 + 19.140625 \right) = \frac{29.4375}{4} = 7.359375
   $$

3. **Вычисляем R-квадрат:**
   $$
   R^2 = 1 - \frac{0.375}{7.359375} \approx 1 - 0.051 \approx 0.949
   $$

### Пример кода

Давайте рассмотрим пример кода на Python, который вычисляет R-квадрат для заданных истинных и предсказанных значений:

```python
import numpy as np

def calculate_r_squared(y_true, y_pred):
    """
    Описание:
        Вычисляет R-квадрат для заданных истинных и предсказанных значений.

    Аргументы:
        y_true: Массив истинных значений.
        y_pred: Массив предсказанных значений.

    Возвращает:
        R-квадрат: Коэффициент детерминации.
    """
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    
    # Вычисляем MSE модели
    mse_model = np.mean((y_true - y_pred) ** 2)
    
    # Вычисляем среднее значение истинных значений
    mean_y_true = np.mean(y_true)
    
    # Вычисляем MSE константной модели
    mse_constant = np.mean((y_true - mean_y_true) ** 2)
    
    # Вычисляем R-квадрат
    r_squared = 1 - (mse_model / mse_constant)
    
    return r_squared

# Пример использования функции
y_true = [3, -0.5, 2, 7]
y_pred = [2.5, 0, 2, 8]

r_squared = calculate_r_squared(y_true, y_pred)
print(f"R-квадрат: {r_squared:.3f}")
```

В этом коде:
- Мы определяем функцию `calculate_r_squared`, которая принимает массивы истинных и предсказанных значений.
- Функция возвращает значение R-квадрат.
- Мы выводим результат на экран.

### Физический и геометрический смысл

В физике R-квадрат может быть использован для оценки качества моделей, которые предсказывают физические явления, такие как скорость, давление или температуру. Например, если мы моделируем температуру в комнате на основе различных факторов, R-квадрат поможет понять, насколько хорошо модель объясняет вариации в температуре. Высокое значение R-квадрат указывает на то, что модель хорошо справляется с предсказаниями, в то время как низкое значение может сигнализировать о необходимости улучшения модели.

## Chunk 11
### **Название фрагмента: Понимание относительных метрик и их применение**

**Предыдущий контекст:** В предыдущем фрагменте мы обсудили метрики MAE и MSE, а также их особенности и применение. Теперь мы сосредоточимся на относительных метриках, таких как MAPE и S-MAPE, и их значении для оценки качества предсказаний моделей.

## **Относительные метрики: MAPE и S-MAPE**

Относительные метрики, такие как MAPE (Mean Absolute Percentage Error) и S-MAPE (Symmetric Mean Absolute Percentage Error), позволяют оценить ошибки предсказаний в процентах. Это делает их более понятными для людей, не знакомых с математикой, например, для бизнес-аналитиков.

### Значение MAPE и S-MAPE

1. **MAPE**: Эта метрика показывает среднее абсолютное процентное отклонение предсказанных значений от истинных значений. Она вычисляется по формуле:

$$
\text{MAPE} = \frac{1}{n} \sum_{i=1}^{n} \left| \frac{y_i - \hat{y}_i}{y_i} \right| \times 100\%
$$

где $y_i$ — истинное значение, $\hat{y}_i$ — предсказанное значение, а $n$ — количество наблюдений.

2. **S-MAPE**: Эта метрика учитывает как истинные, так и предсказанные значения в знаменателе, что делает её более устойчивой к малым значениям:

$$
\text{S-MAPE} = \frac{1}{n} \sum_{i=1}^{n} \frac{|y_i - \hat{y}_i|}{|y_i| + |\hat{y}_i|} \times 100\%
$$

### Проблемы с R-квадрат

R-квадрат (коэффициент детерминации) также является важной метрикой, но она может принимать отрицательные значения, если модель предсказывает хуже, чем простая константная модель. Это может произойти, если модель не учитывает данные должным образом или если в данных есть ошибки. В идеале R-квадрат должен находиться в диапазоне от $0$ до $1$, где $1$ указывает на идеальное предсказание.

### Пример вычисления MAPE и S-MAPE

Для набора данных:
- Истинные значения: $[3, -0.5, 2, 7]$
- Предсказанные значения: $[2.5, 0, 2, 8]$

1. **Вычисляем MAPE:**
   - Абсолютные ошибки:
     - $|3 - 2.5| = 0.5$
     - $|-0.5 - 0| = 0.5$
     - $|2 - 2| = 0$
     - $|7 - 8| = 1$
   - Суммируем: $0.5 + 0.5 + 0 + 1 = 2$
   - MAPE: $\frac{2}{4} \times 100\% = 50\%$

2. **Вычисляем S-MAPE:**
   - S-MAPE: $\frac{1}{4} \left( \frac{|3 - 2.5|}{|3| + |2.5|} + \frac{|-0.5 - 0|}{|-0.5| + |0|} + \frac{|2 - 2|}{|2| + |2|} + \frac{|7 - 8|}{|7| + |8|} \right) \times 100\%$
   - Подсчитываем: $\frac{1}{4} \left( \frac{0.5}{5.5} + \frac{0.5}{0.5} + 0 + \frac{1}{15} \right) \times 100\%$

### Пример кода

Давайте рассмотрим пример кода на Python, который вычисляет MAPE и S-MAPE для заданных истинных и предсказанных значений:

```python
import numpy as np

def calculate_mape(y_true, y_pred):
    """
    Описание:
        Вычисляет MAPE для заданных истинных и предсказанных значений.

    Аргументы:
        y_true: Массив истинных значений.
        y_pred: Массив предсказанных значений.

    Возвращает:
        MAPE: Среднее абсолютное процентное отклонение.
    """
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100

def calculate_smap(y_true, y_pred):
    """
    Описание:
        Вычисляет S-MAPE для заданных истинных и предсказанных значений.

    Аргументы:
        y_true: Массив истинных значений.
        y_pred: Массив предсказанных значений.

    Возвращает:
        S-MAPE: Симметричное среднее абсолютное процентное отклонение.
    """
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    return np.mean(np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred))) * 100

# Пример использования функций
y_true = [3, -0.5, 2, 7]
y_pred = [2.5, 0, 2, 8]

mape = calculate_mape(y_true, y_pred)
smap = calculate_smap(y_true, y_pred)

print(f"MAPE: {mape:.2f}%, S-MAPE: {smap:.2f}%")
```

В этом коде:
- Мы определяем функции `calculate_mape` и `calculate_smap`, которые принимают массивы истинных и предсказанных значений.
- Каждая функция возвращает соответствующую метрику.
- Мы выводим результаты на экран.

### Физический и геометрический смысл

В физике относительные метрики, такие как MAPE и S-MAPE, могут быть использованы для оценки точности измерений. Например, если мы измеряем скорость объекта, MAPE поможет нам понять, насколько близки наши предсказания к истинным значениям скорости. Высокие значения MAPE могут указывать на необходимость улучшения модели, в то время как низкие значения указывают на высокую точность предсказаний.

## Chunk 12
### **Название фрагмента: Гибридные метрики: Huber и Log-Cosh**

**Предыдущий контекст:** В предыдущем фрагменте мы обсудили относительные метрики, такие как MAPE и S-MAPE, и их значение для оценки качества предсказаний моделей. Теперь мы сосредоточимся на гибридных метриках, таких как Huber и Log-Cosh, которые объединяют преимущества других метрик.

## **Метрика Huber и Log-Cosh**

Метрика Huber и Log-Cosh являются гибридными метриками, которые стремятся объединить преимущества средней абсолютной ошибки (MAE) и средней квадратичной ошибки (MSE). Они позволяют более эффективно справляться с выбросами и обеспечивают более стабильные результаты.

### Метрика Huber

Метрика Huber определяется следующим образом:

$$
L_{\delta}(y, \hat{y}) = 
\begin{cases} 
\frac{1}{2}(y - \hat{y})^2 & \text{если } |y - \hat{y}| \leq \delta \\
\delta \cdot (|y - \hat{y}| - \frac{1}{2}\delta) & \text{иначе}
\end{cases}
$$

где:
- $y$ — истинное значение,
- $\hat{y}$ — предсказанное значение,
- $\delta$ — порог, который определяет, когда использовать MSE или MAE.

1. **Когда использовать MSE:** Если ошибка (разность между истинным и предсказанным значениями) меньше порога $\delta$, используется MSE, что позволяет более точно оценивать небольшие ошибки.
2. **Когда использовать MAE:** Если ошибка больше порога $\delta$, используется MAE, что делает метрику менее чувствительной к выбросам.

### Пример вычисления Huber

Для набора данных:
- Истинные значения: $[3, -0.5, 2, 7]$
- Предсказанные значения: $[2.5, 0, 2, 8]$
- Порог $\delta = 1$

1. Вычисляем ошибки:
   - $|3 - 2.5| = 0.5$ (используем MSE)
   - $|-0.5 - 0| = 0.5$ (используем MSE)
   - $|2 - 2| = 0$ (используем MSE)
   - $|7 - 8| = 1$ (используем MAE)

2. Подсчитываем Huber:
   - $L_{\delta}(3, 2.5) = \frac{1}{2}(0.5)^2 = 0.125$
   - $L_{\delta}(-0.5, 0) = \frac{1}{2}(0.5)^2 = 0.125$
   - $L_{\delta}(2, 2) = \frac{1}{2}(0)^2 = 0$
   - $L_{\delta}(7, 8) = 1 \cdot (1 - \frac{1}{2}) = 0.5$

3. Итоговая метрика Huber:
   - $L_H = \frac{0.125 + 0.125 + 0 + 0.5}{4} = 0.1875$

### Метрика Log-Cosh

Log-Cosh — это еще одна гибридная метрика, которая определяется как:

$$
L(y, \hat{y}) = \log(\cosh(y - \hat{y}))
$$

где $\cosh(x)$ — гиперболический косинус, который вычисляется как:

$$
\cosh(x) = \frac{e^x + e^{-x}}{2}
$$

Log-Cosh ведет себя как MSE для небольших ошибок и как MAE для больших ошибок, что делает её устойчивой к выбросам.

### Пример вычисления Log-Cosh

Для тех же данных:

1. Вычисляем Log-Cosh для каждой ошибки:
   - $L(3, 2.5) = \log(\cosh(0.5))$
   - $L(-0.5, 0) = \log(\cosh(0.5))$
   - $L(2, 2) = \log(\cosh(0)) = 0$
   - $L(7, 8) = \log(\cosh(-1))$

2. Подсчитываем итоговое значение Log-Cosh.

### Пример кода

Давайте рассмотрим пример кода на Python, который вычисляет Huber и Log-Cosh для заданных истинных и предсказанных значений:

```python
import numpy as np

def calculate_huber(y_true, y_pred, delta):
    """
    Описание:
        Вычисляет метрику Huber для заданных истинных и предсказанных значений.

    Аргументы:
        y_true: Массив истинных значений.
        y_pred: Массив предсказанных значений.
        delta: Порог для определения использования MSE или MAE.

    Возвращает:
        Huber: Значение метрики Huber.
    """
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    error = np.abs(y_true - y_pred)
    return np.mean(np.where(error <= delta, 0.5 * error**2, delta * (error - 0.5 * delta)))

def calculate_log_cosh(y_true, y_pred):
    """
    Описание:
        Вычисляет метрику Log-Cosh для заданных истинных и предсказанных значений.

    Аргументы:
        y_true: Массив истинных значений.
        y_pred: Массив предсказанных значений.

    Возвращает:
        Log-Cosh: Значение метрики Log-Cosh.
    """
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    return np.mean(np.log(np.cosh(y_true - y_pred)))

# Пример использования функций
y_true = [3, -0.5, 2, 7]
y_pred = [2.5, 0, 2, 8]
delta = 1

huber = calculate_huber(y_true, y_pred, delta)
log_cosh = calculate_log_cosh(y_true, y_pred)

print(f"Huber: {huber:.3f}, Log-Cosh: {log_cosh:.3f}")
```

В этом коде:
- Мы определяем функции `calculate_huber` и `calculate_log_cosh`, которые принимают массивы истинных и предсказанных значений.
- Каждая функция возвращает соответствующую метрику.
- Мы выводим результаты на экран.

### Физический и геометрический смысл

В физике метрики Huber и Log-Cosh могут быть использованы для оценки точности измерений, особенно в ситуациях, когда данные содержат выбросы. Например, если мы измеряем скорость объекта, Huber и Log-Cosh помогут более точно оценить качество предсказаний, так как они учитывают как небольшие, так и большие ошибки, что делает их более надежными в реальных условиях.

## Chunk 13
### **Название фрагмента: Квантильная функция потери и её применение**

**Предыдущий контекст:** В предыдущем фрагменте мы обсудили метрики Huber и Log-Cosh, которые помогают справляться с выбросами и обеспечивают более стабильные результаты. Теперь мы сосредоточимся на квантильной функции потери, которая позволяет учитывать разные уровни ошибок в зависимости от задачи.

## **Квантильная функция потери**

Квантильная функция потери — это метрика, которая позволяет настраивать модель в зависимости от того, насколько критично недопрогнозировать или перепрогнозировать значения. Она используется для оценки качества предсказаний, учитывая, что в некоторых ситуациях недопрогноз может быть более критичным, чем перепрогноз.

### Формула квантильной функции потери

Квантильная функция потери может быть записана следующим образом:

$$
L(y, \hat{y}) = 
\begin{cases} 
\theta (y - \hat{y}) & \text{если } y - \hat{y} < 0 \\
(1 - \theta)(y - \hat{y}) & \text{иначе}
\end{cases}
$$

где:
- $y$ — истинное значение,
- $\hat{y}$ — предсказанное значение,
- $\theta$ — гиперпараметр, который определяет, насколько сильно мы хотим наказывать за недопрогноз или перепрогноз.

### Понимание квантильной функции потери

1. **Недопрогноз:** Если предсказанное значение меньше истинного, то используется первый случай формулы, где мы умножаем разность на $\theta$. Это позволяет более строго наказывать модель за недопрогноз.

2. **Перепрогноз:** Если предсказанное значение больше истинного, то используется второй случай, где разность умножается на $(1 - \theta)$. Это позволяет менее строго наказывать модель за перепрогноз.

### Пример применения

Предположим, у нас есть задача предсказать поставки продуктов на склад. Если мы предскажем 90 коробок, когда нужно 100, это приведет к недопродаже, что может быть критично для бизнеса. Если же мы предскажем 110 коробок, это приведет к избыточным запасам, что менее критично.

Если мы установим $\theta = 0.6$, это означает, что мы будем наказывать модель сильнее за недопрогноз, чем за перепрогноз. Таким образом, если модель предсказывает 90, то потеря будет:

$$
L(100, 90) = 0.6 \cdot (100 - 90) = 6
$$

А если модель предсказывает 110:

$$
L(100, 110) = 0.4 \cdot (100 - 110) = -4
$$

### Пример кода

Давайте рассмотрим пример кода на Python, который вычисляет квантильную функцию потери для заданных истинных и предсказанных значений:

```python
import numpy as np

def calculate_quantile_loss(y_true, y_pred, theta):
    """
    Описание:
        Вычисляет квантильную функцию потери для заданных истинных и предсказанных значений.

    Аргументы:
        y_true: Массив истинных значений.
        y_pred: Массив предсказанных значений.
        theta: Гиперпараметр для настройки потерь.

    Возвращает:
        Quantile Loss: Значение квантильной функции потери.
    """
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    loss = np.where(y_true - y_pred < 0, 
                    theta * (y_true - y_pred), 
                    (1 - theta) * (y_true - y_pred))
    return np.mean(loss)

# Пример использования функции
y_true = [100, 100, 100]
y_pred = [90, 110, 95]
theta = 0.6

quantile_loss = calculate_quantile_loss(y_true, y_pred, theta)
print(f"Квантильная функция потери: {quantile_loss:.2f}")
```

В этом коде:
- Мы определяем функцию `calculate_quantile_loss`, которая принимает массивы истинных и предсказанных значений, а также гиперпараметр $\theta$.
- Функция возвращает значение квантильной функции потери.
- Мы выводим результат на экран.

### Физический и геометрический смысл

В физике квантильная функция потери может быть использована для оценки предсказаний в задачах, где последствия недопрогноза и перепрогноза различны. Например, в задачах управления запасами, где недостача товара может привести к потерям, а избыточные запасы могут быть менее критичными. Квантильная функция потери позволяет более точно настраивать модель в зависимости от бизнес-целей и рисков.

## Chunk 14
### **Название фрагмента: Сравнение MSE и MAE в контексте выбросов**

**Предыдущий контекст:** В предыдущем фрагменте мы обсудили метрики Huber и Log-Cosh, которые помогают справляться с выбросами и обеспечивают более стабильные результаты. Теперь мы сосредоточимся на сравнении средней квадратичной ошибки (MSE) и средней абсолютной ошибки (MAE) в контексте наличия выбросов.

## **Сравнение MSE и MAE**

MSE и MAE — это две основные метрики, используемые для оценки качества предсказаний моделей. Они имеют разные свойства и поведение, особенно в присутствии выбросов.

### Средняя квадратичная ошибка (MSE)

MSE определяется как среднее значение квадратов ошибок:

$$
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

где:
- $y_i$ — истинное значение,
- $\hat{y}_i$ — предсказанное значение,
- $n$ — количество наблюдений.

MSE более чувствительна к выбросам, так как ошибки возводятся в квадрат. Это означает, что большие ошибки оказывают значительно большее влияние на итоговое значение MSE.

### Средняя абсолютная ошибка (MAE)

MAE определяется как среднее значение абсолютных ошибок:

$$
\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
$$

MAE менее чувствительна к выбросам, так как она просто суммирует абсолютные ошибки, не возводя их в квадрат.

### Пример сравнения MSE и MAE

Предположим, у нас есть набор данных:
- Истинные значения: $[3, -0.5, 2, 7]$
- Предсказанные значения: $[2.5, 0, 2, 8]$

1. **Вычисляем MSE:**
   - Ошибки: 
     - $(3 - 2.5)^2 = 0.25$
     - $(-0.5 - 0)^2 = 0.25$
     - $(2 - 2)^2 = 0$
     - $(7 - 8)^2 = 1$
   - Суммируем: $0.25 + 0.25 + 0 + 1 = 1.5$
   - MSE: $\frac{1.5}{4} = 0.375$

2. **Вычисляем MAE:**
   - Абсолютные ошибки:
     - $|3 - 2.5| = 0.5$
     - $|-0.5 - 0| = 0.5$
     - $|2 - 2| = 0$
     - $|7 - 8| = 1$
   - Суммируем: $0.5 + 0.5 + 0 + 1 = 2$
   - MAE: $\frac{2}{4} = 0.5$

### Математическое ожидание и дисперсия

Для дальнейшего анализа, давайте рассмотрим математическое ожидание и дисперсию ошибок. 

1. **Дисперсия** определяется как:

$$
D(X) = E(X^2) - (E(X))^2
$$

где $E(X)$ — математическое ожидание.

2. **Математическое ожидание** для MSE можно записать как:

$$
E(\text{MSE}) = E((y - \hat{y})^2)
$$

3. **Математическое ожидание** для MAE можно записать как:

$$
E(\text{MAE}) = E(|y - \hat{y}|)
$$

### Пример кода

Давайте рассмотрим пример кода на Python, который вычисляет MSE и MAE для заданных истинных и предсказанных значений:

```python
import numpy as np

def calculate_mae(y_true, y_pred):
    """
    Описание:
        Вычисляет среднюю абсолютную ошибку (MAE) для заданных истинных и предсказанных значений.

    Аргументы:
        y_true: Массив истинных значений.
        y_pred: Массив предсказанных значений.

    Возвращает:
        MAE: Средняя абсолютная ошибка.
    """
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    return np.mean(np.abs(y_true - y_pred))

def calculate_mse(y_true, y_pred):
    """
    Описание:
        Вычисляет среднюю квадратичную ошибку (MSE) для заданных истинных и предсказанных значений.

    Аргументы:
        y_true: Массив истинных значений.
        y_pred: Массив предсказанных значений.

    Возвращает:
        MSE: Средняя квадратичная ошибка.
    """
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    return np.mean((y_true - y_pred) ** 2)

# Пример использования функций
y_true = [3, -0.5, 2, 7]
y_pred = [2.5, 0, 2, 8]

mae = calculate_mae(y_true, y_pred)
mse = calculate_mse(y_true, y_pred)

print(f"MAE: {mae:.2f}, MSE: {mse:.2f}")
```

В этом коде:
- Мы определяем функции `calculate_mae` и `calculate_mse`, которые принимают массивы истинных и предсказанных значений.
- Каждая функция возвращает соответствующую метрику.
- Мы выводим результаты на экран.

### Физический и геометрический смысл

В физике MAE и MSE могут быть использованы для оценки точности измерений. Например, если мы измеряем длину объекта, MAE даст нам представление о средней ошибке в измерениях, а MSE покажет, насколько сильно отклоняются измерения от истинного значения, особенно если есть выбросы. Это позволяет исследователям и инженерам лучше понимать, насколько точны их измерения и предсказания.

## Chunk 15
### **Название фрагмента: Влияние выбросов на MSE и MAE**

**Предыдущий контекст:** В предыдущем фрагменте мы обсудили метрики Huber и Log-Cosh, а также их применение для оценки качества предсказаний в условиях наличия выбросов. Теперь мы сосредоточимся на сравнении MSE и MAE в контексте наличия выбросов и их влияния на математическое ожидание.

## **Влияние выбросов на MSE и MAE**

Выбросы в данных могут значительно повлиять на метрики оценки качества предсказаний, такие как средняя квадратичная ошибка (MSE) и средняя абсолютная ошибка (MAE). Понимание этого влияния важно для правильной интерпретации результатов.

### Математическое ожидание MSE и MAE

1. **MSE**: Если в данных присутствуют выбросы, то MSE будет увеличиваться, так как она чувствительна к большим ошибкам. Математическое ожидание MSE можно записать как:

$$
E(\text{MSE}) = P_{\text{базовая}} \cdot E(\text{базовая ошибка}) + P_{\text{выброс}} \cdot E(\text{выброс})
$$

где:
- $P_{\text{базовая}}$ — вероятность того, что ошибка является базовой (например, $0.95$),
- $E(\text{базовая ошибка})$ — математическое ожидание базовой ошибки,
- $P_{\text{выброс}}$ — вероятность того, что ошибка является выбросом (например, $0.05$),
- $E(\text{выброс})$ — математическое ожидание ошибки для выбросов.

2. **MAE**: Аналогично, математическое ожидание MAE можно записать как:

$$
E(\text{MAE}) = P_{\text{базовая}} \cdot E(|\text{базовая ошибка}|) + P_{\text{выброс}} \cdot E(|\text{выброс}|) 
$$

### Пример вычисления

Предположим, что у нас есть 5% выбросов в данных. Если базовая ошибка имеет дисперсию $1$, а выбросы имеют дисперсию $25$, то:

1. **Для MSE:**
   - $E(\text{MSE}) = 0.95 \cdot 1 + 0.05 \cdot 25 = 0.95 + 1.25 = 2.20$

2. **Для MAE:**
   - Если базовая ошибка равна $1$, а выбросы имеют модуль ошибки $5$, то:
   - $E(\text{MAE}) = 0.95 \cdot 1 + 0.05 \cdot 5 = 0.95 + 0.25 = 1.20$

### Пример кода

Давайте рассмотрим пример кода на Python, который вычисляет MSE и MAE с учетом выбросов:

```python
import numpy as np

def calculate_mse_with_outliers(y_true, y_pred, p_outliers, mse_base, mse_outlier):
    """
    Описание:
        Вычисляет MSE с учетом выбросов.

    Аргументы:
        y_true: Массив истинных значений.
        y_pred: Массив предсказанных значений.
        p_outliers: Вероятность выбросов.
        mse_base: Математическое ожидание базовой ошибки.
        mse_outlier: Математическое ожидание ошибки для выбросов.

    Возвращает:
        MSE: Средняя квадратичная ошибка с учетом выбросов.
    """
    p_base = 1 - p_outliers
    return p_base * mse_base + p_outliers * mse_outlier

# Пример использования функции
p_outliers = 0.05  # 5% выбросов
mse_base = 1  # Математическое ожидание базовой ошибки
mse_outlier = 25  # Математическое ожидание ошибки для выбросов

mse_with_outliers = calculate_mse_with_outliers([], [], p_outliers, mse_base, mse_outlier)
print(f"MSE с учетом выбросов: {mse_with_outliers:.2f}")
```

В этом коде:
- Мы определяем функцию `calculate_mse_with_outliers`, которая принимает вероятность выбросов и математические ожидания ошибок.
- Функция возвращает значение MSE с учетом выбросов.
- Мы выводим результат на экран.

### Физический и геометрический смысл

В физике выбросы могут возникать в результате ошибок измерений или аномальных условий. Например, если мы измеряем температуру в разных точках, выбросы могут указывать на неправильные показания термометра. Понимание влияния выбросов на MSE и MAE позволяет исследователям и инженерам более точно оценивать качество своих моделей и принимать обоснованные решения о том, как обрабатывать данные.

## Chunk 16
### **Название фрагмента: Влияние выбросов на MSE и MAE, и применение Huber**

**Предыдущий контекст:** В предыдущем фрагменте мы обсудили метрики MSE и MAE, а также их поведение в условиях наличия выбросов. Теперь мы сосредоточимся на том, как выбросы влияют на эти метрики и как метрика Huber может быть использована для их оценки.

## **Влияние выбросов на MSE и MAE, и применение Huber**

Выбросы в данных могут значительно повлиять на метрики оценки качества предсказаний, такие как MSE и MAE. Понимание этого влияния важно для правильной интерпретации результатов и выбора подходящей метрики.

### Математическое ожидание MSE и MAE

1. **MSE**: Если в данных присутствуют выбросы, то MSE будет увеличиваться, так как она чувствительна к большим ошибкам. Математическое ожидание MSE можно записать как:

$$
E(\text{MSE}) = P_{\text{базовая}} \cdot E(\text{базовая ошибка}) + P_{\text{выброс}} \cdot E(\text{выброс})
$$

где:
- $P_{\text{базовая}}$ — вероятность того, что ошибка является базовой (например, $0.95$),
- $E(\text{базовая ошибка})$ — математическое ожидание базовой ошибки,
- $P_{\text{выброс}}$ — вероятность того, что ошибка является выбросом (например, $0.05$),
- $E(\text{выброс})$ — математическое ожидание ошибки для выбросов.

2. **MAE**: Аналогично, математическое ожидание MAE можно записать как:

$$
E(\text{MAE}) = P_{\text{базовая}} \cdot E(|\text{базовая ошибка}|) + P_{\text{выброс}} \cdot E(|\text{выброс}|) 
$$

### Пример вычисления

Предположим, что у нас есть 5% выбросов в данных. Если базовая ошибка имеет дисперсию $1$, а выбросы имеют дисперсию $25$, то:

1. **Для MSE:**
   - $E(\text{MSE}) = 0.95 \cdot 1 + 0.05 \cdot 25 = 0.95 + 1.25 = 2.20$

2. **Для MAE:**
   - Если базовая ошибка равна $1$, а выбросы имеют модуль ошибки $5$, то:
   - $E(\text{MAE}) = 0.95 \cdot 1 + 0.05 \cdot 5 = 0.95 + 0.25 = 1.20$

### Применение Huber

Метрика Huber может быть использована для более устойчивой оценки ошибок в условиях наличия выбросов. Она сочетает в себе преимущества MSE и MAE, позволяя использовать MSE для небольших ошибок и MAE для больших ошибок.

1. **Формула Huber:**

$$
L_{\delta}(y, \hat{y}) = 
\begin{cases} 
\frac{1}{2}(y - \hat{y})^2 & \text{если } |y - \hat{y}| \leq \delta \\
\delta \cdot (|y - \hat{y}| - \frac{1}{2}\delta) & \text{иначе}
\end{cases}
$$

где $\delta$ — порог, который определяет, когда использовать MSE или MAE.

### Пример кода для Huber

Давайте рассмотрим пример кода на Python, который вычисляет Huber для заданных истинных и предсказанных значений:

```python
import numpy as np

def calculate_huber(y_true, y_pred, delta):
    """
    Описание:
        Вычисляет метрику Huber для заданных истинных и предсказанных значений.

    Аргументы:
        y_true: Массив истинных значений.
        y_pred: Массив предсказанных значений.
        delta: Порог для определения использования MSE или MAE.

    Возвращает:
        Huber: Значение метрики Huber.
    """
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    error = np.abs(y_true - y_pred)
    return np.mean(np.where(error <= delta, 0.5 * error**2, delta * (error - 0.5 * delta)))

# Пример использования функции
y_true = [3, -0.5, 2, 7]
y_pred = [2.5, 0, 2, 8]
delta = 1

huber = calculate_huber(y_true, y_pred, delta)
print(f"Huber: {huber:.3f}")
```

В этом коде:
- Мы определяем функцию `calculate_huber`, которая принимает массивы истинных и предсказанных значений, а также порог $\delta$.
- Функция возвращает значение Huber.
- Мы выводим результат на экран.

### Физический и геометрический смысл

В физике метрики Huber и Log-Cosh могут быть использованы для оценки точности измерений, особенно в ситуациях, когда данные содержат выбросы. Например, если мы измеряем скорость объекта, Huber поможет более точно оценить качество предсказаний, так как она учитывает как небольшие, так и большие ошибки, что делает её более надежной в реальных условиях.

## Chunk 17
### **Название фрагмента: Квантильные интервалы и метод Bootstrap**

**Предыдущий контекст:** В предыдущем фрагменте мы обсудили влияние выбросов на метрики MSE и MAE, а также применение метрики Huber для более устойчивой оценки ошибок. Теперь мы сосредоточимся на методе Bootstrap и его применении для оценки доверительных интервалов метрик.

## **Метод Bootstrap и доверительные интервалы**

Метод Bootstrap — это статистический метод, который позволяет оценить распределение статистики (например, среднего, медианы или других метрик) на основе повторных выборок из имеющихся данных. Он особенно полезен для построения доверительных интервалов, которые помогают понять, насколько надежны результаты модели.

### Зачем нужны доверительные интервалы?

Доверительные интервалы показывают диапазон значений, в котором с определенной вероятностью (например, 99%) находится истинное значение метрики. Это важно для оценки качества модели, так как позволяет понять, насколько результаты воспроизводимы.

1. **Воспроизводимость модели:** Если модель возвращает результаты, которые сильно варьируются, это может указывать на то, что модель не стабильна и не подходит для использования в продакшене. Доверительные интервалы помогают выявить такие проблемы.

2. **Проверка результатов:** Другие исследователи или аналитики могут проверить, действительно ли модель дает стабильные результаты, основываясь на доверительных интервалах.

### Применение метода Bootstrap

Метод Bootstrap включает следующие шаги:

1. **Создание Bootstrap выборок:** Из исходного набора данных создаются несколько выборок (обычно $B$), каждая из которых имеет такой же размер, как и исходный набор. При этом объекты могут повторяться, так как выборка производится с возвращением.

2. **Вычисление метрики:** Для каждой Bootstrap выборки вычисляется интересующая метрика (например, MAE или MSE).

3. **Оценка доверительного интервала:** На основе полученных значений метрики из Bootstrap выборок можно построить доверительный интервал. Например, можно взять 2.5-й и 97.5-й процентиль значений метрики, чтобы получить 95% доверительный интервал.

### Пример кода

Давайте рассмотрим пример кода на Python, который реализует метод Bootstrap для оценки доверительного интервала:

```python
import numpy as np

def bootstrap_ci(data, num_samples, metric_func, alpha=0.05):
    """
    Описание:
        Вычисляет доверительный интервал для заданной метрики с использованием метода Bootstrap.

    Аргументы:
        data: Массив данных.
        num_samples: Количество Bootstrap выборок.
        metric_func: Функция для вычисления метрики.
        alpha: Уровень значимости для доверительного интервала.

    Возвращает:
        Доверительный интервал: нижняя и верхняя границы.
    """
    n = len(data)
    metrics = []

    for _ in range(num_samples):
        sample = np.random.choice(data, size=n, replace=True)  # Bootstrap выборка
        metrics.append(metric_func(sample))  # Вычисляем метрику для выборки

    lower_bound = np.percentile(metrics, 100 * alpha / 2)  # Нижняя граница
    upper_bound = np.percentile(metrics, 100 * (1 - alpha / 2))  # Верхняя граница

    return lower_bound, upper_bound

# Пример использования функции
data = [3, -0.5, 2, 7]  # Исходные данные
num_samples = 1000  # Количество Bootstrap выборок

# Функция для вычисления MAE
def mae(data):
    return np.mean(np.abs(data - np.mean(data)))

ci = bootstrap_ci(data, num_samples, mae)
print(f"95% доверительный интервал для MAE: {ci}")
```

В этом коде:
- Мы определяем функцию `bootstrap_ci`, которая принимает массив данных, количество Bootstrap выборок, функцию для вычисления метрики и уровень значимости.
- Функция возвращает доверительный интервал для заданной метрики.
- Мы выводим результат на экран.

### Физический и геометрический смысл

В физике метод Bootstrap может быть использован для оценки неопределенности в измерениях. Например, если мы измеряем длину объекта несколько раз, метод Bootstrap позволит нам оценить, насколько стабильны наши измерения и какова вероятность того, что истинная длина объекта находится в определенном диапазоне. Это особенно полезно в экспериментах, где точность измерений критична.

## Chunk 18
### **Название фрагмента: Метод Bootstrap для оценки качества моделей**

**Предыдущий контекст:** В предыдущем фрагменте мы обсудили метрики Huber и Log-Cosh, а также их применение для оценки качества предсказаний в условиях наличия выбросов. Теперь мы сосредоточимся на методе Bootstrap и его использовании для оценки качества моделей, а также на том, как он помогает в построении доверительных интервалов.

## **Метод Bootstrap и его применение**

Метод Bootstrap — это статистический метод, который позволяет оценить распределение статистики (например, ошибки модели) на основе повторных выборок из имеющихся данных. Он особенно полезен для построения доверительных интервалов, которые помогают понять, насколько надежны результаты модели.

### Как работает метод Bootstrap

1. **Создание Bootstrap выборок:** Из исходного набора данных создаются несколько выборок (обычно $B$), каждая из которых имеет такой же размер, как и исходный набор. При этом объекты могут повторяться, так как выборка производится с возвращением.

2. **Вычисление метрики:** Для каждой Bootstrap выборки вычисляется интересующая метрика (например, MSE или MAE).

3. **Оценка доверительного интервала:** На основе полученных значений метрики из Bootstrap выборок можно построить доверительный интервал. Например, можно взять 5-й и 95-й процентиль значений метрики, чтобы получить 90% доверительный интервал.

### Пример вычисления доверительного интервала

Предположим, у нас есть две модели с MSE:
- Модель A: MSE = 1100
- Модель B: MSE = 1000

Чтобы понять, какая модель лучше, мы можем использовать метод Bootstrap для оценки их качества.

1. **Создаем Bootstrap выборки:** Генерируем $B$ выборок из исходных данных для каждой модели.

2. **Вычисляем MSE для каждой выборки:** Для каждой выборки вычисляем MSE и сохраняем результаты.

3. **Строим доверительный интервал:** Находим 5-й и 95-й процентиль для каждой модели, чтобы получить доверительные интервалы.

### Пример кода

Давайте рассмотрим пример кода на Python, который реализует метод Bootstrap для оценки доверительного интервала MSE:

```python
import numpy as np

def bootstrap_mse(y_true, y_pred, num_samples=1000, alpha=0.05):
    """
    Описание:
        Вычисляет доверительный интервал для MSE с использованием метода Bootstrap.

    Аргументы:
        y_true: Массив истинных значений.
        y_pred: Массив предсказанных значений.
        num_samples: Количество Bootstrap выборок.
        alpha: Уровень значимости для доверительного интервала.

    Возвращает:
        Доверительный интервал: нижняя и верхняя границы.
    """
    n = len(y_true)
    mse_samples = []

    for _ in range(num_samples):
        indices = np.random.choice(range(n), size=n, replace=True)  # Bootstrap выборка
        mse = np.mean((y_true[indices] - y_pred[indices]) ** 2)  # Вычисляем MSE для выборки
        mse_samples.append(mse)

    lower_bound = np.percentile(mse_samples, 100 * alpha / 2)  # Нижняя граница
    upper_bound = np.percentile(mse_samples, 100 * (1 - alpha / 2))  # Верхняя граница

    return lower_bound, upper_bound

# Пример использования функции
y_true = [3, -0.5, 2, 7]
y_pred = [2.5, 0, 2, 8]

ci = bootstrap_mse(np.array(y_true), np.array(y_pred))
print(f"95% доверительный интервал для MSE: {ci}")
```

В этом коде:
- Мы определяем функцию `bootstrap_mse`, которая принимает массивы истинных и предсказанных значений, количество Bootstrap выборок и уровень значимости.
- Функция возвращает доверительный интервал для MSE.
- Мы выводим результат на экран.

### Физический и геометрический смысл

В физике метод Bootstrap может быть использован для оценки неопределенности в измерениях. Например, если мы измеряем длину объекта несколько раз, метод Bootstrap позволит нам оценить, насколько стабильны наши измерения и какова вероятность того, что истинная длина объекта находится в определенном диапазоне. Это особенно полезно в экспериментах, где точность измерений критична.

## Chunk 19
### **Название фрагмента: Доверительные интервалы и центральная предельная теорема**

**Предыдущий контекст:** В предыдущем фрагменте мы обсудили метод Bootstrap для оценки доверительных интервалов метрик, таких как MSE и MAE. Теперь мы сосредоточимся на том, как строить доверительные интервалы с использованием центральной предельной теоремы и как это связано с метриками качества моделей.

## **Доверительные интервалы и центральная предельная теорема**

Доверительные интервалы позволяют оценить, насколько надежны результаты модели, и помогают понять, в каком диапазоне значений может находиться истинное значение метрики. Центральная предельная теорема (ЦПТ) играет ключевую роль в построении этих интервалов.

### Центральная предельная теорема

ЦПТ утверждает, что при достаточно большом размере выборки распределение выборочного среднего будет стремиться к нормальному распределению, независимо от формы исходного распределения. Это позволяет использовать нормальное распределение для построения доверительных интервалов.

1. **Формулировка ЦПТ:** Если $X_1, X_2, \ldots, X_n$ — независимые случайные величины с одинаковым распределением, имеющими конечное математическое ожидание $E(X)$ и конечную дисперсию $D(X)$, то при увеличении $n$ распределение среднего:

$$
\bar{X} = \frac{1}{n} \sum_{i=1}^{n} X_i
$$

будет стремиться к нормальному распределению с математическим ожиданием $E(X)$ и дисперсией $\frac{D(X)}{n}$.

### Построение доверительного интервала

Для построения доверительного интервала для MSE можно использовать следующую формулу:

$$
\text{CI} = \hat{mse} \pm z_{\alpha/2} \cdot \frac{\sigma}{\sqrt{n}}
$$

где:
- $\hat{mse}$ — оценка MSE,
- $z_{\alpha/2}$ — квантиль стандартного нормального распределения,
- $\sigma$ — стандартное отклонение выборки,
- $n$ — размер выборки.

### Пример вычисления доверительного интервала

Предположим, у нас есть две модели с MSE:
- Модель A: $\hat{mse} = 1100$
- Модель B: $\hat{mse} = 1000$

1. **Вычисляем стандартное отклонение:** Предположим, что стандартное отклонение для модели A равно $30$, а для модели B — $25$.

2. **Вычисляем доверительный интервал для модели A:**
   - Уровень значимости $\alpha = 0.05$, тогда $z_{\alpha/2} \approx 1.96$.
   - Доверительный интервал:
   $$
   \text{CI}_A = 1100 \pm 1.96 \cdot \frac{30}{\sqrt{n}}
   $$

3. **Вычисляем доверительный интервал для модели B:**
   - Доверительный интервал:
   $$
   \text{CI}_B = 1000 \pm 1.96 \cdot \frac{25}{\sqrt{n}}
   $$

### Пример кода

Давайте рассмотрим пример кода на Python, который вычисляет доверительный интервал для MSE с использованием ЦПТ:

```python
import numpy as np
import scipy.stats as stats

def calculate_ci_mse(mse, std_dev, n, alpha=0.05):
    """
    Описание:
        Вычисляет доверительный интервал для MSE с использованием центральной предельной теоремы.

    Аргументы:
        mse: Оценка MSE.
        std_dev: Стандартное отклонение.
        n: Размер выборки.
        alpha: Уровень значимости для доверительного интервала.

    Возвращает:
        Доверительный интервал: нижняя и верхняя границы.
    """
    z = stats.norm.ppf(1 - alpha / 2)  # Квантиль стандартного нормального распределения
    margin_of_error = z * (std_dev / np.sqrt(n))  # Погрешность
    return mse - margin_of_error, mse + margin_of_error

# Пример использования функции
mse_a = 1100
std_dev_a = 30
n_a = 100  # Размер выборки

ci_a = calculate_ci_mse(mse_a, std_dev_a, n_a)
print(f"95% доверительный интервал для модели A: {ci_a}")

mse_b = 1000
std_dev_b = 25
n_b = 100  # Размер выборки

ci_b = calculate_ci_mse(mse_b, std_dev_b, n_b)
print(f"95% доверительный интервал для модели B: {ci_b}")
```

В этом коде:
- Мы определяем функцию `calculate_ci_mse`, которая принимает оценку MSE, стандартное отклонение, размер выборки и уровень значимости.
- Функция возвращает доверительный интервал для MSE.
- Мы выводим результаты на экран.

### Физический и геометрический смысл

В физике доверительные интервалы могут быть использованы для оценки неопределенности в измерениях. Например, если мы измеряем длину объекта несколько раз, доверительный интервал позволит нам оценить, насколько стабильны наши измерения и какова вероятность того, что истинная длина объекта находится в определенном диапазоне. Это особенно полезно в экспериментах, где точность измерений критична.

## Chunk 20
### **Название фрагмента: Доверительные интервалы для MAE и MSE с использованием Bootstrap и ЦПТ**

**Предыдущий контекст:** В предыдущем фрагменте мы обсудили метод Bootstrap для оценки доверительных интервалов метрик, таких как MSE и MAE, а также влияние выбросов на эти метрики. Теперь мы сосредоточимся на том, как строить доверительные интервалы для MAE и MSE, используя центральную предельную теорему (ЦПТ) и метод Bootstrap.

## **Доверительные интервалы для MAE и MSE**

Доверительные интервалы позволяют оценить, насколько надежны результаты модели, и помогают понять, в каком диапазоне значений может находиться истинное значение метрики. Для построения доверительных интервалов для MAE и MSE можно использовать как метод Bootstrap, так и центральную предельную теорему.

### Метод Bootstrap

Метод Bootstrap позволяет генерировать множество выборок из исходных данных и оценивать метрики для каждой выборки. Это позволяет получить распределение метрик и, следовательно, доверительные интервалы.

1. **Создание Bootstrap выборок:** Из исходного набора данных создаются несколько выборок (обычно $B$), каждая из которых имеет такой же размер, как и исходный набор. При этом объекты могут повторяться, так как выборка производится с возвращением.

2. **Вычисление метрики:** Для каждой Bootstrap выборки вычисляется интересующая метрика (например, MAE или MSE).

3. **Оценка доверительного интервала:** На основе полученных значений метрики из Bootstrap выборок можно построить доверительный интервал, например, взяв 5-й и 95-й процентиль значений метрики.

### Центральная предельная теорема (ЦПТ)

ЦПТ утверждает, что при достаточно большом размере выборки распределение выборочного среднего будет стремиться к нормальному распределению, независимо от формы исходного распределения. Это позволяет использовать нормальное распределение для построения доверительных интервалов.

1. **Формулировка ЦПТ:** Если $X_1, X_2, \ldots, X_n$ — независимые случайные величины с одинаковым распределением, имеющими конечное математическое ожидание $E(X)$ и конечную дисперсию $D(X)$, то при увеличении $n$ распределение среднего:

$$
\bar{X} = \frac{1}{n} \sum_{i=1}^{n} X_i
$$

будет стремиться к нормальному распределению с математическим ожиданием $E(X)$ и дисперсией $\frac{D(X)}{n}$.

### Пример вычисления доверительного интервала для MAE и MSE

Для набора данных:
- Истинные значения: $[3, -0.5, 2, 7]$
- Предсказанные значения: $[2.5, 0, 2, 8]$

1. **Вычисляем MAE и MSE:**
   - MAE: $0.5$
   - MSE: $0.375$

2. **Строим доверительные интервалы:**
   - Для MAE и MSE можно использовать как метод Bootstrap, так и ЦПТ для оценки доверительных интервалов.

### Пример кода для оценки доверительных интервалов

```python
import numpy as np
import scipy.stats as stats

def bootstrap_ci(data, num_samples, metric_func, alpha=0.05):
    """
    Описание:
        Вычисляет доверительный интервал для заданной метрики с использованием метода Bootstrap.

    Аргументы:
        data: Массив данных.
        num_samples: Количество Bootstrap выборок.
        metric_func: Функция для вычисления метрики.
        alpha: Уровень значимости для доверительного интервала.

    Возвращает:
        Доверительный интервал: нижняя и верхняя границы.
    """
    n = len(data)
    metrics = []

    for _ in range(num_samples):
        sample = np.random.choice(data, size=n, replace=True)  # Bootstrap выборка
        metrics.append(metric_func(sample))  # Вычисляем метрику для выборки

    lower_bound = np.percentile(metrics, 100 * alpha / 2)  # Нижняя граница
    upper_bound = np.percentile(metrics, 100 * (1 - alpha / 2))  # Верхняя граница

    return lower_bound, upper_bound

# Пример использования функции
data = [3, -0.5, 2, 7]  # Исходные данные
num_samples = 1000  # Количество Bootstrap выборок

# Функция для вычисления MAE
def mae(data):
    return np.mean(np.abs(data - np.mean(data)))

ci = bootstrap_ci(data, num_samples, mae)
print(f"95% доверительный интервал для MAE: {ci}")
```

В этом коде:
- Мы определяем функцию `bootstrap_ci`, которая принимает массив данных, количество Bootstrap выборок, функцию для вычисления метрики и уровень значимости.
- Функция возвращает доверительный интервал для заданной метрики.
- Мы выводим результат на экран.

### Физический и геометрический смысл

В физике метод Bootstrap может быть использован для оценки неопределенности в измерениях. Например, если мы измеряем длину объекта несколько раз, метод Bootstrap позволит нам оценить, насколько стабильны наши измерения и какова вероятность того, что истинная длина объекта находится в определенном диапазоне. Это особенно полезно в экспериментах, где точность измерений критична.

## Chunk 21
### **Название фрагмента: Понимание распределений и их влияние на метрики**

**Предыдущий контекст:** В предыдущем фрагменте мы обсудили метод Bootstrap для оценки доверительных интервалов и его применение в контексте метрик MSE и MAE. Теперь мы сосредоточимся на том, как распределения случайных величин влияют на метрики и как это понимание может помочь в решении практических задач.

## **Влияние распределений на метрики и функции потерь**

Распределение случайных величин играет важную роль в оценке качества предсказаний моделей. Понимание того, как различные функции потерь и метрики ведут себя в зависимости от распределения данных, позволяет более точно интерпретировать результаты и выбирать подходящие методы.

### Распределения случайных величин

1. **Нормальное распределение:** Многие статистические методы, включая MSE и MAE, предполагают, что ошибки распределены нормально. Это означает, что большинство ошибок будут близки к нулю, а крайние значения будут встречаться реже.

2. **Ненормальное распределение:** В реальных данных часто встречаются выбросы и аномалии, которые могут нарушить предположение о нормальности. Например, если ошибки имеют распределение с тяжелыми хвостами, это может привести к неправильным выводам при использовании MSE.

### Математическое ожидание и дисперсия

Для оценки качества предсказаний важно вычислить математическое ожидание и дисперсию ошибок. Например, для MAE и MSE можно записать:

1. **MAE:**
   $$ 
   E(\text{MAE}) = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i| 
   $$

2. **MSE:**
   $$ 
   E(\text{MSE}) = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 
   $$

### Пример с выбросами

Если в данных присутствуют выбросы, то MSE будет значительно увеличиваться, так как она чувствительна к большим ошибкам. Например, если 5% данных являются выбросами, то математическое ожидание MSE можно записать как:

$$
E(\text{MSE}) = P_{\text{базовая}} \cdot E(\text{базовая ошибка}) + P_{\text{выброс}} \cdot E(\text{выброс})
$$

где $P_{\text{базовая}}$ — вероятность базовой ошибки, а $P_{\text{выброс}}$ — вероятность выброса.

### Пример кода для оценки MSE и MAE с учетом выбросов

```python
import numpy as np

def calculate_mae(y_true, y_pred):
    """
    Описание:
        Вычисляет среднюю абсолютную ошибку (MAE) для заданных истинных и предсказанных значений.

    Аргументы:
        y_true: Массив истинных значений.
        y_pred: Массив предсказанных значений.

    Возвращает:
        MAE: Средняя абсолютная ошибка.
    """
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    return np.mean(np.abs(y_true - y_pred))

def calculate_mse(y_true, y_pred):
    """
    Описание:
        Вычисляет среднюю квадратичную ошибку (MSE) для заданных истинных и предсказанных значений.

    Аргументы:
        y_true: Массив истинных значений.
        y_pred: Массив предсказанных значений.

    Возвращает:
        MSE: Средняя квадратичная ошибка.
    """
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    return np.mean((y_true - y_pred) ** 2)

# Пример использования функций
y_true = [3, -0.5, 2, 7]
y_pred = [2.5, 0, 2, 8]

mae = calculate_mae(y_true, y_pred)
mse = calculate_mse(y_true, y_pred)

print(f"MAE: {mae:.2f}, MSE: {mse:.2f}")
```

В этом коде:
- Мы определяем функции `calculate_mae` и `calculate_mse`, которые принимают массивы истинных и предсказанных значений.
- Каждая функция возвращает соответствующую метрику.
- Мы выводим результаты на экран.

### Физический и геометрический смысл

В физике понимание распределений случайных величин и их влияние на метрики позволяет более точно оценивать качество моделей. Например, если мы измеряем температуру в разных точках, выбросы могут указывать на неправильные показания термометра. Понимание того, как распределения влияют на MAE и MSE, позволяет исследователям и инженерам более точно оценивать качество своих моделей и принимать обоснованные решения о том, как обрабатывать данные.

## Chunk 22
### **Название фрагмента: Заключение и рекомендации по курсу**

**Предыдущий контекст:** В предыдущем фрагменте мы обсудили влияние выбросов на метрики MSE и MAE, а также применение метода Bootstrap для оценки доверительных интервалов. Теперь мы подводим итоги и делаем рекомендации по курсу.

## **Заключение и рекомендации по курсу**

В ходе семинара мы рассмотрели различные метрики, используемые для оценки качества предсказаний моделей, такие как MSE, MAE, Huber и Log-Cosh. Мы также обсудили, как выбросы в данных могут влиять на эти метрики и как использовать метод Bootstrap для построения доверительных интервалов.

### Основные выводы

1. **Выбор метрики:** Важно выбирать метрику, которая соответствует специфике задачи. Например, если в данных присутствуют выбросы, лучше использовать MAE или Huber, так как они менее чувствительны к большим ошибкам.

2. **Доверительные интервалы:** Построение доверительных интервалов позволяет оценить надежность предсказаний модели. Метод Bootstrap является универсальным инструментом для этой цели.

3. **Центральная предельная теорема:** ЦПТ позволяет использовать нормальное распределение для построения доверительных интервалов, что делает её важным инструментом в статистическом анализе.

### Рекомендации

- **Практика:** Рекомендуется практиковаться в вычислении различных метрик и доверительных интервалов на реальных данных. Это поможет лучше понять, как они работают и как их применять в различных ситуациях.

- **Обсуждение:** Если у вас возникли вопросы или предложения по курсу, не стесняйтесь делиться ими. Обратная связь важна для улучшения качества обучения.

- **Дополнительные материалы:** Мы планируем предоставить дополнительные материалы, такие как PDF с условиями задач, чтобы вы могли лучше подготовиться к следующему семинару.

### Заключение

Спасибо всем за участие в семинаре! Надеемся, что вы получили полезные знания и навыки, которые сможете применить в своей практике. Если у вас есть какие-либо вопросы или комментарии, пожалуйста, пишите в чат или обращайтесь напрямую. Желаем вам удачи в дальнейших исследованиях и обучении!

## Final Summary
### **Название фрагмента: Итоги семинара и рекомендации**

**Предыдущий контекст:** В предыдущем фрагменте мы обсудили влияние выбросов на метрики MSE и MAE, а также применение метода Bootstrap для оценки доверительных интервалов. Теперь мы подводим итоги семинара и делаем рекомендации по дальнейшему обучению.

## **Итоги семинара и рекомендации**

В ходе семинара мы рассмотрели различные метрики, используемые для оценки качества предсказаний моделей, такие как MSE, MAE, Huber и Log-Cosh. Мы также обсудили, как выбросы в данных могут влиять на эти метрики и как использовать метод Bootstrap для построения доверительных интервалов.

### Основные выводы

1. **Выбор метрики:** Важно выбирать метрику, которая соответствует специфике задачи. Например, если в данных присутствуют выбросы, лучше использовать MAE или Huber, так как они менее чувствительны к большим ошибкам.

2. **Доверительные интервалы:** Построение доверительных интервалов позволяет оценить надежность предсказаний модели. Метод Bootstrap является универсальным инструментом для этой цели.

3. **Центральная предельная теорема:** ЦПТ позволяет использовать нормальное распределение для построения доверительных интервалов, что делает её важным инструментом в статистическом анализе.

### Рекомендации

- **Практика:** Рекомендуется практиковаться в вычислении различных метрик и доверительных интервалов на реальных данных. Это поможет лучше понять, как они работают и как их применять в различных ситуациях.

- **Обсуждение:** Если у вас возникли вопросы или предложения по курсу, не стесняйтесь делиться ими. Обратная связь важна для улучшения качества обучения.

- **Дополнительные материалы:** Мы планируем предоставить дополнительные материалы, такие как PDF с условиями задач, чтобы вы могли лучше подготовиться к следующему семинару.

### Заключение

Спасибо всем за участие в семинаре! Надеемся, что вы получили полезные знания и навыки, которые сможете применить в своей практике. Если у вас есть какие-либо вопросы или комментарии, пожалуйста, пишите в чат или обращайтесь напрямую. Желаем вам удачи в дальнейших исследованиях и обучении!
